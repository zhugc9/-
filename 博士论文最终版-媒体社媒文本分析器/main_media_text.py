# -*- coding: utf-8 -*-
"""
åª’ä½“æ–‡æœ¬åˆ†æä¸»å…¥å£ - ç§å­æ‰©å±•ä¸¤é˜¶æ®µåˆ†æ
"""
import os
import glob
import asyncio
import aiohttp
import pandas as pd
from asyncio import Lock
from tqdm.asyncio import tqdm as aio_tqdm

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core import (
    UX, load_config, APIService, MediaTextProcessor, 
    ReliabilityTestModule, safe_str_convert, identify_source
)
from core.processors import ProcessingStatus

# åŠ è½½é…ç½®
CONFIG = load_config()
if CONFIG is None:
    raise RuntimeError("æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")

# ä»é…ç½®ä¸­æå–å˜é‡
INPUT_PATH = CONFIG['file_paths']['input']
OUTPUT_PATH = CONFIG['file_paths']['output']
RELIABILITY_TEST_MODE = CONFIG['reliability_test']['enabled']
RELIABILITY_SAMPLING_CONFIG = CONFIG['reliability_test']['sampling_config']
AUTO_RETRY_CONFIG = CONFIG.get('auto_retry', {})
AUTO_RETRY_ENABLED = AUTO_RETRY_CONFIG.get('enabled', False)
MAX_RETRY_ROUNDS = AUTO_RETRY_CONFIG.get('max_retry_rounds', 5)
RETRY_DELAY_MINUTES = AUTO_RETRY_CONFIG.get('retry_delay_minutes', 2)
MIN_FAILED_THRESHOLD = AUTO_RETRY_CONFIG.get('min_failed_threshold', 3)
COLUMN_MAPPING = CONFIG['column_mapping']
REQUIRED_OUTPUT_COLUMNS = CONFIG['required_output_columns']

# APIé…ç½®
API_RETRY_CONFIG = CONFIG.get('api_retry_config', {})
MAX_CONCURRENT_REQUESTS = API_RETRY_CONFIG.get('max_concurrent_requests', 1)
BUFFER_CONFIG = CONFIG.get('buffer_config', {})
QUEUE_TIMEOUT = API_RETRY_CONFIG.get('queue_timeout', 30.0)

# æ¨¡å‹æ± é…ç½®
MODEL_POOLS = CONFIG.get('model_pools', {})
API_CONFIG = CONFIG.get('api_config', {})

# å…¨å±€é”
file_write_lock = Lock()

# æç¤ºè¯ç±»
class Prompts:
    def __init__(self):
        self.prompts_dir = os.path.join(os.path.dirname(__file__), 'prompts')
        self._load_prompts()

    def _load_prompts(self):
        prompt_files = {
            'UNIT_EXTRACTION': 'SEED.txt',
            'UNIT_ANALYSIS': 'ANALYSIS.txt'
        }
        for attr_name, filename in prompt_files.items():
            file_path = os.path.join(self.prompts_dir, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    setattr(self, attr_name, f.read())
            except Exception as e:
                print(f"åŠ è½½æç¤ºè¯æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                setattr(self, attr_name, "")

    @property
    def UNIT_EXTRACTION(self):
        return getattr(self, 'UNIT_EXTRACTION', "")

    @property
    def UNIT_ANALYSIS(self):
        return getattr(self, 'UNIT_ANALYSIS', "")

# åˆ›å»ºæç¤ºè¯å®ä¾‹
prompts = Prompts()

async def api_worker(name, task_queue, results_queue, api_service, source, pbar, processor, output_file_path=None, failed_unit_ids=None):
   while True:
       try:
           item = await task_queue.get()
           if item is None:
               break
           
           # æ¯10ä¸ªä»»åŠ¡æ£€æŸ¥ä¸€æ¬¡æ´»åŠ¨çŠ¶æ€
           if pbar.n % 10 == 0:
               UX.check_activity()
           
           try:
               item_with_source = (item[0], item[1], source)
               current_id = safe_str_convert(item[1].get(COLUMN_MAPPING["ID"], "unknown")) if len(item) > 1 else "unknown"
               original_id, result_Units = await processor.process_row(
                   item_with_source,
                   output_file_path=output_file_path,
                   failed_unit_ids=failed_unit_ids
               )
               await results_queue.put((original_id, result_Units, None))  # ä¸¤é˜¶æ®µæ¨¡å‹ï¼šåªè¿”å›è®®é¢˜å•å…ƒ
           except Exception as e:
               error_brief = str(e)[:30] + "..." if len(str(e)) > 30 else str(e)
               UX.api_failed(f"Worker-{name}", error_brief)
               original_id = safe_str_convert(item[1].get(COLUMN_MAPPING["ID"], "unknown")) if len(item) > 1 else "unknown"
               await results_queue.put((original_id, [processor.create_unified_record(ProcessingStatus.API_FAILED, original_id, source, "", f"Workeré”™è¯¯: {str(e)[:100]}")], None))
           finally:
               # ä¿è¯æ— è®ºæˆåŠŸå¤±è´¥ï¼Œä»»åŠ¡å®Œæˆåéƒ½æ›´æ–°è¿›åº¦æ¡
               pbar.update(1)
               task_queue.task_done()
       
       except asyncio.CancelledError:
           break
       except Exception as e:
           error_brief = str(e)[:30] + "..." if len(str(e)) > 30 else str(e)
           UX.api_failed(f"Worker-{name}-Critical", error_brief)

async def saver_worker(results_queue, df_input, output_file_path, total_tasks=None):
    """
    [ä¸¤é˜¶æ®µæ¨¡å‹] æ‰¹é‡ä¿å­˜workerï¼š
    - ç¼“å†²å¹¶åˆ†æ‰¹ä¿å­˜è®®é¢˜å•å…ƒåˆ†æç»“æœ
    """
    analysis_buffer = []
    received_count = 0
    
    # ä»é…ç½®è¯»å–ç¼“å†²åŒºå¤§å°é˜ˆå€¼
    ANALYSIS_BUFFER_LIMIT = BUFFER_CONFIG.get('analysis_buffer_limit', 30)    # åˆ†æç»“æœç¼“å†²åŒº

    async def save_analysis_batch(data):
        if not data:
            return
        async with file_write_lock:
            # è°ƒç”¨ç°æœ‰çš„åˆ†æç»“æœä¿å­˜å‡½æ•°
            save_data_to_excel(data, df_input, output_file_path)

    while True:
        try:
            # ç­‰å¾…ç»“æœï¼Œä»é…ç½®è¯»å–è¶…æ—¶æ—¶é—´
            item = await asyncio.wait_for(results_queue.get(), timeout=QUEUE_TIMEOUT)
            if item is None: # æ”¶åˆ°ç»“æŸä¿¡å·
                await save_analysis_batch(analysis_buffer)
                break

            original_id, result_Units, _ = item  # ç¬¬ä¸‰ä¸ªå‚æ•°ä¸ºå…¼å®¹æ€§ä¿ç•™ï¼Œä¸¤é˜¶æ®µæ¨¡å‹ä¸­æœªä½¿ç”¨
            received_count += 1

            # æ”¶é›†åˆ†æç»“æœåˆ°ç¼“å†²åŒº
            if result_Units:
                for unit in result_Units:
                    unit[COLUMN_MAPPING["ID"]] = original_id
                analysis_buffer.extend(result_Units)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç©ºå¹¶ä¿å­˜ç¼“å†²åŒº
            if len(analysis_buffer) >= ANALYSIS_BUFFER_LIMIT:
                await save_analysis_batch(analysis_buffer)
                UX.info(f"ğŸ’¾ æ‰¹é‡ä¿å­˜: {len(analysis_buffer)} æ¡è®®é¢˜å•å…ƒåˆ†æç»“æœ")
                analysis_buffer = []

            results_queue.task_done()
            if total_tasks and received_count >= total_tasks:
                await save_analysis_batch(analysis_buffer)
                break

        except asyncio.TimeoutError:
            # è¶…æ—¶åä¿å­˜æ‰€æœ‰ç¼“å†²åŒºå†…å®¹ï¼Œé˜²æ­¢åœ¨ä»»åŠ¡é—´éš™ä¸¢å¤±æ•°æ®
            await save_analysis_batch(analysis_buffer)
            analysis_buffer = []
            if total_tasks and received_count >= total_tasks:
                break
            continue

        except asyncio.CancelledError:
            await save_analysis_batch(analysis_buffer)
            break

        except Exception as e:
            UX.err(f"ä¿å­˜çº¿ç¨‹é”™è¯¯: {e}")

def save_data_to_excel(new_Units_list, df_input, output_file_path):
   try:
       df_existing = pd.DataFrame()
       if os.path.exists(output_file_path):
           try:
               df_existing = pd.read_excel(output_file_path)
           except Exception as e:
               UX.warn(f"è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}")
       df_new_Units = pd.DataFrame(new_Units_list)

       if df_new_Units.empty:
           return

       df_existing = ensure_required_columns(df_existing)
       df_new_Units = ensure_required_columns(df_new_Units)
       
       if COLUMN_MAPPING["ID"] in df_input.columns and COLUMN_MAPPING["ID"] in df_new_Units.columns:
           df_input[COLUMN_MAPPING["ID"]] = df_input[COLUMN_MAPPING["ID"]].astype(str)
           df_new_Units[COLUMN_MAPPING["ID"]] = df_new_Units[COLUMN_MAPPING["ID"]].astype(str)
           new_original_ids = df_new_Units[COLUMN_MAPPING["ID"]].unique()
           
           # ğŸ”§ ä¿®å¤æ ¸å¿ƒé—®é¢˜ï¼šæ¸…ç†æ—§çš„"æ¦‚è¦çº§"å¤±è´¥è®°å½•
           # å½“ä¸€ç¯‡æ–‡ç« è¢«é‡æ–°å¤„ç†æˆåŠŸæ—¶ï¼Œåˆ é™¤å®ƒçš„æ—§å¤±è´¥è®°å½•
           if not df_existing.empty and 'Unit_ID' in df_existing.columns:
               # è¯†åˆ«"æ¦‚è¦çº§"è®°å½•ï¼šUnit_IDä¸åŒ…å«"-Unit-"çš„è®°å½•
               # ä¾‹å¦‚ï¼š"(3)-RU1012-API_FAILED" æˆ– "(3)-RU1012-NO_RELEVANT"
               summary_mask = (
                   df_existing[COLUMN_MAPPING["ID"]].isin(new_original_ids) & 
                   ~df_existing['Unit_ID'].str.contains('-Unit-', na=False)
               )
               
               removed_count = summary_mask.sum()
               if removed_count > 0:
                   UX.info(f"ğŸ§¹ æ¸…ç† {removed_count} æ¡æ—§çš„æ¦‚è¦çº§è®°å½•ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰")
                   df_existing = df_existing[~summary_mask].copy()

           input_base_cols = [col for col in df_input.columns if col in df_new_Units.columns and col != COLUMN_MAPPING["ID"]]
           if input_base_cols:
               df_new_Units = df_new_Units.drop(columns=input_base_cols)
           
           df_input_subset = df_input[df_input[COLUMN_MAPPING["ID"]].isin(new_original_ids)].copy()
           df_newly_merged = pd.merge(df_input_subset, df_new_Units, on=COLUMN_MAPPING["ID"], how='left')
       else:
           df_newly_merged = df_new_Units.copy()

       df_final_to_save = pd.concat([df_existing, df_newly_merged], ignore_index=True)

       if 'Unit_ID' in df_final_to_save.columns:
           df_final_to_save = df_final_to_save.drop_duplicates(subset=['Unit_ID'], keep='last')

       df_final_to_save = reorder_columns(df_final_to_save, list(df_input.columns))
       df_final_to_save.to_excel(output_file_path, index=False)
       UX.ok(f"ğŸ’¾ å·²ä¿å­˜: {os.path.basename(output_file_path)} (ç´¯è®¡ {len(df_final_to_save)} æ¡è®°å½•)")

   except Exception as e:
       UX.err(f"Excelä¿å­˜å¤±è´¥: {e}")

async def main_async():
   UX.start_run()
   UX.phase("é•¿æ–‡æœ¬åˆ†æå™¨å¯åŠ¨")
   UX.info(f"ä¿¡åº¦æ£€éªŒæ¨¡å¼: {'å¼€å¯' if RELIABILITY_TEST_MODE else 'å…³é—­'}")
   UX.info(f"è‡ªåŠ¨é‡è¯•æ¨¡å¼: {'å¼€å¯' if AUTO_RETRY_ENABLED else 'å…³é—­'}")
   
   # å¦‚æœå¯ç”¨è‡ªåŠ¨é‡è¯•ï¼Œè¿›å…¥é‡è¯•å¾ªç¯
   if AUTO_RETRY_ENABLED:
       await main_with_auto_retry()
   else:
       await main_processing_logic()

async def main_with_auto_retry():
   """å¸¦è‡ªåŠ¨é‡è¯•çš„ä¸»é€»è¾‘"""
   retry_round = 0
   
   while retry_round < MAX_RETRY_ROUNDS:
       retry_round += 1
       
       # åŒå±‚åˆ†éš”ç¬¦æ ‡è®°è½®æ¬¡å¼€å§‹
       print("\n" + "="*100)
       print("="*100)
       UX.phase(f"ğŸ”„ ç¬¬ {retry_round} è½®å¤„ç†å¼€å§‹")
       print("="*100)
       
       # æ‰§è¡Œä¸»å¤„ç†é€»è¾‘
       await main_processing_logic()
       
       # è½®æ¬¡ç»“æŸåˆ†éš”ç¬¦
       print("="*100)
       print(f"=== ğŸ ç¬¬ {retry_round} è½®å¤„ç†å®Œæˆ ===")
       print("="*100 + "\n")
       
       # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¤±è´¥è®°å½•
       total_failed, failed_files = check_failed_records(OUTPUT_PATH)
       
       if total_failed >= MIN_FAILED_THRESHOLD:
           UX.warn(f"ğŸ”„ æ£€æµ‹åˆ° {total_failed} æ¡APIå¤±è´¥è®°å½•ï¼Œå‡†å¤‡ç¬¬ {retry_round + 1} è½®é‡è¯•")
           for file_info in failed_files:
               UX.info(f"   ğŸ“„ {file_info['file']}: {file_info['failed_count']} æ¡å¤±è´¥")
           
           if retry_round < MAX_RETRY_ROUNDS:
               UX.info(f"â³ ç­‰å¾… {RETRY_DELAY_MINUTES} åˆ†é’Ÿåå¼€å§‹é‡è¯•...")
               await asyncio.sleep(RETRY_DELAY_MINUTES * 60)  # è½¬æ¢ä¸ºç§’
           else:
               UX.warn(f"âš ï¸  å·²è¾¾åˆ°æœ€å¤§é‡è¯•è½®æ•° ({MAX_RETRY_ROUNDS})ï¼Œåœæ­¢é‡è¯•")
               break
       else:
           if total_failed > 0:
               UX.info(f"âœ… å‰©ä½™ {total_failed} æ¡å¤±è´¥è®°å½•ï¼ˆå°äºé˜ˆå€¼ {MIN_FAILED_THRESHOLD}ï¼‰ï¼Œå¤„ç†å®Œæˆ")
           else:
               UX.ok("ğŸ‰ æ‰€æœ‰è®°å½•å¤„ç†æˆåŠŸï¼Œæ— éœ€é‡è¯•ï¼")
           break
   
   UX.phase("è‡ªåŠ¨é‡è¯•æµç¨‹å®Œæˆ")

async def activity_monitor():
    """æ´»åŠ¨ç›‘æ§ä»»åŠ¡ï¼Œæ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡"""
    while True:
        await asyncio.sleep(300)  # 5åˆ†é’Ÿ
        UX.check_activity()

async def main_processing_logic():
    """ä¸»å¤„ç†é€»è¾‘ï¼ˆä»åŸmain_asyncå‡½æ•°ç§»åŠ¨è€Œæ¥ï¼‰"""
    
    # å¯åŠ¨æ´»åŠ¨ç›‘æ§ä»»åŠ¡
    monitor_task = asyncio.create_task(activity_monitor())
    
    try:
        # éªŒè¯ä¸¤é˜¶æ®µæ¨¡å‹é…ç½®
        required_keys = ["UNIT_EXTRACTION", "UNIT_ANALYSIS"]
        # æ£€æŸ¥æ–°çš„model_poolsé…ç½®
        primary_models = MODEL_POOLS.get('primary_models', {})
        missing = [k for k in required_keys if k not in primary_models]
        if missing:
            raise ValueError(f"ç¼ºå°‘ä¸¤é˜¶æ®µæ¨¡å‹æ± é…ç½®: {missing}")

        # æ£€æŸ¥APIå¯†é’¥
        if not API_CONFIG["API_KEYS"] or not API_CONFIG["API_KEYS"][0]:
            UX.err("æœªæä¾›æœ‰æ•ˆAPIå¯†é’¥")
            return

        # ç¡®å®šè¾“å…¥æ–‡ä»¶
        if os.path.isdir(INPUT_PATH):
            files_to_process = glob.glob(os.path.join(INPUT_PATH, '*.xlsx'))
            is_folder_mode = True
            os.makedirs(OUTPUT_PATH, exist_ok=True)
        elif os.path.isfile(INPUT_PATH):
            files_to_process = [INPUT_PATH]
            is_folder_mode = False
        else:
            UX.err("è¾“å…¥è·¯å¾„æ— æ•ˆ")
            return

        async with aiohttp.ClientSession() as session:
            api_service = APIService(session, CONFIG)
            # åˆ›å»ºåª’ä½“æ–‡æœ¬å¤„ç†å™¨
            processor = MediaTextProcessor(api_service, CONFIG, prompts)

            for file_path in files_to_process:
                file_basename = os.path.basename(file_path)
                
                # æ–‡ä»¶å¤„ç†å¼€å§‹æ ‡è®°
                print("\n" + "="*100)
                print("="*100)
                UX.phase(f"ğŸ“ å¤„ç†æ–‡ä»¶: {file_basename}")
                print("="*50)

                # è¯†åˆ«ä¿¡æº
                source = identify_source(file_basename)
                UX.info(f"ğŸŒ è¯†åˆ«ä¿¡æº: {source}")

                output_file_path = os.path.join(OUTPUT_PATH, f"(ä¸èƒ½åˆ )analyzed_{file_basename}") if is_folder_mode else OUTPUT_PATH

                # è¯»å–è¾“å…¥æ–‡ä»¶
                try:
                    df_input = pd.read_excel(file_path)
                    if COLUMN_MAPPING["MEDIA_TEXT"] not in df_input.columns:
                        UX.err("æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„æ–‡æœ¬åˆ—")
                        continue
                except Exception as e:
                    UX.err(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                    continue

                # ä¸¤é˜¶æ®µæ¨¡å‹ï¼šç›´æ¥å¤„ç†è®®é¢˜å•å…ƒï¼Œæ— éœ€ä¸­é—´æ•°æ®åº“

                # ğŸ” æ„å»ºæ–­ç‚¹ç»­ä¼ è®¡åˆ’
                UX.resume_plan(file_basename)
                total_input_articles = len(set(df_input[COLUMN_MAPPING["ID"]].astype(str)))
                never_processed_ids, failed_unit_ids = build_resume_plan(
                    output_file_path, df_input, COLUMN_MAPPING["ID"]
                )

                ids_to_process = set(never_processed_ids) | set(failed_unit_ids.keys())
                failed_units_count = sum(len(v) for v in failed_unit_ids.values())

                print(f"ğŸ“„ æ€»æ–‡ç« æ•°: {total_input_articles}")
                print(f"ğŸ†• ä»æœªå¤„ç†: {len(never_processed_ids)} ç¯‡")
                print(f"ğŸ”„ å¤±è´¥é‡è¯•: {len(failed_unit_ids)} ç¯‡ ({failed_units_count} ä¸ªå•å…ƒ)")
                print(f"ğŸ¯ æœ¬æ¬¡å¤„ç†: {len(ids_to_process)} ç¯‡ | è·³è¿‡: {total_input_articles - len(ids_to_process)} ç¯‡")
                UX.resume_end()

                df_to_process = df_input[df_input[COLUMN_MAPPING["ID"]].astype(str).isin(ids_to_process)].copy()

                if len(df_to_process) > 0:
                    UX.phase(f"å¼€å§‹å¤„ç† {len(df_to_process)} ç¯‡æ–‡ç« ")
                    UX.info(f"ğŸš€ ä¸¤é˜¶æ®µæ¨¡å‹å¤„ç†: ç¬¬ä¸€é˜¶æ®µ(å•å…ƒæå–) â†’ ç¬¬äºŒé˜¶æ®µ(æ·±åº¦åˆ†æ)")

                    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
                    task_queue = asyncio.Queue()
                    results_queue = asyncio.Queue()
                    total_tasks = len(df_to_process)

                    # æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—
                    for item in df_to_process.iterrows():
                        await task_queue.put(item)

                    # åˆ›å»ºè¿›åº¦æ¡
                    pbar = aio_tqdm(total=total_tasks, desc=f"ğŸ”„ å¤„ç†æ–‡ç«  ({file_basename})", 
                                   bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]")

                    # åˆ›å»ºä¿å­˜ä»»åŠ¡ 
                    saver_task = asyncio.create_task(
                        saver_worker(results_queue, df_input, output_file_path, total_tasks)
                    )

                    # åˆ›å»ºå·¥ä½œä»»åŠ¡
                    worker_tasks = [
                        asyncio.create_task(
                            api_worker(
                                f'worker-{i}',
                                task_queue,
                                results_queue,
                                api_service,
                                source,
                                pbar,
                                processor,
                                output_file_path=output_file_path,
                                failed_unit_ids=failed_unit_ids
                            )
                        )
                        for i in range(MAX_CONCURRENT_REQUESTS)
                    ]

                    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡éƒ½è¢«workerå¤„ç†å®Œæ¯•
                    await task_queue.join()
                    
                    pbar.close()
                
                elif len(ids_to_process) == 0:
                    UX.ok(f"ğŸ‰ æ–‡ä»¶ {file_basename}: æ‰€æœ‰æ¡ç›®å·²å®Œç¾å¤„ç†å®Œæ¯•ï¼")
                    continue
           
                # ğŸ‰ å¤„ç†å®Œæˆæ€»ç»“
                try:
                    df_final_check = pd.read_excel(output_file_path)
                    if not df_final_check.empty and 'processing_status' in df_final_check.columns:
                        final_success = (df_final_check['processing_status'] == ProcessingStatus.SUCCESS).sum()
                        final_no_relevant = (df_final_check['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                        final_failed = (df_final_check['processing_status'] == ProcessingStatus.API_FAILED).sum()
                        final_total_units = len(df_final_check)
                        
                        # è®¡ç®—æ–‡ç« çº§åˆ«å®Œæˆåº¦
                        final_processed_ids, final_failed_ids = get_processing_state(df_final_check, COLUMN_MAPPING["ID"])
                        final_completed_articles = len(final_processed_ids - final_failed_ids)
                        final_completion_rate = (final_completed_articles / max(1, total_input_articles)) * 100
                        
                        # æ–‡ä»¶å¤„ç†ç»“æœæ€»ç»“
                        print("\n" + "-"*80)
                        UX.phase(f"ğŸ“Š æ–‡ä»¶ {file_basename} å¤„ç†å®Œæˆæ€»ç»“")
                        print("-"*80)
                        UX.ok(f"ğŸ“Š æ–‡ç« å®Œæˆåº¦: {final_completed_articles}/{total_input_articles} ({final_completion_rate:.1f}%)")
                        UX.ok(f"ğŸ¯ è®®é¢˜å•å…ƒç»Ÿè®¡: âœ…æˆåŠŸ {final_success}æ¡ | ğŸ“æ— ç›¸å…³ {final_no_relevant}æ¡ | âŒå¤±è´¥ {final_failed}æ¡")
                        
                        if final_failed_ids:
                            UX.warn(f"âš ï¸  ä»æœ‰ {len(final_failed_ids)} ç¯‡æ–‡ç« å¤„ç†å¤±è´¥ï¼Œå¯å†æ¬¡è¿è¡Œè¿›è¡Œæ™ºèƒ½é‡è¯•")
                        else:
                            UX.ok(f"âœ¨ å®Œç¾ï¼æ‰€æœ‰æ–‡ç« å‡å·²æˆåŠŸå¤„ç†")
                        print("-"*80)
                    else:
                        UX.ok(f"âœ… æ–‡ä»¶ {file_basename} å¤„ç†å®Œæˆ")
                except Exception:
                    UX.ok(f"âœ… æ–‡ä»¶ {file_basename} å¤„ç†å®Œæˆ")

        # ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶
        if RELIABILITY_TEST_MODE:
            UX.phase("ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶")

            # åˆå¹¶æ‰€æœ‰ç»“æœæ–‡ä»¶
            final_results_files = glob.glob(os.path.join(OUTPUT_PATH, "*analyzed_*.xlsx"))
            if final_results_files:
                all_results = []
                for file in final_results_files:
                    df = pd.read_excel(file)
                    df = ensure_required_columns(df)
                    source = identify_source(os.path.basename(file))
                    df['Source'] = source
                    all_results.append(df)

                if all_results:
                    df_all_results = pd.concat(all_results, ignore_index=True)
                    df_all_results = ensure_required_columns(df_all_results)
                    df_all_results = reorder_columns(df_all_results, [])

                    combined_results_path = os.path.join(OUTPUT_PATH, 'åª’ä½“_æœ€ç»ˆåˆ†ææ•°æ®åº“.xlsx')
                    df_all_results.to_excel(combined_results_path, index=False)

                    if 'processing_status' in df_all_results.columns:
                        success_count = (df_all_results['processing_status'] == ProcessingStatus.SUCCESS).sum()
                        no_relevant_count = (df_all_results['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                        failed_count = (df_all_results['processing_status'] == ProcessingStatus.API_FAILED).sum()
                        UX.ok(f"ğŸ“Š æœ€ç»ˆæ•°æ®åº“å·²ä¿å­˜: {combined_results_path}")
                        UX.info(f"   ğŸ“ˆ æ€»è®°å½•: {len(df_all_results)} | âœ…æˆåŠŸ: {success_count} | ğŸ“æ— ç›¸å…³: {no_relevant_count} | âŒå¤±è´¥: {failed_count}")

                    # ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶
                    try:
                        UX.info("ğŸ” å¼€å§‹ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶...")
                        from core.reliability import create_reliability_test_module
                        
                        reliability_module = create_reliability_test_module(
                            input_path=INPUT_PATH,
                            output_path=OUTPUT_PATH,
                            sampling_config=RELIABILITY_SAMPLING_CONFIG,
                            id_column=COLUMN_MAPPING["ID"],
                            text_column=COLUMN_MAPPING["MEDIA_TEXT"]
                        )
                        
                        reliability_module.generate_reliability_files()
                        UX.ok("âœ… ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆå®Œæˆ")
                    except Exception as e:
                        UX.err(f"âŒ ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
                        UX.info("â­ï¸  å°†ç»§ç»­å…¶ä»–ä»»åŠ¡...")

                else:
                    UX.warn("åˆå¹¶ç»“æœæ–‡ä»¶å¤±è´¥")
            else:
                UX.warn("æœªæ‰¾åˆ°ä»»ä½•analyzedç»“æœæ–‡ä»¶")

    finally:
        # å–æ¶ˆæ´»åŠ¨ç›‘æ§ä»»åŠ¡
        monitor_task.cancel()
        try:
            await monitor_task
        except asyncio.CancelledError:
            pass
    
    UX.phase("æ‰€æœ‰ä»»åŠ¡å®Œæˆ")

# æ·»åŠ ç¼ºå¤±çš„è¾…åŠ©å‡½æ•°
def ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ç¡®ä¿ç»“æœè¡¨å…·å¤‡ç»Ÿä¸€åˆ—ï¼šç¼ºå¤±åˆ™ä»¥ç©ºå€¼è¡¥é½ï¼Œä¸ç§»é™¤å·²æœ‰åˆ—ã€‚"""
    if df is None or df.empty:
        return pd.DataFrame(columns=REQUIRED_OUTPUT_COLUMNS)
    
    for col in REQUIRED_OUTPUT_COLUMNS:
        if col not in df.columns:
            df[col] = ""
    return df

def reorder_columns(df: pd.DataFrame, df_input_columns: list) -> pd.DataFrame:
    """å°†åˆ—é¡ºåºæ•´ç†ä¸ºï¼šè¾“å…¥è¡¨åŸæœ‰åˆ—åœ¨å‰ + REQUIRED_OUTPUT_COLUMNSï¼ˆå»é‡ä¿åºï¼‰"""
    if df is None or df.empty:
        return df
    
    preferred = list(dict.fromkeys(list(df_input_columns) + REQUIRED_OUTPUT_COLUMNS))
    # ä¿ç•™è¡¨å†…å·²æœ‰çš„åˆ—ï¼Œä¸”æŒ‰ç…§ preferred é¡ºåºé‡æ’
    ordered_existing = [c for c in preferred if c in df.columns]
    # è¿½åŠ ä»»ä½•æœªåœ¨ preferred ä¸­ä½†å­˜åœ¨äº df çš„åˆ—ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
    tail = [c for c in df.columns if c not in preferred]
    return df.reindex(columns=ordered_existing + tail)

def get_processing_state(df, id_col):
    """ç»Ÿä¸€çš„çŠ¶æ€æ£€æŸ¥ï¼šè¿”å›(å®Œå…¨æˆåŠŸIDé›†åˆ, æœ‰å¤±è´¥çš„IDé›†åˆ)"""
    if df is None or df.empty or id_col not in df.columns:
        return set(), set()
    
    status_col = 'processing_status'
    try:
        if status_col in df.columns:
            # åŸºäºæ–‡ç« ç»´åº¦åˆ¤æ–­çŠ¶æ€
            fully_successful_ids = set()
            has_failed_ids = set()
            
            # æŒ‰æ–‡ç« IDåˆ†ç»„ç»Ÿè®¡çŠ¶æ€
            for article_id in df[id_col].unique():
                article_records = df[df[id_col] == article_id]
                statuses = article_records[status_col].tolist()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥è®°å½•
                if ProcessingStatus.API_FAILED in statuses:
                    has_failed_ids.add(str(article_id))
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è®°å½•éƒ½æ˜¯æˆåŠŸæˆ–æ— ç›¸å…³
                elif all(s in [ProcessingStatus.SUCCESS, ProcessingStatus.NO_RELEVANT] for s in statuses):
                    fully_successful_ids.add(str(article_id))
                
            return fully_successful_ids, has_failed_ids
        else:
            # å…¼å®¹æ—§ç‰ˆ
            speaker_col = 'speaker'
            if speaker_col in df.columns:
                success = df[~df[speaker_col].astype(str).str.contains('API_CALL_FAILED', na=False)][id_col]
                failed = df[df[speaker_col].astype(str).str.contains('API_CALL_FAILED', na=False)][id_col]
                return set(success.astype(str)), set(failed.astype(str))
            else:
                return set(), set()
        
    except Exception:
        return set(), set()

def build_resume_plan(output_file_path: str, df_input: pd.DataFrame, id_col: str):
    """æ„å»ºä¸¤é˜¶æ®µæ¨¡å‹çš„æ–­ç‚¹ç»­ä¼ è®¡åˆ’"""
    all_input_ids = set(df_input[id_col].astype(str)) if (df_input is not None and id_col in df_input.columns) else set()

    never_processed_ids = set(all_input_ids)
    failed_unit_ids = {}

    if os.path.exists(output_file_path):
        try:
            df_existing = pd.read_excel(output_file_path)
            if not df_existing.empty and id_col in df_existing.columns:
                processed_article_ids = set(df_existing[id_col].astype(str).unique())
                never_processed_ids = all_input_ids - processed_article_ids

                if 'processing_status' in df_existing.columns:
                    # æ‰¾å‡ºå¤±è´¥çš„è®®é¢˜å•å…ƒ
                    failed_records = df_existing[df_existing['processing_status'] == ProcessingStatus.API_FAILED]
                    for _, record in failed_records.iterrows():
                        article_id = str(record.get(id_col, '')).strip()
                        unit_id = str(record.get('Unit_ID', '')).strip()
                        
                        if article_id and unit_id:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¬ä¸€é˜¶æ®µå¤±è´¥ï¼ˆæ•´ç¯‡æ–‡ç« çº§åˆ«ï¼‰
                            if 'ç¬¬ä¸€é˜¶æ®µ' in str(record.get('Unit_Text', '')) or 'è®®é¢˜å•å…ƒæå–å¤±è´¥' in str(record.get('Unit_Text', '')):
                                # ç¬¬ä¸€é˜¶æ®µå¤±è´¥ï¼Œéœ€è¦é‡æ–°å¤„ç†æ•´ç¯‡æ–‡ç« 
                                never_processed_ids.add(article_id)
                            else:
                                # ç¬¬äºŒé˜¶æ®µå¤±è´¥ï¼Œè®°å½•å…·ä½“çš„Unit_ID
                                s = failed_unit_ids.setdefault(article_id, set())
                                s.add(unit_id)
        except Exception as e:
            UX.warn(f"è¯»å–å·²å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")

    return never_processed_ids, failed_unit_ids

def check_failed_records(output_path: str) -> tuple:
    """æ£€æŸ¥è¾“å‡ºç›®å½•ä¸­æ˜¯å¦å­˜åœ¨APIå¤±è´¥è®°å½•"""
    failed_files = []
    total_failed_count = 0
    
    try:
        # æŸ¥æ‰¾æ‰€æœ‰analyzedç»“æœæ–‡ä»¶
        analyzed_files = glob.glob(os.path.join(output_path, "*analyzed_*.xlsx"))
        
        for file_path in analyzed_files:
            try:
                df = pd.read_excel(file_path)
                if not df.empty and 'processing_status' in df.columns:
                    failed_count = (df['processing_status'] == ProcessingStatus.API_FAILED).sum()
                    if failed_count > 0:
                        failed_files.append({
                            'file': os.path.basename(file_path),
                            'failed_count': failed_count
                        })
                        total_failed_count += failed_count
            except Exception as e:
                UX.warn(f"æ£€æŸ¥æ–‡ä»¶å¤±è´¥è®°å½•æ—¶å‡ºé”™ {file_path}: {e}")
        
        return total_failed_count, failed_files
    
    except Exception as e:
        UX.err(f"æ£€æŸ¥å¤±è´¥è®°å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return 0, []

# ç¨‹åºå…¥å£
if __name__ == "__main__":
    asyncio.run(main_async())