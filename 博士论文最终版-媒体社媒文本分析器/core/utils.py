# -*- coding: utf-8 -*-
"""
Utility functions and classes for the media and social media text analysis system.
"""

import os
import re
import time
import yaml
import tiktoken
import pandas as pd
import contextlib
from datetime import datetime

# ==============================================================================
# å¤„ç†çŠ¶æ€å¸¸é‡ç±» - åŸºç¡€ç»„ä»¶ï¼Œä¸ä¾èµ–å…¶ä»–æ¨¡å—
# ==============================================================================

class ProcessingStatus:
    """å¤„ç†çŠ¶æ€å¸¸é‡"""
    SUCCESS = "SUCCESS"
    NO_RELEVANT = "NO_RELEVANT"
    API_FAILED = "API_FAILED"

# åˆ†æåˆ—å¸¸é‡ - ä»processors.pyç§»è‡³æ­¤å¤„
ANALYSIS_COLUMNS = [
    'Incident', 'Frame_SolutionRecommendation', 'Frame_ResponsibilityAttribution',
    'Frame_CausalExplanation', 'Frame_MoralEvaluation', 'Frame_ProblemDefinition',
    'Frame_ActionStatement', 'Valence', 'Evidence_Type', 'Attribution_Level',
    'Temporal_Focus', 'Primary_Actor_Type', 'Geographic_Scope',
    'Relationship_Model_Definition', 'Discourse_Type'
]

class UX:
    @staticmethod
    def _fmt(msg):
        return str(msg).strip()
    
    # è¿è¡Œçº§è®¡æ—¶èµ·ç‚¹
    RUN_T0 = time.perf_counter()
    LAST_LOG_TIME = time.perf_counter()  # æœ€åä¸€æ¬¡æ—¥å¿—æ—¶é—´
    
    @staticmethod
    def start_run():
        UX.RUN_T0 = time.perf_counter()
        UX.LAST_LOG_TIME = time.perf_counter()

    @staticmethod
    def _ts():
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    @staticmethod
    def _elapsed():
        return time.perf_counter() - UX.RUN_T0
    
    @staticmethod
    def _elapsed_str():
        total = int(UX._elapsed())
        days, rem = divmod(total, 86400)
        hours, rem = divmod(rem, 3600)
        minutes, seconds = divmod(rem, 60)
        if days > 0:
            return f"{days}å¤©{hours:02d}:{minutes:02d}:{seconds:02d}"
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
    
    @staticmethod
    def _update_log_time():
        """æ›´æ–°æœ€åæ—¥å¿—æ—¶é—´"""
        UX.LAST_LOG_TIME = time.perf_counter()
    
    @staticmethod
    def check_activity():
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è¾“å‡ºæ´»åŠ¨æé†’"""
        if time.perf_counter() - UX.LAST_LOG_TIME > 300:  # 5åˆ†é’Ÿ = 300ç§’
            print(f"[{UX._ts()}][{UX._elapsed_str()}] [â³] ç®—æ³•ä»åœ¨è¿è¡Œä¸­...")
            UX.LAST_LOG_TIME = time.perf_counter()
    
    @staticmethod
    def phase(title):
        print(f"\n=== [{UX._ts()}][{UX._elapsed_str()}] {UX._fmt(title)} ===")
        UX._update_log_time()
    
    @staticmethod
    def resume_plan(title):
        """æ–­ç‚¹ç»­ä¼ ä¸“ç”¨æ ¼å¼ï¼Œæ›´æ˜¾çœ¼"""
        print(f"\n" + "="*100)
        print("="*100)
        print(f"=== ğŸ“‹ æ–­ç‚¹ç»­ä¼ è®¡åˆ’ - {UX._fmt(title)} ===")
        print("="*100)
        UX._update_log_time()
    
    @staticmethod
    def resume_end():
        """æ–­ç‚¹ç»­ä¼ ç»“æŸ"""
        print("="*100)
        print("="*100 + "\n")
        UX._update_log_time()
    
    @staticmethod
    def info(msg):
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [i] {UX._fmt(msg)}")
        UX._update_log_time()
    
    @staticmethod
    def ok(msg):
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [OK] {UX._fmt(msg)}")
        UX._update_log_time()
    
    @staticmethod
    def warn(msg):
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [!] {UX._fmt(msg)}")
        UX._update_log_time()
    
    @staticmethod
    def err(msg):
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [!!!] {UX._fmt(msg)}")
        UX._update_log_time()
    
    @staticmethod
    def api_failed(stage, error_brief):
        """APIå¤±è´¥ç®€è¦æ—¥å¿—"""
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [âŒ] {stage}: {error_brief}")
        UX._update_log_time()
    
    @staticmethod
    @contextlib.contextmanager
    def timer(label):
        t0 = time.perf_counter()
        UX.info(f"{label} å¼€å§‹")
        try:
            yield
        finally:
            dt = time.perf_counter() - t0
            total = int(dt)
            h, rem = divmod(total, 3600)
            m, s = divmod(rem, 60)
            UX.ok(f"{label} å®Œæˆï¼Œç”¨æ—¶ {h:02d}:{m:02d}:{s:02d}")


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config.yaml')
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # ç¯å¢ƒå˜é‡å…œåº•è¦†ç›–
        env_api_key = os.getenv("OPENAI_API_KEY") or os.getenv("API_KEY")
        if env_api_key:
            try:
                if isinstance(config.get("api_config", {}).get("API_KEYS"), list) and config["api_config"]["API_KEYS"]:
                    config["api_config"]["API_KEYS"][0] = env_api_key
                else:
                    config["api_config"]["API_KEYS"] = [env_api_key]
            except Exception:
                pass
        
        env_base_url = os.getenv("OPENAI_BASE_URL") or os.getenv("API_BASE_URL")
        if env_base_url:
            try:
                config["api_config"]["BASE_URL"] = env_base_url.rstrip("/")
            except Exception:
                pass
        
        return config
    except Exception as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return None


# Tokenè®¡ç®—å™¨åˆå§‹åŒ–
_tokenizer = None

def get_tokenizer():
    """è·å–tokenè®¡ç®—å™¨ï¼Œä½¿ç”¨å•ä¾‹æ¨¡å¼"""
    global _tokenizer
    if _tokenizer is None:
        try:
            _tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            UX.err(f"åˆå§‹åŒ–tokenizerå¤±è´¥: {e}")
            raise
    return _tokenizer

def count_tokens(text: str) -> int:
    """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
    if not isinstance(text, str) or not text.strip():
        return 0
    
    try:
        tokenizer = get_tokenizer()
        return len(tokenizer.encode(text))
    except Exception as e:
        UX.warn(f"è®¡ç®—tokenæ•°å¤±è´¥: {e}")
        # é™çº§ç­–ç•¥ï¼šä½¿ç”¨å­—ç¬¦æ•°çš„ç²—ç•¥ä¼°ç®—
        return len(text) // 2  # ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 2 å­—ç¬¦


def safe_str_convert(value):
    if value is None:
        return ''
    if isinstance(value, pd.Series):
        if value.empty:
            return ''
        return str(value.iloc[0])
    if pd.isna(value):
        return ''
    return str(value)


def normalize_text(text: str) -> str:
    if not isinstance(text, str):
        text = safe_str_convert(text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def detect_language_and_get_config(text):
    if re.search(r'[\u4e00-\u9fa5]', text):
        return 'zh'
    elif re.search(r'[\u0400-\u04FF]', text):
        return 'ru'
    else:
        return 'en'


def identify_source(filename):
    source_map = {
        'ä¿„æ€»ç»Ÿ': ['ä¿„æ€»ç»Ÿ', 'æ€»ç»Ÿ', 'Putin', 'president'],
        'ä¿„è¯­åª’ä½“': ['ä¿„è¯­åª’ä½“', 'ä¿„è¯­', 'russian', 'ru_media', 'ä¿„åª’'],
        'ä¸­æ–‡åª’ä½“': ['ä¸­æ–‡åª’ä½“', 'ä¸­æ–‡', 'chinese', 'cn_media', 'ä¸­åª’'],
        'è‹±è¯­åª’ä½“': ['è‹±è¯­åª’ä½“', 'è‹±è¯­', 'english', 'en_media', 'è‹±åª’'],
        'vk': ['vk'],
        'çŸ¥ä¹': ['çŸ¥ä¹', 'zhihu']
    }

    filename_lower = filename.lower()
    for source, keywords in source_map.items():
        if any(kw.lower() in filename_lower for kw in keywords):
            UX.info(f"æ–‡ä»¶ {filename} è¯†åˆ«ä¸º: {source}")
            return source

    UX.warn(f"æ–‡ä»¶ {filename} æ— æ³•è¯†åˆ«ä¿¡æºï¼Œæ ‡è®°ä¸º: æœªçŸ¥æ¥æº")
    return 'æœªçŸ¥æ¥æº'

# ==============================================================================
# æ•°æ®å¤„ç†è¾…åŠ©å‡½æ•° - ä» processors.py ç§»è‡³æ­¤å¤„
# ==============================================================================

# get_language_configå‡½æ•°å·²åˆ é™¤ï¼Œè¯·ä»config.yamlè·å–è¯­è¨€é…ç½®

def get_processing_state(df, id_col):
    """ç»Ÿä¸€çš„çŠ¶æ€æ£€æŸ¥ï¼šè¿”å›(å®Œå…¨æˆåŠŸIDé›†åˆ, æœ‰å¤±è´¥çš„IDé›†åˆ)"""
    # ProcessingStatusç°åœ¨åœ¨æœ¬æ–‡ä»¶ä¸­å®šä¹‰
    
    if df is None or df.empty or id_col not in df.columns:
        return set(), set()
    
    status_col = 'processing_status'
    try:
        if status_col in df.columns:
            # åŸºäºæ–‡ç« ç»´åº¦åˆ¤æ–­çŠ¶æ€
            fully_successful_ids = set()
            has_failed_ids = set()
            
            # æŒ‰æ–‡ç« IDåˆ†ç»„ç»Ÿè®¡çŠ¶æ€
            for article_id in df[id_col].unique():
                article_records = df[df[id_col] == article_id]
                statuses = article_records[status_col].tolist()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥è®°å½•
                if ProcessingStatus.API_FAILED in statuses:
                    has_failed_ids.add(str(article_id))
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è®°å½•éƒ½æ˜¯æˆåŠŸæˆ–æ— ç›¸å…³
                elif all(s in [ProcessingStatus.SUCCESS, ProcessingStatus.NO_RELEVANT] for s in statuses):
                    fully_successful_ids.add(str(article_id))
                
            return fully_successful_ids, has_failed_ids
        else:
            # å…¼å®¹æ—§ç‰ˆ
            speaker_col = 'speaker'
            if speaker_col in df.columns:
                success = df[~df[speaker_col].astype(str).str.contains('API_CALL_FAILED', na=False)][id_col]
                failed = df[df[speaker_col].astype(str).str.contains('API_CALL_FAILED', na=False)][id_col]
                return set(success.astype(str)), set(failed.astype(str))
            else:
                return set(), set()
        
    except Exception:
        return set(), set()

def clean_failed_records(output_path, id_column):
    """æ¸…ç†å¤±è´¥è®°å½•"""
    # ProcessingStatusç°åœ¨åœ¨æœ¬æ–‡ä»¶ä¸­å®šä¹‰
    
    if not os.path.exists(output_path):
        return set()

    try:
        df = pd.read_excel(output_path)
        if df.empty or id_column not in df.columns:
            return set()

        if 'processing_status' in df.columns:
            failed_mask = df['processing_status'] == ProcessingStatus.API_FAILED
        else:
            failed_mask = df['speaker'].astype(str).str.contains('API_CALL_FAILED', na=False) if 'speaker' in df.columns else pd.Series([False] * len(df))

        failed_ids = set(df[failed_mask][id_column].astype(str).unique())
        if failed_ids:
            df_clean = df[~failed_mask]
            df_clean.to_excel(output_path, index=False)

            if 'processing_status' in df_clean.columns:
                success_count = (df_clean['processing_status'] == ProcessingStatus.SUCCESS).sum()
                no_relevant_count = (df_clean['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                UX.info(f"ä¿ç•™äº† {success_count} æ¡æˆåŠŸè®°å½•, {no_relevant_count} æ¡æ— ç›¸å…³è®°å½•")

            UX.info(f"æ¸…ç†äº† {len(failed_ids)} ä¸ªAPIå¤±è´¥è®°å½•ï¼Œå‰©ä½™ {len(df_clean)} æ¡")

        return failed_ids

    except Exception as e:
        UX.warn(f"æ¸…ç†å¤±è´¥è®°å½•æ—¶å‡ºé”™: {e}")
        return set()

def create_unified_record(record_type, original_id, source="æœªçŸ¥æ¥æº", text_snippet="", failure_reason=""):
    """åˆ›å»ºç»Ÿä¸€è®°å½•"""
    # ProcessingStatusç°åœ¨åœ¨æœ¬æ–‡ä»¶ä¸­å®šä¹‰
    
    base_record = {
        "processing_status": record_type,
        "Source": source,
        "Unit_ID": f"{original_id}-{record_type}"
    }

    if record_type == ProcessingStatus.NO_RELEVANT:
        return {
            **base_record,
            "speaker": "NO_RELEVANT_CONTENT",
            "Unit_Text": "[æ— ç›¸å…³å†…å®¹]",
            "Incident": "",
            "Frame_SolutionRecommendation": "",
            "Frame_ResponsibilityAttribution": "",
            "Frame_CausalExplanation": "",
            "Frame_MoralEvaluation": "",
            "Frame_ProblemDefinition": "",
            "Frame_ActionStatement": "",
            "Valence": "",
            "Evidence_Type": "",
            "Attribution_Level": "",
            "Temporal_Focus": "",
            "Primary_Actor_Type": "",
            "Geographic_Scope": "",
            "Relationship_Model_Definition": "",
            "Discourse_Type": ""
        }

    elif record_type == ProcessingStatus.API_FAILED:
        return {
            **base_record,
            "speaker": "API_CALL_FAILED",
            "Unit_Text": f"[API_FAILED] {failure_reason}: {text_snippet[:200]}...",
            "Incident": "API_CALL_FAILED",
            "Frame_SolutionRecommendation": "[]",
            "Frame_ResponsibilityAttribution": "[]",
            "Frame_CausalExplanation": "[]",
            "Frame_MoralEvaluation": "[]",
            "Frame_ProblemDefinition": "[]",
            "Frame_ActionStatement": "[]",
            "Valence": "API_CALL_FAILED",
            "Evidence_Type": "API_CALL_FAILED",
            "Attribution_Level": "API_CALL_FAILED",
            "Temporal_Focus": "API_CALL_FAILED",
            "Primary_Actor_Type": "API_CALL_FAILED",
            "Geographic_Scope": "API_CALL_FAILED",
            "Relationship_Model_Definition": "API_CALL_FAILED",
            "Discourse_Type": "API_CALL_FAILED"
        }

def detect_language_and_get_config(text, config=None):
    """æ£€æµ‹è¯­è¨€å¹¶è·å–é…ç½®"""
    import re
    
    # é»˜è®¤é…ç½®
    default_configs = {
        'zh': {'MAX_SINGLE_TEXT': 50000},
        'ru': {'MAX_SINGLE_TEXT': 50000}, 
        'en': {'MAX_SINGLE_TEXT': 50000}
    }
    
    # ä½¿ç”¨æä¾›çš„é…ç½®æˆ–é»˜è®¤é…ç½®
    language_configs = config.get('LANGUAGE_CONFIGS', default_configs) if config else default_configs
    
    if re.search(r'[\u4e00-\u9fa5]', text):
        return 'zh', language_configs.get('zh', default_configs['zh'])
    elif re.search(r'[\u0400-\u04FF]', text):
        return 'ru', language_configs.get('ru', default_configs['ru'])
    else:
        return 'en', language_configs.get('en', default_configs['en'])
