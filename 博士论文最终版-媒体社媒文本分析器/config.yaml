# ==============================================================================
# 共用配置 - API、信度检验、通用处理
# ==============================================================================

# API密钥和基础URL
api:
  credentials:
    keys:
      - "sk-oS1mPWSBCvf5czQQ1CyRXRGWOWQ4CFWis8qNV0UlEqbQYzoH"  # 主API密钥
    base_url: "https://openai.sharkmagic.com.cn/v1"  # API服务地址

  # API请求参数
  request_params:
    temperature: 0  # 生成随机性控制
    response_format:
      type: "json_object"  # 响应格式

  # 重试与并发策略
  strategy:
    max_concurrent_requests: 2      # 最大并发请求数
    attempts_per_model: 3           # 每个模型重试次数
    max_model_switches: 2           # 最大模型切换次数（主模型3次→备用模型3次→放弃）
    retry_delays_sec: [2, 5, 10]    # 重试延迟时间（秒）
    queue_timeout_sec: 30.0         # 队列超时时间

  # 动态超时配置（基于token数量）
  timeout:
    token_threshold_short: 2000     # 短文本token阈值
    token_threshold_medium: 4000    # 中等文本token阈值
    timeout_short_sec: 400          # 短文本超时时间
    timeout_medium_sec: 500         # 中等文本超时时间
    timeout_long_sec: 500           # 长文本超时时间

# 信度检验配置
reliability_test:
  enabled: true  # 是否启用信度检验
  sampling_config:
    # 媒体文本信源抽样配置（准确率检验样本数, 召回率检验样本数）
    俄总统: { precision: 80, recall: 40 }
    俄语媒体: { precision: 80, recall: 40 }
    中文媒体: { precision: 80, recall: 40 }
    英语媒体: { precision: 80, recall: 40 }
    未知来源: { precision: 80, recall: 40 } 
    # 社交媒体信源抽样配置
    vk: { precision: 0, recall: 50 }    # VK只检验召回率
    zhihu: { precision: 0, recall: 0 }  # 知乎暂不配置信度检验

# 通用数据处理配置
processing:
  # 自动重试配置（通用）
  auto_retry:
    enabled: true             # 启用自动重试
    max_rounds: 5            # 最大重试轮数
    delay_minutes: 2         # 重试间隔（分钟）
    min_failed_threshold: 1  # 触发重试的最小失败数

  general:
    skip_on_api_failure: true   # API失败时是否跳过
    buffer_limit: 20           # 结果保存缓冲区大小
    # 数据质量验证阈值
    quality_thresholds:
      text_similarity_threshold: 0.7    # 文本相似度匹配阈值
      token_char_ratio: 2               # Token与字符数的粗略比例
    # 多语言处理配置
    language_configs:
      zh:  # 中文
        max_single_text: 50000
        timeout: 600               # 知乎使用更长超时
        max_concurrent: 2
        batch_size_limit: 100
        save_interval: 20
      ru:  # 俄语
        max_single_text: 50000
        timeout: 180
        max_concurrent: 2
        batch_size_limit: 100
        save_interval: 20
      en:  # 英语
        max_single_text: 50000
        timeout: 240
        max_concurrent: 6          # 英语配置更高并发
        batch_size_limit: 30
        save_interval: 10

# ==============================================================================
# 媒体文本分析专用配置
# ==============================================================================
media_text:
  # 文件路径
  paths:
    input: "C:\\Users\\87010\\Desktop\\媒体文本议题单元划分素材"   # 输入文件夹
    output: "C:\\Users\\87010\\Desktop\\媒体文本议题单元划分结果"  # 输出文件夹
  

  # 模型池配置（包含备用模型）
  model_pools:
    # 主模型配置：两阶段处理模型
    primary_models:
      UNIT_EXTRACTION: "[官自-3]gemini-2-5-pro"               # 第一阶段：议题单元提取
      UNIT_ANALYSIS: "[官自-0.7]gemini-2-5-flash"             # 第二阶段：单元深度分析
    # 备用模型池：主模型失败时的降级选择
    fallback_models:
      UNIT_EXTRACTION:
        - "[羊毛-2]gemini-2.5-pro-0605-100k"                            
      UNIT_ANALYSIS:
        - ""  # 无备用模型

  # 列名映射
  column_mapping:
    ID: "序号"           # 文章ID列名
    MEDIA_TITLE: "标题"  # 文章标题列名
    MEDIA_TEXT: "text"   # 文章正文列名

# ==============================================================================
# 社交媒体分析专用配置
# ==============================================================================
social_media:
  # 文件路径
  paths:
    input: "C:\\Users\\87010\\Desktop\\社交媒体议题单元划分素材"   # 输入文件夹
    output: "C:\\Users\\87010\\Desktop\\社交媒体议题单元划分结果"  # 输出文件夹
  
  # 文本长度阈值配置
  text_length_thresholds:
    vk_long_text: 1500        # VK长文本阈值，超过此长度使用高性能模型
    zhihu_short_text: 100     # 知乎短文本阈值，低于此长度直接分析
    zhihu_long_text: 1300     # 知乎长文本阈值，超过此长度使用高性能模型

  # 社交媒体专用模型配置（支持备用模型）
  model_pools:
    # 主模型配置
    primary_models:
      # VK评论处理流程：
      # ├── 计算 post_text + comments_json 的总token数
      # ├── ≤2000 tokens → 使用 VK_BATCH （较高质量模型）
      # └── >2000 tokens → 使用 VK_BATCH_LONG （高性能模型）
      VK_BATCH: "[官自-0.7]gemini-2-5-flash"           # VK批处理：使用较高质量模型
      VK_BATCH_LONG: "[官自-3]gemini-2-5-pro"          # VK长文本批处理：使用高性能模型
      
      # 知乎回答处理流程：
      # 短文本直接分析模式：
      # ├── 计算answer_text的token数
      # ├── <100 tokens → 使用 ZHIHU_ANALYSIS_SHORT （轻量快速模型）
      # └── ≥100 tokens → 使用两步式分析模式
      # 长文本两步式分析模式：
      # ├── 第一步：议题单元划分（使用 ZHIHU_CHUNKING 轻量模型）
      # └── 第二步：议题单元分析（根据单元长度选择）：
      #     ├── <100 tokens → 使用 ZHIHU_ANALYSIS_SHORT （轻量快速模型）
      #     ├── 100-1300 tokens → 使用 ZHIHU_ANALYSIS （较高质量模型）
      #     └── >1300 tokens → 使用 ZHIHU_ANALYSIS_LONG （高性能模型）
      ZHIHU_CHUNKING: "[官自-0.7]gemini-2-5-flash"     # 知乎分块：使用较高质量模型
      ZHIHU_ANALYSIS_SHORT: "[官自-0.3]gemini-2-5-flash-lite"  # 知乎短文本分析：使用轻量快速模型
      ZHIHU_ANALYSIS: "[官自-0.7]gemini-2-5-flash"     # 知乎标准分析：使用较高质量模型
      ZHIHU_ANALYSIS_LONG: "[官自-3]gemini-2-5-pro"    # 知乎长文本分析：使用高性能模型
    
    # 备用模型配置（主模型失败时的降级选择）
    fallback_models:
      VK_BATCH:
        - "[羊毛-2]gemini-2.5-pro-0605-100k"          # VK批处理备用模型
      VK_BATCH_LONG:
        - ""                                          # VK长文本暂无备用模型
      ZHIHU_CHUNKING:
        - "[羊毛-2]gemini-2.5-pro-0605-100k"          # 知乎分块备用模型
      ZHIHU_ANALYSIS_SHORT:
        - "[官自-0.7]gemini-2-5-flash"                # 知乎短文本备用模型
      ZHIHU_ANALYSIS:
        - "[羊毛-2]gemini-2.5-pro-0605-100k"          # 知乎标准分析备用模型
      ZHIHU_ANALYSIS_LONG:
        - ""                                          # 知乎长文本暂无备用模型

  # 列名映射配置
  column_mapping:
    vk:  # VK平台数据列名映射
      post_id: "post_id"           # 帖子ID列
      post_text: "post_text"       # 帖子内容列
      comment_id: "comment_id"     # 评论ID列
      comment_text: "comment_text" # 评论内容列
      channel_name: "channel_name" # 频道名称列
    zhihu:  # 知乎平台数据列名映射
      id: "序号"                    # 回答序号列
      question: "知乎问题标题及描述"  # 问题内容列
      answer_text: "回答内容"       # 回答内容列
      author: "回答用户名"           # 回答作者列

# ==============================================================================
# 统一输出列定义
# ==============================================================================
required_output_columns:
  # 基础信息列
  - "序号"      # 文章/回答唯一标识
  - "日期"      # 发布日期
  - "标题"      # 文章标题
  - "token数"   # 文章token数统计（媒体文本专用）
  - "text"      # 文章正文内容
  - "来源类型"   # 来源分类标识
  
  # 议题单元信息列
  - "Unit_ID"                    # 议题单元唯一标识
  - "Source"                     # 信源标识（俄总统/俄语媒体/中文媒体/英语媒体/未知来源）
  - "speaker"                    # 发言人/信源标准化标识
  - "Unit_Text"                  # 议题单元原文
  - "seed_sentence"              # 种子句子（用于单元构建的锚点）
  - "expansion_logic"            # 单元构建逻辑报告
  - "Unit_Hash"                  # 单元文本哈希值（用于去重）
  - "processing_status"          # 处理状态（SUCCESS/NO_RELEVANT/API_FAILED）

  # 框架分析列
  - "Incident"                   # 核心事件、观点或行动的一句话概括
  - "Frame_SolutionRecommendation"   # 方案建议（优先级1）
  - "Frame_ResponsibilityAttribution" # 归因指责（优先级2）
  - "Frame_CausalExplanation"        # 因果解释（优先级3）
  - "Frame_MoralEvaluation"          # 道德评价（优先级4）
  - "Frame_ProblemDefinition"        # 问题建构（优先级5）
  - "Frame_ActionStatement"          # 事实宣称（优先级6）

  # 分析维度列
  - "Valence"                           # 情感极性（正面/负面/中立/事实陈述）
  - "Evidence_Type"                     # 证据类型（数据/统计/官方声明/专家观点等）
  - "Attribution_Level"                 # 归因层级（个体/国家/社会/系统/不适用）
  - "Temporal_Focus"                    # 时间焦点（追溯/现状/展望/混合）
  - "Primary_Actor_Type"                # 主要行动者类型（国家/个人/次国家/国际组织等）
  - "Geographic_Scope"                  # 地理范围（双边/区域/全球/国内/混合）
  - "Relationship_Model_Definition"     # 关系模式定义（新型关系/传统伙伴/利益关系/未界定）
  - "Discourse_Type"                    # 语段类型（经验性断言/规范性断言/展演性语段）

# 项目全局配置
project:
  random_seed: 2025  # 随机种子，确保实验可复现