# -*- coding: utf-8 -*-
"""
==============================================================================
é•¿æ–‡æœ¬åˆ†æå™¨ - æ–°é—»ä¸“ç”¨å¼•æ“ (ç§å­æ‰©å±•ä¸€ä½“åŒ–+è®®é¢˜å•å…ƒï¼‰
==============================================================================
"""
import os
import re
import glob
import json
import time
import asyncio
import aiohttp
import pandas as pd
import hashlib
import yaml
import tiktoken
from datetime import datetime
from tqdm.asyncio import tqdm as aio_tqdm
from asyncio import Lock
import contextlib

class UX:
    @staticmethod
    def _fmt(msg):
        return str(msg).strip()
    
    # è¿è¡Œçº§è®¡æ—¶èµ·ç‚¹
    RUN_T0 = time.perf_counter()
    
    @staticmethod
    def start_run():
        UX.RUN_T0 = time.perf_counter()

    @staticmethod
    def _ts():
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    @staticmethod
    def _elapsed():
        return time.perf_counter() - UX.RUN_T0
    
    @staticmethod
    def _elapsed_str():
        total = int(UX._elapsed())
        days, rem = divmod(total, 86400)
        hours, rem = divmod(rem, 3600)
        minutes, seconds = divmod(rem, 60)
        if days > 0:
            return f"{days}å¤©{hours:02d}:{minutes:02d}:{seconds:02d}"
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
    
    @staticmethod
    def phase(title):
        print(f"\n=== [{UX._ts()}][{UX._elapsed_str()}] {UX._fmt(title)} ===")
    
    @staticmethod
    def info(msg):
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [i] {UX._fmt(msg)}")
    
    @staticmethod
    def ok(msg):
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [OK] {UX._fmt(msg)}")
    
    @staticmethod
    def warn(msg):
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [!] {UX._fmt(msg)}")
    
    @staticmethod
    def err(msg):
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [!!!] {UX._fmt(msg)}")
    
    @staticmethod
    @contextlib.contextmanager
    def timer(label):
        t0 = time.perf_counter()
        UX.info(f"{label} å¼€å§‹")
        try:
            yield
        finally:
            dt = time.perf_counter() - t0
            total = int(dt)
            h, rem = divmod(total, 3600)
            m, s = divmod(rem, 60)
            UX.ok(f"{label} å®Œæˆï¼Œç”¨æ—¶ {h:02d}:{m:02d}:{s:02d}")

# ==============================================================================
# === ğŸ›ï¸ é…ç½®åŠ è½½åŒº ================================
# ==============================================================================
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.path.dirname(__file__), 'config.yaml')
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # ç¯å¢ƒå˜é‡å…œåº•è¦†ç›–
        env_api_key = os.getenv("OPENAI_API_KEY") or os.getenv("API_KEY")
        if env_api_key:
            try:
                if isinstance(config.get("api_config", {}).get("API_KEYS"), list) and config["api_config"]["API_KEYS"]:
                    config["api_config"]["API_KEYS"][0] = env_api_key
                else:
                    config["api_config"]["API_KEYS"] = [env_api_key]
            except Exception:
                pass
        
        env_base_url = os.getenv("OPENAI_BASE_URL") or os.getenv("API_BASE_URL")
        if env_base_url:
            try:
                config["api_config"]["BASE_URL"] = env_base_url.rstrip("/")
            except Exception:
                pass
        
        return config
    except Exception as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return None

# åŠ è½½é…ç½®
CONFIG = load_config()
if CONFIG is None:
    raise RuntimeError("æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")

# ä»é…ç½®ä¸­æå–å˜é‡ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
INPUT_PATH = CONFIG['file_paths']['input']
OUTPUT_PATH = CONFIG['file_paths']['output']
RELIABILITY_TEST_MODE = CONFIG['reliability_test']['enabled']
RELIABILITY_SAMPLING_CONFIG = CONFIG['reliability_test']['sampling_config']
LANGUAGE_CONFIGS = CONFIG['LANGUAGE_CONFIGS']
SKIP_FAILED_TEXTS = CONFIG['data_processing']['skip_failed_texts']
API_CONFIG = CONFIG['api_config']
COLUMN_MAPPING = CONFIG['column_mapping']
REQUIRED_OUTPUT_COLUMNS = CONFIG['required_output_columns']

# ä»ç®€åŒ–çš„é…ç½®ç»“æ„ä¸­æå–
API_RETRY_CONFIG = CONFIG.get('api_retry_config', {})
MODEL_POOLS = CONFIG.get('model_pools', {})

# å…·ä½“å€¼
MAX_CONCURRENT_REQUESTS = API_RETRY_CONFIG.get('max_concurrent_requests', 1)
API_RETRY_ATTEMPTS = API_RETRY_CONFIG.get('attempts_per_model', 3)
RETRY_DELAYS = API_RETRY_CONFIG.get('retry_delays', [2, 5, 10])
MAX_MODEL_SWITCHES = API_RETRY_CONFIG.get('max_model_switches', 10)
QUEUE_TIMEOUT = API_RETRY_CONFIG.get('queue_timeout', 30.0)

# å…¶ä»–é…ç½®é¡¹
BUFFER_CONFIG = CONFIG.get('buffer_config', {})
API_REQUEST_PARAMS = CONFIG.get('api_request_params', {})
QUALITY_THRESHOLDS = CONFIG.get('data_processing', {}).get('quality_thresholds', {})
RANDOMIZATION_CONFIG = CONFIG.get('randomization', {})
SHORT_TEXT_THRESHOLD = CONFIG.get('data_processing', {}).get('SHORT_TEXT_THRESHOLD', 100)
LONG_TEXT_THRESHOLD = CONFIG.get('data_processing', {}).get('LONG_TEXT_THRESHOLD', 1200)

# æ‰¹å¤„ç†é…ç½®
ENABLE_BATCH_PROCESSING = CONFIG.get('data_processing', {}).get('enable_batch_processing', False)
BATCH_SIZE = CONFIG.get('data_processing', {}).get('batch_size', 5)

def ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ç¡®ä¿ç»“æœè¡¨å…·å¤‡ç»Ÿä¸€åˆ—ï¼šç¼ºå¤±åˆ™ä»¥ç©ºå€¼è¡¥é½ï¼Œä¸ç§»é™¤å·²æœ‰åˆ—ã€‚è¯¥æ“ä½œæ˜¯æ— ç ´åæ€§çš„ï¼šä»…æ·»åŠ ç¼ºåˆ—ï¼Œé¿å…åˆå¹¶æ—¶åˆ—è¢«ä¸¢å¤±ã€‚"""
    if df is None or df.empty:
        # å³ä½¿ç©ºè¡¨ï¼Œä¹Ÿè¿”å›åŒ…å«æ‰€æœ‰åˆ—çš„ç©ºDataFrameï¼Œä¿è¯ä¸‹æ¸¸concatæœ‰åˆ—å¤´
        return pd.DataFrame(columns=REQUIRED_OUTPUT_COLUMNS)
    
    for col in REQUIRED_OUTPUT_COLUMNS:
        if col not in df.columns:
            df[col] = ""
    return df

def reorder_columns(df: pd.DataFrame, df_input_columns: list) -> pd.DataFrame:
    """å°†åˆ—é¡ºåºæ•´ç†ä¸ºï¼šè¾“å…¥è¡¨åŸæœ‰åˆ—åœ¨å‰ + REQUIRED_OUTPUT_COLUMNSï¼ˆå»é‡ä¿åºï¼‰"""
    if df is None or df.empty:
        return df
    
    preferred = list(dict.fromkeys(list(df_input_columns) + REQUIRED_OUTPUT_COLUMNS))
    # ä¿ç•™è¡¨å†…å·²æœ‰çš„åˆ—ï¼Œä¸”æŒ‰ç…§ preferred é¡ºåºé‡æ’
    ordered_existing = [c for c in preferred if c in df.columns]
    # è¿½åŠ ä»»ä½•æœªåœ¨ preferred ä¸­ä½†å­˜åœ¨äº df çš„åˆ—ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
    tail = [c for c in df.columns if c not in preferred]
    return df.reindex(columns=ordered_existing + tail)

# === ç»Ÿä¸€çŠ¶æ€å®šä¹‰ï¼ˆä¸ç¤¾äº¤åª’ä½“ä»£ç ä¿æŒä¸€è‡´ï¼‰===
class ProcessingStatus:
    """ç»Ÿä¸€çš„å¤„ç†çŠ¶æ€å®šä¹‰"""
    SUCCESS = "SUCCESS"              # æˆåŠŸå¤„ç†
    NO_RELEVANT = "NO_RELEVANT"      # æ— ç›¸å…³å†…å®¹ï¼ˆä¸éœ€è¦é‡è¯•ï¼‰
    API_FAILED = "API_FAILED"        # APIè°ƒç”¨å¤±è´¥ï¼ˆéœ€è¦é‡è¯•ï¼‰

# ==============================================================================
# === ğŸ”§ æ ¸å¿ƒåŠŸèƒ½æ¨¡å— =========================================================
# ==============================================================================

# Tokenè®¡ç®—å™¨åˆå§‹åŒ–
_tokenizer = None

def get_tokenizer():
    """è·å–tokenè®¡ç®—å™¨ï¼Œä½¿ç”¨å•ä¾‹æ¨¡å¼"""
    global _tokenizer
    if _tokenizer is None:
        try:
            _tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            UX.err(f"åˆå§‹åŒ–tokenizerå¤±è´¥: {e}")
            raise
    return _tokenizer

def count_tokens(text: str) -> int:
    """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
    if not isinstance(text, str) or not text.strip():
        return 0
    
    try:
        tokenizer = get_tokenizer()
        return len(tokenizer.encode(text))
    except Exception as e:
        UX.warn(f"è®¡ç®—tokenæ•°å¤±è´¥: {e}")
        # é™çº§ç­–ç•¥ï¼šä½¿ç”¨å­—ç¬¦æ•°çš„ç²—ç•¥ä¼°ç®—
        token_char_ratio = QUALITY_THRESHOLDS.get('token_char_ratio', 2)
        return len(text) // token_char_ratio  # ä»é…ç½®è¯»å–tokenä¸å­—ç¬¦æ¯”ä¾‹

def get_processing_state(df, id_col):
    """ç»Ÿä¸€çš„çŠ¶æ€æ£€æŸ¥ï¼šè¿”å›(å®Œå…¨æˆåŠŸIDé›†åˆ, æœ‰å¤±è´¥çš„IDé›†åˆ)"""
    if df is None or df.empty or id_col not in df.columns:
        return set(), set()
    
    status_col = 'processing_status'
    try:
        if status_col in df.columns:
            # ğŸ”§ ä¿®å¤ï¼šåŸºäºæ–‡ç« ç»´åº¦åˆ¤æ–­çŠ¶æ€
            fully_successful_ids = set()
            has_failed_ids = set()
            
            # æŒ‰æ–‡ç« IDåˆ†ç»„ç»Ÿè®¡çŠ¶æ€
            for article_id in df[id_col].unique():
                article_records = df[df[id_col] == article_id]
                statuses = article_records[status_col].tolist()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥è®°å½•
                if ProcessingStatus.API_FAILED in statuses:
                    has_failed_ids.add(str(article_id))
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è®°å½•éƒ½æ˜¯æˆåŠŸæˆ–æ— ç›¸å…³
                elif all(s in [ProcessingStatus.SUCCESS, ProcessingStatus.NO_RELEVANT] for s in statuses):
                    fully_successful_ids.add(str(article_id))
                # å…¶ä»–æƒ…å†µï¼ˆæ¯”å¦‚åªæœ‰éƒ¨åˆ†è®°å½•ï¼‰æš‚ä¸åˆ†ç±»
                
            return fully_successful_ids, has_failed_ids
        else:
            # å…¼å®¹æ—§ç‰ˆ
            speaker_col = 'speaker'
            if speaker_col in df.columns:
                success = df[~df[speaker_col].astype(str).str.contains('API_CALL_FAILED', na=False)][id_col]
                failed = df[df[speaker_col].astype(str).str.contains('API_CALL_FAILED', na=False)][id_col]
                return set(success.astype(str)), set(failed.astype(str))
            else:
                return set(), set()
        
    except Exception:
        return set(), set()

def get_stage_model(stage_key: str) -> str:
    try:
        # ä¼˜å…ˆä½¿ç”¨æ–°çš„æ¨¡å‹æ± é…ç½®
        if MODEL_POOLS and 'primary_models' in MODEL_POOLS:
            return MODEL_POOLS['primary_models'][stage_key]
        # å‘åå…¼å®¹ï¼šä½¿ç”¨æ—§çš„é…ç½®
        return API_CONFIG["STAGE_MODELS"][stage_key]
    except Exception:
        raise ValueError(f"æœªé…ç½®é˜¶æ®µæ¨¡å‹: {stage_key}")

file_write_lock = Lock()

def safe_str_convert(value):
    if value is None:
        return ''
    if isinstance(value, pd.Series):
        if value.empty:
            return ''
        return str(value.iloc[0])
    if pd.isna(value):
        return ''
    return str(value)

def normalize_text(text: str) -> str:
    if not isinstance(text, str):
        text = safe_str_convert(text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def detect_language_and_get_config(text):
    if re.search(r'[\u4e00-\u9fa5]', text):
        return 'zh', LANGUAGE_CONFIGS['zh']
    elif re.search(r'[\u0400-\u04FF]', text):
        return 'ru', LANGUAGE_CONFIGS['ru']
    else:
        return 'en', LANGUAGE_CONFIGS['en']

def safe_get_speaker(data, fallback="æœªçŸ¥ä¿¡æº"):
    try:
        if not isinstance(data, dict):
            return fallback
        
        speaker_raw = data.get('speaker')
        if speaker_raw is None:
            return fallback
        
        if not isinstance(speaker_raw, str):
            speaker_raw = str(speaker_raw)
        
        speaker_clean = speaker_raw.strip()
        # ç§»é™¤å¸¸è§çš„æ— æ•ˆå€¼
        if not speaker_clean or speaker_clean in ["speaker", '"speaker"', "'speaker'"]:
            return fallback

        # å»é™¤å¼•å·
        if len(speaker_clean) >= 2:
            if (speaker_clean[0] == speaker_clean[-1]) and speaker_clean[0] in ['"', "'"]:
                speaker_clean = speaker_clean[1:-1].strip()

        return speaker_clean if speaker_clean else fallback
    except Exception:
        return fallback

def create_unified_record(record_type, original_id, source="æœªçŸ¥æ¥æº", text_snippet="", failure_reason=""):
    base_record = {
        "processing_status": record_type,
        "Source": source,
        "Unit_ID": f"{original_id}-{record_type}"
    }

    if record_type == ProcessingStatus.NO_RELEVANT:
        return {
            **base_record,
            "Macro_Chunk_ID": "NO_RELEVANT",
            "speaker": "NO_RELEVANT_CONTENT",
            "Unit_Text": "[æ— ç›¸å…³å†…å®¹]",
            "Incident": "",
            "Frame_ProblemDefinition": "",
            "Frame_ResponsibilityAttribution": "",
            "Frame_MoralEvaluation": "",
            "Frame_SolutionRecommendation": "",
            "Frame_ActionStatement": "",
            "Frame_CausalExplanation": "",
            "Valence": "",
            "Evidence_Type": "",
            "Attribution_Level": "",
            "Temporal_Focus": "",
            "Primary_Actor_Type": "",
            "Geographic_Scope": "",
            "Relationship_Model_Definition": "",
            "Discourse_Type": ""
        }

    elif record_type == ProcessingStatus.API_FAILED:
        return {
            **base_record,
            "Macro_Chunk_ID": "API_FAILED",
            "speaker": "API_CALL_FAILED",
            "Unit_Text": f"[API_FAILED] {failure_reason}: {text_snippet[:200]}...",
            "Incident": "API_CALL_FAILED",
            "Frame_ProblemDefinition": "[]",
            "Frame_ResponsibilityAttribution": "[]",
            "Frame_MoralEvaluation": "[]",
            "Frame_SolutionRecommendation": "[]",
            "Frame_ActionStatement": "[]",
            "Frame_CausalExplanation": "[]",
            "Valence": "API_CALL_FAILED",
            "Evidence_Type": "API_CALL_FAILED",
            "Attribution_Level": "API_CALL_FAILED",
            "Temporal_Focus": "API_CALL_FAILED",
            "Primary_Actor_Type": "API_CALL_FAILED",
            "Geographic_Scope": "API_CALL_FAILED",
            "Relationship_Model_Definition": "API_CALL_FAILED",
            "Discourse_Type": "API_CALL_FAILED"
        }

def clean_failed_records(output_path, id_column):
    if not os.path.exists(output_path):
        return set()

    try:
        df = pd.read_excel(output_path)
        if df.empty or id_column not in df.columns:
            return set()

        if 'processing_status' in df.columns:
            # ä¿®æ­£ï¼šåªæ¸…ç†API_FAILEDçš„è®°å½•ï¼Œä¿ç•™NO_RELEVANTè®°å½•
            failed_mask = df['processing_status'] == ProcessingStatus.API_FAILED
        else:
            failed_mask = df['speaker'].astype(str).str.contains('API_CALL_FAILED', na=False) if 'speaker' in df.columns else pd.Series([False] * len(df))

        failed_ids = set(df[failed_mask][id_column].astype(str).unique())
        if failed_ids:
            df_clean = df[~failed_mask]
            df_clean.to_excel(output_path, index=False)

            if 'processing_status' in df_clean.columns:
                success_count = (df_clean['processing_status'] == ProcessingStatus.SUCCESS).sum()
                no_relevant_count = (df_clean['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                UX.info(f"ä¿ç•™äº† {success_count} æ¡æˆåŠŸè®°å½•, {no_relevant_count} æ¡æ— ç›¸å…³è®°å½•")

            UX.info(f"æ¸…ç†äº† {len(failed_ids)} ä¸ªAPIå¤±è´¥è®°å½•ï¼Œå‰©ä½™ {len(df_clean)} æ¡")

        return failed_ids

    except Exception as e:
        UX.warn(f"æ¸…ç†å¤±è´¥è®°å½•æ—¶å‡ºé”™: {e}")
        return set()

def identify_source(filename):
    source_map = {
        'ä¿„æ€»ç»Ÿ': ['ä¿„æ€»ç»Ÿ', 'æ€»ç»Ÿ', 'Putin', 'president'],
        'ä¿„è¯­åª’ä½“': ['ä¿„è¯­åª’ä½“', 'ä¿„è¯­', 'russian', 'ru_media', 'ä¿„åª’'],
        'ä¸­æ–‡åª’ä½“': ['ä¸­æ–‡åª’ä½“', 'ä¸­æ–‡', 'chinese', 'cn_media', 'ä¸­åª’'],
        'è‹±è¯­åª’ä½“': ['è‹±è¯­åª’ä½“', 'è‹±è¯­', 'english', 'en_media', 'è‹±åª’']
    }

    filename_lower = filename.lower()
    for source, keywords in source_map.items():
        if any(kw.lower() in filename_lower for kw in keywords):
            UX.info(f"æ–‡ä»¶ {filename} è¯†åˆ«ä¸º: {source}")
            return source

    UX.warn(f"æ–‡ä»¶ {filename} æ— æ³•è¯†åˆ«ä¿¡æºï¼Œæ ‡è®°ä¸º: æœªçŸ¥æ¥æº")
    return 'æœªçŸ¥æ¥æº'

def save_macro_chunks_database(new_macro_chunks_list, database_path):
    """åˆ†æ‰¹æ¬¡ã€å¢é‡å¼åœ°ä¿å­˜å®è§‚å—åˆ°ä¸»æ•°æ®åº“"""
    if not new_macro_chunks_list:
        return
    try:
        df_new = pd.DataFrame(new_macro_chunks_list)
        if df_new.empty:
            return
        if os.path.exists(database_path):
            try:
                # è¯»å–ç°æœ‰æ•°æ®å¹¶è¿½åŠ ï¼Œç„¶åå»é‡
                df_existing = pd.read_excel(database_path)
                df_final = pd.concat([df_existing, df_new], ignore_index=True)
                df_final = df_final.drop_duplicates(subset=['Macro_Chunk_ID'], keep='last')
            except Exception as e:
                UX.warn(f"è¯»å–ç°æœ‰å®è§‚å—æ•°æ®åº“å¤±è´¥: {e}ï¼Œå°†è¦†ç›–å†™å…¥")
                df_final = df_new
        else:
            df_final = df_new
        df_final.to_excel(database_path, index=False)
        UX.ok(f"å®è§‚å—æ•°æ®åº“å·²æ›´æ–°: {database_path} (ç´¯è®¡ {len(df_final)} æ¡)")
    except Exception as e:
        UX.err(f"ä¿å­˜å®è§‚å—æ•°æ®åº“å¤±è´¥: {e}")

def get_existing_macro_chunks(original_id, macro_db_path):
    """è·å–å·²å­˜åœ¨çš„å®è§‚å—ä¿¡æ¯ï¼ˆåŒ…å«æ°¸ä¹…IDï¼‰ï¼Œç”¨äºé‡æ–°åˆ†ææ—¶ä¿æŒä¸€è‡´æ€§"""
    if not os.path.exists(macro_db_path):
        return None
    try:
        df_macro = pd.read_excel(macro_db_path)
        if df_macro.empty or 'Original_ID' not in df_macro.columns:
            return None
        # æŸ¥æ‰¾è¯¥åŸå§‹IDå¯¹åº”çš„å®è§‚å—
        existing_chunks_df = df_macro[df_macro['Original_ID'].astype(str) == str(original_id)]
        if existing_chunks_df.empty:
            return None
        
        # å°†DataFrameç›´æ¥è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼Œä¿ç•™æ‰€æœ‰åˆ—ï¼ˆç‰¹åˆ«æ˜¯Macro_Chunk_IDï¼‰
        macro_chunks = existing_chunks_df.to_dict('records')
        UX.info(f"æ‰¾åˆ°ID {original_id} çš„å·²ä¿å­˜å®è§‚å—: {len(macro_chunks)} ä¸ª")
        return macro_chunks
    except Exception as e:
        UX.warn(f"è¯»å–å·²ä¿å­˜å®è§‚å—å¤±è´¥: {e}")
        return None

def build_resume_plan(output_file_path: str, df_input: pd.DataFrame, id_col: str):
    """æ„å»ºæ–­ç‚¹ç»­ä¼ è®¡åˆ’
    è¿”å›:
    - never_processed_ids: ä»æœªå¤„ç†è¿‡çš„æ–‡ç« IDé›†åˆ
    - rechunk_article_ids: éœ€è¦é‡æ–°åˆ‡åˆ†å®è§‚å—çš„æ–‡ç« IDé›†åˆï¼ˆå…¨æœ‰æˆ–å…¨æ— ï¼‰
    - macro_chunks_to_rerun: dict[original_id] -> set(Macro_Chunk_ID) éœ€é‡æ–°åˆ†æçš„å®è§‚å—é›†åˆ
    """
    all_input_ids = set(df_input[id_col].astype(str)) if (df_input is not None and id_col in df_input.columns) else set()

    never_processed_ids = set(all_input_ids)
    rechunk_article_ids = set()
    macro_chunks_to_rerun = {}

    if os.path.exists(output_file_path):
        try:
            df_existing = pd.read_excel(output_file_path)
            if not df_existing.empty and id_col in df_existing.columns:
                processed_article_ids = set(df_existing[id_col].astype(str).unique())
                never_processed_ids = all_input_ids - processed_article_ids

                if 'processing_status' in df_existing.columns:
                    failed_records = df_existing[df_existing['processing_status'] == ProcessingStatus.API_FAILED]
                    for _, record in failed_records.iterrows():
                        macro_chunk_id = str(record.get('Macro_Chunk_ID', '')).strip()
                        article_id = str(record.get(id_col, '')).strip()

                        # å…¼å®¹ï¼šæ—§æ•°æ®æœªå¡«ç‰¹æ®Šæ ‡è®°ï¼Œä»…è®°å½•äº†å¤±è´¥æ–‡æ¡ˆ
                        if not macro_chunk_id or macro_chunk_id == 'API_FAILED':
                            unit_text = str(record.get('Unit_Text', '')).strip()
                            # ä¼˜å…ˆï¼šä»æ–‡æ¡ˆä¸­è§£æå½¢å¦‚ ABC123-M45 çš„å®è§‚å—ç¼–å·
                            # é¿å…è¯¯åŒ¹é… MACRO_CHUNKING_FAILED
                            try:
                                match = re.search(r'([A-Za-z0-9_-]+-M\d+)', unit_text)
                            except Exception:
                                match = None
                            if match:
                                macro_chunk_id = match.group(1)
                            else:
                                # è‹¥èƒ½è¯†åˆ«åˆ°"åˆ‡åˆ†å¤±è´¥"å­—æ ·ï¼Œåˆ™æŒ‰æ–‡ç« çº§é‡åˆ‡åˆ†å¤„ç†
                                if ('åˆ‡åˆ†' in unit_text and 'å¤±è´¥' in unit_text) or ('å®è§‚å—åˆ‡åˆ†å¤±è´¥' in unit_text):
                                    if article_id:
                                        rechunk_article_ids.add(article_id)
                                    continue
                                # æ— æ³•å®šä½å®è§‚å—ç¼–å·ä¸”ä¸æ˜¯æ˜ç¡®çš„åˆ‡åˆ†å¤±è´¥ -> è·³è¿‡è¯¥æ¡ï¼Œç­‰å¾…ä¸‹ä¸€è½®
                                continue

                        if macro_chunk_id.endswith('-MACRO_CHUNKING_FAILED'):
                            if article_id:
                                rechunk_article_ids.add(article_id)
                        elif macro_chunk_id and macro_chunk_id != 'API_FAILED':
                            s = macro_chunks_to_rerun.setdefault(article_id, set())
                            s.add(macro_chunk_id)
        except Exception as e:
            UX.warn(f"è¯»å–å·²å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")

    return never_processed_ids, rechunk_article_ids, macro_chunks_to_rerun

def get_failed_macro_chunk_ids(original_id, output_file_path):

    """è·å–è¯¥æ–‡ç« ä¸­å¤±è´¥çš„å®è§‚å—IDåˆ—è¡¨"""
    if not os.path.exists(output_file_path):
        return set()
    try:
        df_output = pd.read_excel(output_file_path)
        if df_output.empty:
            return set()
        # æŸ¥æ‰¾è¯¥åŸå§‹IDå¯¹åº”çš„å¤±è´¥è®°å½•
        id_mask = df_output[COLUMN_MAPPING["ID"]].astype(str) == str(original_id)
        failed_mask = df_output['processing_status'] == ProcessingStatus.API_FAILED
        failed_records = df_output[id_mask & failed_mask]
        if failed_records.empty:
            return set()
        # æå–å¤±è´¥çš„å®è§‚å—ID
        failed_macro_chunk_ids = set()
        if 'Macro_Chunk_ID' in failed_records.columns:
            failed_macro_chunk_ids = set(failed_records['Macro_Chunk_ID'].dropna().astype(str))
        return failed_macro_chunk_ids
    except Exception as e:
        UX.warn(f"è¯»å–å¤±è´¥å®è§‚å—IDå¤±è´¥: {e}")
        return set()

def get_successful_macro_chunk_ids(original_id, output_file_path):
    """è·å–è¯¥æ–‡ç« ä¸­å·²æˆåŠŸå¤„ç†çš„å®è§‚å—IDåˆ—è¡¨"""
    if not os.path.exists(output_file_path):
        return set()
    try:
        df_output = pd.read_excel(output_file_path)
        if df_output.empty:
            return set()
        # æŸ¥æ‰¾è¯¥åŸå§‹IDå¯¹åº”çš„æˆåŠŸè®°å½•
        id_mask = df_output[COLUMN_MAPPING["ID"]].astype(str) == str(original_id)
        success_mask = df_output['processing_status'] == ProcessingStatus.SUCCESS
        success_records = df_output[id_mask & success_mask]
        if success_records.empty:
            return set()
        # æå–æˆåŠŸçš„å®è§‚å—ID
        successful_macro_chunk_ids = set()
        if 'Macro_Chunk_ID' in success_records.columns:
            successful_macro_chunk_ids = set(success_records['Macro_Chunk_ID'].dropna().astype(str))
        return successful_macro_chunk_ids
    except Exception as e:
        UX.warn(f"è¯»å–æˆåŠŸå®è§‚å—IDå¤±è´¥: {e}")
        return set()

def needs_reprocessing(original_id, output_file_path, macro_db_path):
    """åˆ¤æ–­æ–‡ç« æ˜¯å¦éœ€è¦é‡æ–°å¤„ç†ï¼ˆæ”¯æŒå®è§‚å—çº§åˆ«çš„ç²¾ç¡®åˆ¤æ–­ï¼‰"""
    # å¦‚æœæ²¡æœ‰åˆ†æç»“æœæ–‡ä»¶ï¼Œéœ€è¦å¤„ç†
    if not os.path.exists(output_file_path):
        return True
    
    # å¦‚æœæ²¡æœ‰å®è§‚å—æ•°æ®åº“ï¼ŒæŒ‰åŸæœ‰é€»è¾‘å¤„ç†
    if not macro_db_path or not os.path.exists(macro_db_path):
        return True
    
    try:
        # è·å–è¯¥æ–‡ç« çš„æ‰€æœ‰å®è§‚å—
        existing_macro_chunks = get_existing_macro_chunks(original_id, macro_db_path)
        if not existing_macro_chunks:
            return True
        
        # è·å–æ‰€æœ‰åº”è¯¥å­˜åœ¨çš„å®è§‚å—ID
        expected_macro_chunk_ids = set()
        for i in range(len(existing_macro_chunks)):
            expected_macro_chunk_ids.add(f"{original_id}-M{i+1}")
        
        # è·å–å·²æˆåŠŸå¤„ç†çš„å®è§‚å—ID
        successful_macro_chunk_ids = get_successful_macro_chunk_ids(original_id, output_file_path)
        
        # è·å–å¤±è´¥çš„å®è§‚å—ID  
        failed_macro_chunk_ids = get_failed_macro_chunk_ids(original_id, output_file_path)
        
        # ğŸ”§ ä¿®å¤é€»è¾‘ï¼šæ›´ç²¾ç¡®çš„åˆ¤æ–­
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰å®è§‚å—åˆ‡åˆ†å¤±è´¥ï¼ˆéœ€è¦é‡æ–°å¤„ç†æ•´ç¯‡æ–‡ç« ï¼‰
        macro_chunking_failed = any(chunk_id.endswith('-MACRO_CHUNKING_FAILED') for chunk_id in failed_macro_chunk_ids)
        if macro_chunking_failed:
            UX.info(f"æ–‡ç«  {original_id}: æ£€æµ‹åˆ°å®è§‚å—åˆ‡åˆ†å¤±è´¥ï¼Œéœ€è¦é‡æ–°å¤„ç†æ•´ç¯‡æ–‡ç« ")
            return True
        
        # 2. æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å®è§‚å—éƒ½å·²æˆåŠŸå¤„ç†
        missing_macro_chunks = expected_macro_chunk_ids - successful_macro_chunk_ids
        has_missing = len(missing_macro_chunks) > 0
        has_failed = len(failed_macro_chunk_ids) > 0
        
        if has_failed or has_missing:
            UX.info(f"æ–‡ç«  {original_id}: éœ€è¦é‡æ–°å¤„ç†éƒ¨åˆ†å®è§‚å— (å¤±è´¥:{len(failed_macro_chunk_ids)}ä¸ª, ç¼ºå¤±:{len(missing_macro_chunks)}ä¸ª)")
            return True
        
        # 3. æ‰€æœ‰å®è§‚å—éƒ½å·²æˆåŠŸå¤„ç†
        UX.info(f"æ–‡ç«  {original_id}: æ‰€æœ‰{len(expected_macro_chunk_ids)}ä¸ªå®è§‚å—å‡å·²æˆåŠŸå¤„ç†ï¼Œè·³è¿‡")
        return False
        
    except Exception as e:
        UX.warn(f"åˆ¤æ–­æ–‡ç« {original_id}æ˜¯å¦éœ€è¦é‡æ–°å¤„ç†æ—¶å‡ºé”™: {e}")
        return True
        
class APIService:
    def __init__(self, session):
        self.session = session
        self.fail_counts = {}
        self.model_index = {}
        self.total_switches = {}

    def _select_model(self, stage_key: str, explicit_model: str = None) -> str:
        if explicit_model:
            return explicit_model
        
        # è·å–ä¸»æ¨¡å‹å’Œå¤‡ç”¨æ¨¡å‹
        if MODEL_POOLS and 'primary_models' in MODEL_POOLS:
            primary = MODEL_POOLS['primary_models'].get(stage_key)
            fallback_list = MODEL_POOLS.get('fallback_models', {}).get(stage_key, [])
            fallback = fallback_list[0] if fallback_list else primary  # æ²¡æœ‰å¤‡ç”¨å°±ç”¨ä¸»æ¨¡å‹
        else:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨æ—§çš„é…ç½®
            primary = get_stage_model(stage_key)
            fallback_list = API_CONFIG.get('FALLBACK', {}).get('STAGE_CANDIDATES', {}).get(stage_key, [])
            fallback = fallback_list[0] if fallback_list else primary
        
        # å¦‚æœä¸»å¤‡ç›¸åŒï¼Œå°±ä¸åˆ‡æ¢
        if primary == fallback:
            return primary
        
        models = [primary, fallback]
        idx = self.model_index.get(stage_key, 0)
        return models[idx]

    def _handle_failure(self, stage_key: str) -> bool:
        """3æ¬¡å¤±è´¥ååˆ‡æ¢æ¨¡å‹"""
        # åˆ‡æ¢è®¡æ•°
        self.total_switches[stage_key] = self.total_switches.get(stage_key, 0) + 1
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ä¸Šé™ï¼ˆ20æ¬¡åˆ‡æ¢ = 10è½®ï¼‰
        if self.total_switches[stage_key] >= MAX_MODEL_SWITCHES:
            return False
        
        # åˆ‡æ¢æ¨¡å‹ï¼šä¸»æ¨¡å‹ â†â†’ å¤‡ç”¨æ¨¡å‹
        self.model_index[stage_key] = 1 - self.model_index.get(stage_key, 0)
        UX.info(f"[{stage_key}] åˆ‡æ¢æ¨¡å‹ (ç¬¬{self.total_switches[stage_key]}æ¬¡)")
        return True

    async def _call_api(self, prompt, language='zh', model_name=None, stage_key=None, context_label=None):
        # åˆå§‹åŒ–å˜é‡ï¼Œé¿å…åœ¨å¼‚å¸¸å¤„ç†ä¸­å¼•ç”¨æœªå®šä¹‰çš„å˜é‡
        response = None
        url = ""
        config = LANGUAGE_CONFIGS.get(language, LANGUAGE_CONFIGS['zh'])
        prompt_tokens = count_tokens(prompt)
        
        # ä»ç»Ÿä¸€è¶…æ—¶é…ç½®è¯»å–tokené˜ˆå€¼å’Œè¶…æ—¶æ—¶é—´ï¼ˆæ‰€æœ‰è¯­è¨€é€šç”¨ï¼‰
        timeout_config = CONFIG['TIMEOUT_CONFIG']
        threshold_short = timeout_config['TOKEN_THRESHOLD_SHORT']
        threshold_medium = timeout_config['TOKEN_THRESHOLD_MEDIUM']
        
        if prompt_tokens < threshold_short:
            timeout = timeout_config['TIMEOUT_SHORT']  # çŸ­æ–‡æœ¬
        elif prompt_tokens <= threshold_medium:
            timeout = timeout_config['TIMEOUT_MEDIUM']  # ä¸­ç­‰æ–‡æœ¬
        else:
            timeout = timeout_config['TIMEOUT_LONG']  # é•¿æ–‡æœ¬
        
        for attempt in range(API_RETRY_ATTEMPTS):
            try:
                api_key = API_CONFIG["API_KEYS"][0]
                url = f"{API_CONFIG['BASE_URL']}/chat/completions"
                headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
                chosen_model = self._select_model(stage_key or 'INTEGRATED_ANALYSIS', model_name)
                
                # ä»é…ç½®è¯»å–APIè¯·æ±‚å‚æ•°
                payload = {
                    "model": chosen_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": API_REQUEST_PARAMS.get('temperature', 0),
                    "response_format": API_REQUEST_PARAMS.get('response_format', {"type": "json_object"})
                }
                
                async with self.session.post(url, headers=headers, json=payload,
                                           timeout=aiohttp.ClientTimeout(total=timeout)) as response:
                    response_text = await response.text()
                    response.raise_for_status()
                    response_json = json.loads(response_text)
                    content = response_json.get('choices', [{}])[0].get('message', {}).get('content')
                    
                    if not content:
                        raise ValueError("å“åº”ç¼ºå°‘content")
                    
                    # æå–JSON
                    first_brace = content.find('{')
                    last_brace = content.rfind('}')
                    
                    if first_brace >= 0 and last_brace > first_brace:
                        cleaned_text = content[first_brace:last_brace+1]
                        result = json.loads(cleaned_text)
                        
                        # æˆåŠŸé‡ç½®å¤±è´¥è®¡æ•°
                        if stage_key:
                            self.fail_counts[stage_key] = 0
                        return result
                    
                    raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆJSONç»“æ„")
            
            except Exception as e:
                # è®°å½•APIå¤±è´¥ä¿¡æ¯
                status_code = response.status if response else 'N/A'
                
                # æ‰“å°åŸå§‹å“åº”ä½“ï¼Œä¾¿äºå®šä½é—®é¢˜
                try:
                    if 'response_text' in locals() and response_text:
                        UX.err(f"APIå“åº”ä½“åŸæ–‡: {response_text}")
                    elif response is not None:
                        try:
                            raw_text = await response.text()
                        except Exception:
                            raw_text = "<æ— æ³•è¯»å–å“åº”ä½“>"
                        UX.err(f"APIå“åº”ä½“åŸæ–‡: {raw_text}")
                except Exception:
                    pass

                UX.warn(f"[{stage_key or 'API'}] APIå¤±è´¥: {status_code}, message='{str(e)}', url='{url}'")
                
                # é‡è¯•å»¶è¿Ÿï¼ˆé™¤äº†æœ€åä¸€æ¬¡ï¼‰
                if attempt < API_RETRY_ATTEMPTS - 1:
                    delay = RETRY_DELAYS[attempt] if attempt < len(RETRY_DELAYS) else 2
                    await asyncio.sleep(delay)
                # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
        
        # 3æ¬¡éƒ½å¤±è´¥äº†ï¼Œç°åœ¨å¤„ç†æ¨¡å‹åˆ‡æ¢
        if stage_key:
            should_continue = self._handle_failure(stage_key)
            if should_continue:
                # é€’å½’è°ƒç”¨ï¼Œç”¨æ–°æ¨¡å‹é‡æ–°è¯•3æ¬¡
                UX.info(f"[{stage_key}] ä½¿ç”¨æ–°æ¨¡å‹é‡æ–°å°è¯•")
                return await self._call_api(prompt, language, None, stage_key, context_label)
            else:
                UX.warn(f"[{stage_key}] è¾¾åˆ°åˆ‡æ¢ä¸Šé™ï¼Œæ”¾å¼ƒ")
        
        return None

    async def get_analysis(self, prompt, expected_key, language='zh', model_name=None, stage_key=None, context_label=None):
        result = await self._call_api(prompt, language, model_name, stage_key, context_label)
        
        if result is None:
            return None
        
        
        if not isinstance(result, dict) or expected_key not in result:
            if SKIP_FAILED_TEXTS:
                return None
            raise ValueError(f"è¿”å›JSONç¼ºå°‘é”®: '{expected_key}'")
        
        return result[expected_key]

# ==============================================================================
# === ğŸ¤– AIæŒ‡ä»¤æ¨¡æ¿ =================================================
# ==============================================================================
class Prompts:
    def __init__(self):
        """ä»æ–‡ä»¶åŠ è½½æç¤ºè¯"""
        self.prompts_dir = os.path.join(os.path.dirname(__file__), 'prompts')
        self._load_prompts()

    def _load_prompts(self):
        """åŠ è½½æ‰€æœ‰æç¤ºè¯æ–‡ä»¶"""
        prompt_files = {
            'MACRO_CHUNKING_ZH': 'macro_chunking_zh.txt',
            'MACRO_CHUNKING_EN': 'macro_chunking_en.txt',
            'MACRO_CHUNKING_RU': 'macro_chunking_ru.txt',
            'INTEGRATED_ANALYSIS_V2': 'integrated_analysis_v2.txt'
        }

        for attr_name, filename in prompt_files.items():
            file_path = os.path.join(self.prompts_dir, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    setattr(self, f'_{attr_name}', f.read())
            except Exception as e:
                print(f"åŠ è½½æç¤ºè¯æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                setattr(self, f'_{attr_name}', "")

    # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™å±æ€§è®¿é—®æ–¹å¼
    @property
    def MACRO_CHUNKING_ZH(self):
        return getattr(self, '_MACRO_CHUNKING_ZH', "")

    @property
    def MACRO_CHUNKING_EN(self):
        return getattr(self, '_MACRO_CHUNKING_EN', "")

    @property
    def MACRO_CHUNKING_RU(self):
        return getattr(self, '_MACRO_CHUNKING_RU', "")

    @property
    def INTEGRATED_ANALYSIS_V2(self):
        return getattr(self, '_INTEGRATED_ANALYSIS_V2', "")

# åˆ›å»ºPromptså®ä¾‹
prompts = Prompts()

# ==============================================================================
# === ğŸ“¦ æ‰¹å¤„ç†ä¸“ç”¨Promptæ¨¡æ¿ ==========================================
# ==============================================================================

PROMPT_BATCH_ANALYSIS = """
# è§’è‰²
ä½ æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ã€æ”¯æŒå¹¶è¡Œå¤„ç†çš„æ–‡æœ¬åˆ†æå¼•æ“ã€‚

# ä»»åŠ¡
ä½ å°†æ”¶åˆ°ä¸€ä¸ªJSONæ ¼å¼çš„è¾“å…¥ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªã€å¾…åˆ†ææ–‡æœ¬å—åˆ—è¡¨ã€‘ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ï¼Œä¸¥æ ¼æŒ‰ç…§åˆ—è¡¨ä¸­çš„é¡ºåºï¼Œã€ç‹¬ç«‹åœ°ã€‘å¯¹åˆ—è¡¨ä¸­çš„ã€æ¯ä¸€ä¸ªã€‘æ–‡æœ¬å—ï¼Œæ‰§è¡Œå®Œæ•´çš„"ç§å­æ‰©å±•ä¸å¤šç»´åº¦å†…å®¹åˆ†æ"ã€‚
ä½ å¿…é¡»ä¸ºåˆ—è¡¨ä¸­çš„æ¯ä¸€ä¸ªæ–‡æœ¬å—ï¼Œéƒ½ç”Ÿæˆä¸€ä¸ªå¯¹åº”çš„ã€å®Œæ•´çš„åˆ†æç»“æœå¯¹è±¡ã€‚

# æ ¸å¿ƒè§„åˆ™
- **ç»å¯¹ç‹¬ç«‹æ€§**: æ–‡æœ¬å—çš„åˆ†æã€ç»å¯¹ä¸èƒ½ã€‘å—åˆ°å…¶ä»–æ–‡æœ¬å—çš„ä»»ä½•å½±å“ã€‚ä½ è¦åƒå¤„ç†Nä¸ª**å®Œå…¨**ç‹¬ç«‹çš„APIè¯·æ±‚ä¸€æ ·å¤„ç†è¿™ä¸ªåˆ—è¡¨ã€‚å®ƒä»¬è¢«æ‰“åŒ…åœ¨ä¸€èµ·åªæ˜¯ä¸ºäº†æé«˜æ•ˆç‡ã€‚
- **é¡ºåºä¸€è‡´æ€§**: ä½ çš„è¾“å‡ºç»“æœåˆ—è¡¨ï¼Œå¿…é¡»ä¸è¾“å…¥åˆ—è¡¨çš„é¡ºåºã€ä¸¥æ ¼ä¿æŒä¸€è‡´ã€‘ã€‚è¾“å‡ºåˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œå¿…é¡»æ˜¯è¾“å…¥åˆ—è¡¨ç¬¬ä¸€ä¸ªæ–‡æœ¬å—çš„åˆ†æç»“æœã€‚
- **å®Œæ•´æ€§**: å³ä½¿æŸä¸ªæ–‡æœ¬å—ä½ è®¤ä¸ºä¸ä¸­ä¿„æ— å…³ï¼Œæˆ–è€…æ— æ³•æå–å‡º"è®®é¢˜å•å…ƒ"ï¼Œä½ ä¹Ÿå¿…é¡»ä¸ºå®ƒç”Ÿæˆä¸€ä¸ªå¯¹åº”çš„ç©ºç»“æœå¯¹è±¡ï¼ˆä¾‹å¦‚ `{"analyzed_Units": []}`ï¼‰ï¼Œä»¥ä¿æŒä½ç½®å¯¹åº”ã€‚ç»ä¸èƒ½å› ä¸ºæŸä¸ªæ–‡æœ¬å—æ²¡æœ‰ç»“æœå°±è·³è¿‡å®ƒã€‚

# è¾“å…¥æ ¼å¼ç¤ºä¾‹
```json
{
  "texts_to_analyze": [
    {
      "chunk_id": 0,
      "speaker": "æ™®äº¬ (ä¿„ç½—æ–¯æ€»ç»Ÿ)",
      "text": "è¿™æ˜¯ä¸€ä¸ªéœ€è¦åˆ†æçš„çŸ­æ–‡æœ¬å—ã€‚"
    },
    {
      "chunk_id": 1,
      "speaker": "è®°è€…",
      "text": "è¿™æ˜¯ç¬¬äºŒä¸ªå®Œå…¨ç‹¬ç«‹çš„çŸ­æ–‡æœ¬å—ã€‚"
    }
  ]
}
```

# è¾“å‡ºæ ¼å¼
ä½ çš„å›å¤ã€å¿…é¡»ä¸”åªèƒ½ã€‘æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ä¸€ä¸ªé”® "batch_results"ã€‚
å…¶å€¼ä¸ºä¸€ä¸ªJSONå¯¹è±¡çš„åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸€ä¸ªå¯¹è±¡éƒ½å¯¹åº”è¾“å…¥åˆ—è¡¨ä¸­çš„ä¸€ä¸ªæ–‡æœ¬å—ã€‚
æ¯ä¸€ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«ä»¥ä¸‹ä¸¤ä¸ªé”®ï¼š
- "chunk_id": (æ•´æ•°) å¯¹åº”è¾“å…¥çš„chunk_idï¼Œç”¨äºæ ¸å¯¹ã€‚
- "analysis_output": (JSONå¯¹è±¡) è¿™æ˜¯å¯¹è¯¥æ–‡æœ¬å—æ‰§è¡Œ"ç§å­æ‰©å±•ä¸å¤šç»´åº¦å†…å®¹åˆ†æ"åäº§å‡ºçš„ã€å®Œæ•´ç»“æœã€‘ï¼Œå…¶ç»“æ„å¿…é¡»ä¸ä½ ç‹¬ç«‹åˆ†ææ—¶è¾“å‡ºçš„ `{"analyzed_Units": [...]}` å®Œå…¨ä¸€è‡´ã€‚

# ã€æ³¨æ„ã€‘ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚å¤„ç†ä¸‹æ–¹æä¾›çš„å®é™…è¾“å…¥ã€‚

"""
# ==============================================================================
# === âš™ï¸ ä¸»å¤„ç†æµç¨‹ ==================================================
# ==============================================================================

async def process_chunks_batch(chunks_to_process, original_id, source, api_service):
    """æ‰¹å¤„ç†å®è§‚å—åˆ†æ"""
    final_data = []
    
    # å°†å®è§‚å—åˆ†ç»„ä¸ºæ‰¹æ¬¡
    for i in range(0, len(chunks_to_process), BATCH_SIZE):
        batch = chunks_to_process[i:i + BATCH_SIZE]
        UX.info(f"ID {original_id}: å¤„ç†æ‰¹æ¬¡ {i//BATCH_SIZE + 1}, åŒ…å« {len(batch)} ä¸ªå®è§‚å—")
        
        # æ„å»ºæ‰¹å¤„ç†è¾“å…¥
        texts_to_analyze = []
        chunk_mapping = {}  # ä¿å­˜chunk_idåˆ°åŸå§‹æ•°æ®çš„æ˜ å°„
        
        for idx, chunk_data in enumerate(batch):
            chunk_text = chunk_data.get('Macro_Chunk_Text', '').strip()
            if not chunk_text:
                continue
                
            chunk_id = i + idx
            texts_to_analyze.append({
                "chunk_id": chunk_id,
                "speaker": chunk_data.get('Speaker', ''),
                "text": chunk_text
            })
            chunk_mapping[chunk_id] = chunk_data
        
        if not texts_to_analyze:
            continue
            
        # å‡†å¤‡æ‰¹å¤„ç†è¯·æ±‚
        batch_input = {"texts_to_analyze": texts_to_analyze}
        batch_prompt = PROMPT_BATCH_ANALYSIS + f"\n\n{json.dumps(batch_input, ensure_ascii=False, indent=2)}"
        
        # æ£€æµ‹è¯­è¨€ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡æœ¬å—çš„è¯­è¨€ï¼‰
        first_text = texts_to_analyze[0]["text"]
        language, _ = detect_language_and_get_config(first_text)
        
        # æ‰¹å¤„ç†ä¸“ç”¨äºè¶…çŸ­å—ï¼Œç›´æ¥ä½¿ç”¨çŸ­æ–‡æœ¬æ¨¡å‹
        model_key = 'INTEGRATED_ANALYSIS_SHORT'
        
        # æ‰§è¡Œæ‰¹å¤„ç†APIè°ƒç”¨
        batch_results = await api_service.get_analysis(
            batch_prompt, 'batch_results', language,
            model_name=get_stage_model(model_key), stage_key=f"BATCH_{model_key}",
            context_label=f"{original_id}:BATCH_{i//BATCH_SIZE + 1}"
        )
        
        # å¤„ç†æ‰¹å¤„ç†ç»“æœ
        if batch_results and isinstance(batch_results, list):
            for result in batch_results:
                if not isinstance(result, dict):
                    continue
                    
                chunk_id = result.get('chunk_id')
                analysis_output = result.get('analysis_output', {})
                
                if chunk_id not in chunk_mapping:
                    continue
                    
                chunk_data = chunk_mapping[chunk_id]
                macro_chunk_id = chunk_data.get('Macro_Chunk_ID')
                speaker = chunk_data.get('Speaker')
                
                # å¤„ç†åˆ†æç»“æœ
                analyzed_Units = analysis_output.get('analyzed_Units', [])
                if isinstance(analyzed_Units, list):
                    unit_counter_in_chunk = 1
                    for u in analyzed_Units:
                        Unit_Text = safe_str_convert(u.get("Unit_Text", ""))
                        if not Unit_Text.strip():
                            continue
                            
                        unit_id = f"{macro_chunk_id}-{unit_counter_in_chunk}"
                        norm_text = normalize_text(Unit_Text)
                        Unit_hash = hashlib.sha256(norm_text.encode('utf-8')).hexdigest()
                        
                        final_data.append({
                            "Unit_ID": unit_id,
                            "Source": source,
                            "speaker": speaker, 
                            "Unit_Text": Unit_Text, 
                            "seed_sentence": u.get("seed_sentence", ""),
                            "expansion_logic": u.get("expansion_logic", ""), 
                            "Macro_Chunk_ID": macro_chunk_id,
                            "Unit_Hash": Unit_hash, 
                            "processing_status": ProcessingStatus.SUCCESS, 
                            "Incident": u.get("Incident", ""),
                            "Frame_ProblemDefinition": u.get("Frame_ProblemDefinition", []),
                            "Frame_ResponsibilityAttribution": u.get("Frame_ResponsibilityAttribution", []),
                            "Frame_MoralEvaluation": u.get("Frame_MoralEvaluation", []),
                            "Frame_SolutionRecommendation": u.get("Frame_SolutionRecommendation", []),
                            "Frame_ActionStatement": u.get("Frame_ActionStatement", []),
                            "Frame_CausalExplanation": u.get("Frame_CausalExplanation", []),
                            "Valence": u.get("Valence", ""), 
                            "Evidence_Type": u.get("Evidence_Type", ""),
                            "Attribution_Level": u.get("Attribution_Level", ""), 
                            "Temporal_Focus": u.get("Temporal_Focus", ""),
                            "Primary_Actor_Type": u.get("Primary_Actor_Type", ""), 
                            "Geographic_Scope": u.get("Geographic_Scope", ""),
                            "Relationship_Model_Definition": u.get("Relationship_Model_Definition", ""),
                            "Discourse_Type": u.get("Discourse_Type", "")
                        })
                        
                        unit_counter_in_chunk += 1
        else:
            # æ‰¹å¤„ç†å¤±è´¥ï¼Œè®°å½•å¤±è´¥ä¿¡æ¯
            for chunk_data in batch:
                macro_chunk_id = chunk_data.get('Macro_Chunk_ID')
                failed_unit = create_unified_record(
                    ProcessingStatus.API_FAILED, original_id, source, 
                    chunk_data.get('Macro_Chunk_Text', '')[:200], 
                    f"å®è§‚å— {macro_chunk_id} æ‰¹å¤„ç†åˆ†æå¤±è´¥"
                )
                failed_unit["Macro_Chunk_ID"] = macro_chunk_id
                final_data.append(failed_unit)
    
    return final_data

async def get_macro_chunks_media(row, api_service):
   full_text = safe_str_convert(row.get(COLUMN_MAPPING["MEDIA_TEXT"], ''))
   if not full_text.strip():
       return []
   language, config = detect_language_and_get_config(full_text)
   text_tokens = count_tokens(full_text)

   if text_tokens > config['MAX_SINGLE_TEXT']:
       UX.warn(f"æ–‡æœ¬è¿‡é•¿({text_tokens} tokens, {len(full_text)}å­—ç¬¦)ï¼Œè·³è¿‡")
       return None
   # é€‰æ‹©è¯­è¨€å¯¹åº”çš„æ¨¡æ¿
   template_map = {
       'ru': prompts.MACRO_CHUNKING_RU,
       'zh': prompts.MACRO_CHUNKING_ZH,
       'en': prompts.MACRO_CHUNKING_EN
   }
   macro_tpl = template_map.get(language, prompts.MACRO_CHUNKING_EN)
   try:
       prompt = macro_tpl.replace("{full_text}", full_text)
       macro_chunks = await api_service.get_analysis(
           prompt, 'macro_chunks', language,
           model_name=get_stage_model('MACRO_CHUNKING'),
           stage_key='MACRO_CHUNKING',
           context_label=f"ID={safe_str_convert(row.get(COLUMN_MAPPING['ID'], 'unknown'))}:MACRO"
       )
       if macro_chunks is None:
           return None

       if not isinstance(macro_chunks, list):
           # APIè¿”å›æ ¼å¼å¼‚å¸¸ï¼Œè§†ä¸ºå¤±è´¥
           UX.warn(f"å®è§‚å—åˆ‡åˆ†APIè¿”å›æ ¼å¼å¼‚å¸¸ï¼Œè§†ä¸ºå¤±è´¥")
           return None
       return [chunk for chunk in macro_chunks if isinstance(chunk, dict)]

   except Exception as e:
       UX.warn(f"å®è§‚åˆ†å—å¤±è´¥: {e}")
       # æ‰€æœ‰å¼‚å¸¸éƒ½è§†ä¸ºå¤±è´¥ï¼Œç¡®ä¿"å…¨æœ‰æˆ–å…¨æ— "
       return None

async def process_row(row_data, api_service, macro_db_path=None, output_file_path=None, macro_chunks_rerun=None, force_rechunk=False):
    """å¤„ç†å•è¡Œæ•°æ®ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼Œä½¿ç”¨æ°¸ä¹…IDï¼‰"""
    row = row_data[1]
    original_id = safe_str_convert(row.get(COLUMN_MAPPING["ID"]))
    source = row_data[2]
    
    failed_macro_chunk_ids = set()
    if macro_chunks_rerun and isinstance(macro_chunks_rerun, dict):
        failed_macro_chunk_ids = set(macro_chunks_rerun.get(original_id, set()))

    try:
        media_text = safe_str_convert(row.get(COLUMN_MAPPING["MEDIA_TEXT"], ''))
        article_title = safe_str_convert(row.get(COLUMN_MAPPING['MEDIA_TITLE'], 'æ— æ ‡é¢˜'))
        macro_chunking_failed = bool(force_rechunk)
        
        macro_chunks = None
        is_reprocessing = False

        if macro_db_path and not macro_chunking_failed:
            macro_chunks = get_existing_macro_chunks(original_id, macro_db_path)
            if macro_chunks is not None:
                is_reprocessing = True
        
        if macro_chunks is None:
            if macro_chunking_failed: UX.info(f"ID {original_id}: é‡æ–°åˆ‡åˆ†å®è§‚å—")
            macro_chunks = await get_macro_chunks_media(row, api_service)
            if macro_chunks: # é¦–æ¬¡åˆ‡åˆ†åï¼Œä¸ºæ–°å—èµ‹äºˆæ°¸ä¹…ID
                 for i, chunk in enumerate(macro_chunks):
                     chunk['Macro_Chunk_ID'] = f"{original_id}-M{i+1}"
                     chunk['Speaker'] = safe_get_speaker(chunk, 'æœªçŸ¥ä¿¡æº')
                     chunk['Macro_Chunk_Text'] = chunk.pop('macro_chunk_text', '')

        if macro_chunks is None:
            failed_record = create_unified_record(ProcessingStatus.API_FAILED, original_id, source, media_text, "å®è§‚å—åˆ‡åˆ†å¤±è´¥")
            failed_record["Macro_Chunk_ID"] = f"{original_id}-MACRO_CHUNKING_FAILED"
            return original_id, [failed_record], None

        if not macro_chunks:
            no_relevant_record = create_unified_record(ProcessingStatus.NO_RELEVANT, original_id, source)
            no_relevant_record["Macro_Chunk_ID"] = f"{original_id}-NO_CONTENT"
            return original_id, [no_relevant_record], None

        macro_chunks_info = []
        if not is_reprocessing:
            for chunk in macro_chunks:
                macro_chunks_info.append({
                    'Macro_Chunk_ID': chunk['Macro_Chunk_ID'], 'Original_ID': original_id,
                    'Source': source, 'Article_Title': article_title,
                    'Speaker': chunk['Speaker'], 'Macro_Chunk_Text': chunk['Macro_Chunk_Text']
                })
        
        chunks_to_process = []
        if not failed_macro_chunk_ids: 
            chunks_to_process = macro_chunks
        else:
            for chunk in macro_chunks:
                if chunk.get('Macro_Chunk_ID') in failed_macro_chunk_ids:
                    chunks_to_process.append(chunk)
        
        if not chunks_to_process:
             UX.ok(f"âœ… ID {original_id}: æœ¬æ¬¡è¿è¡Œæ— éœ€å¤„ç†æ–°çš„å®è§‚å—ã€‚")
             return original_id, [], None
        
        UX.info(f"ID {original_id}: è®¡åˆ’å¤„ç† {len(chunks_to_process)} ä¸ªå®è§‚å—ã€‚")
        final_data = []  # ç›´æ¥æ„å»ºæœ€ç»ˆæ•°æ®åˆ—è¡¨

        # æ ¹æ®é…ç½®å’Œå®è§‚å—é•¿åº¦é€‰æ‹©å¤„ç†æ–¹å¼
        if ENABLE_BATCH_PROCESSING and len(chunks_to_process) > 1:
            # å°†å®è§‚å—æŒ‰é•¿åº¦åˆ†ç»„ï¼šè¶…çŸ­å—æ‰¹å¤„ç†ï¼Œå…¶ä»–å•ç‹¬å¤„ç†
            short_chunks = []
            normal_chunks = []
            
            for chunk_data in chunks_to_process:
                chunk_text = chunk_data.get('Macro_Chunk_Text', '').strip()
                if not chunk_text:
                    continue
                    
                token_count = count_tokens(chunk_text)
                if token_count < SHORT_TEXT_THRESHOLD:
                    short_chunks.append(chunk_data)
                else:
                    normal_chunks.append(chunk_data)
            
            UX.info(f"ID {original_id}: è¯†åˆ«åˆ° {len(short_chunks)} ä¸ªè¶…çŸ­å—ï¼ˆæ‰¹å¤„ç†ï¼‰ï¼Œ{len(normal_chunks)} ä¸ªæ­£å¸¸å—ï¼ˆå•ç‹¬å¤„ç†ï¼‰")
            
            # å¤„ç†è¶…çŸ­å—ï¼ˆæ‰¹å¤„ç†ï¼‰
            if short_chunks:
                UX.info(f"ID {original_id}: æ‰¹å¤„ç† {len(short_chunks)} ä¸ªè¶…çŸ­å—ï¼Œæ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
                batch_data = await process_chunks_batch(short_chunks, original_id, source, api_service)
                final_data.extend(batch_data)
            
            # å¤„ç†æ­£å¸¸/é•¿å—ï¼ˆå•ç‹¬å¤„ç†ï¼‰
            if normal_chunks:
                UX.info(f"ID {original_id}: å•ç‹¬å¤„ç† {len(normal_chunks)} ä¸ªæ­£å¸¸/é•¿å—")
                for chunk_data in normal_chunks:
                    chunk_text = chunk_data.get('Macro_Chunk_Text', '').strip()
                    if not chunk_text: continue

                    macro_chunk_id = chunk_data.get('Macro_Chunk_ID')
                    speaker = chunk_data.get('Speaker')
                    token_count = count_tokens(chunk_text)
                    language, _ = detect_language_and_get_config(chunk_text)

                    analysis_model_key = 'INTEGRATED_ANALYSIS'
                    if token_count < SHORT_TEXT_THRESHOLD: analysis_model_key = 'INTEGRATED_ANALYSIS_SHORT'
                    elif token_count > LONG_TEXT_THRESHOLD: analysis_model_key = 'INTEGRATED_ANALYSIS_LONG'
                    
                    integrated_prompt = prompts.INTEGRATED_ANALYSIS_V2.replace("{speaker}", speaker).replace("{macro_chunk_text}", chunk_text)
                    
                    analyzed_Units = await api_service.get_analysis(
                        integrated_prompt, 'analyzed_Units', language,
                        model_name=get_stage_model(analysis_model_key), stage_key=analysis_model_key,
                        context_label=f"{macro_chunk_id}:{analysis_model_key}"
                    )
                    
                    if analyzed_Units is None:
                        failed_unit = create_unified_record(ProcessingStatus.API_FAILED, original_id, source, chunk_text[:200], f"å®è§‚å— {macro_chunk_id} åˆ†æå¤±è´¥")
                        failed_unit["Macro_Chunk_ID"] = macro_chunk_id
                        # å¤±è´¥è®°å½•ç›´æ¥æ·»åŠ ï¼Œå®ƒçš„Unit_IDç”±create_unified_recordå†…éƒ¨å®šä¹‰
                        final_data.append(failed_unit)
                        continue
                    
                    if isinstance(analyzed_Units, list):
                        # å¦‚æœAPIè¿”å›ç©ºåˆ—è¡¨ï¼Œåˆ™ä¸æ‰§è¡Œä»»ä½•æ“ä½œï¼Œè‡ªç„¶åœ°å®ç°äº†"è‰¯æ€§è¿‡æ»¤"
                        
                        # [æ ¸å¿ƒä¿®æ”¹] åœ¨å®è§‚å—å†…éƒ¨åˆå§‹åŒ–åºå·è®¡æ•°å™¨
                        unit_counter_in_chunk = 1
                        for u in analyzed_Units:
                            Unit_Text = safe_str_convert(u.get("Unit_Text", ""))
                            if not Unit_Text.strip():
                                # æ‚¨å¤„ç†ç©ºå†…å®¹çš„é€»è¾‘ï¼Œä¹Ÿç›´æ¥åœ¨è¿™é‡Œæ·»åŠ 
                                failed_unit = create_unified_record(
                                    ProcessingStatus.API_FAILED, 
                                    original_id, 
                                    source, 
                                    chunk_text[:200], 
                                    f"å®è§‚å— {macro_chunk_id} è¿”å›ç©ºå†…å®¹"
                                )
                                failed_unit["Macro_Chunk_ID"] = macro_chunk_id
                                final_data.append(failed_unit)
                                continue
                            
                            # æ„å»ºç¨³å®šã€åˆ†å±‚çš„Unit_ID
                            unit_id = f"{macro_chunk_id}-{unit_counter_in_chunk}"
                            
                            # å‡†å¤‡å¹¶ç›´æ¥æ·»åŠ å®Œæ•´çš„å•å…ƒæ•°æ®åˆ° final_data
                            norm_text = normalize_text(Unit_Text)
                            Unit_hash = hashlib.sha256(norm_text.encode('utf-8')).hexdigest()
                            
                            final_data.append({
                                "Unit_ID": unit_id,
                                "Source": source,
                                "speaker": speaker, 
                                "Unit_Text": Unit_Text, 
                                "seed_sentence": u.get("seed_sentence", ""),
                                "expansion_logic": u.get("expansion_logic", ""), 
                                "Macro_Chunk_ID": macro_chunk_id,
                                "Unit_Hash": Unit_hash, 
                                "processing_status": ProcessingStatus.SUCCESS, 
                                "Incident": u.get("Incident", ""),
                                "Frame_ProblemDefinition": u.get("Frame_ProblemDefinition", []),
                                "Frame_ResponsibilityAttribution": u.get("Frame_ResponsibilityAttribution", []),
                                "Frame_MoralEvaluation": u.get("Frame_MoralEvaluation", []),
                                "Frame_SolutionRecommendation": u.get("Frame_SolutionRecommendation", []),
                                "Frame_ActionStatement": u.get("Frame_ActionStatement", []),
                                "Frame_CausalExplanation": u.get("Frame_CausalExplanation", []),
                                "Valence": u.get("Valence", ""), 
                                "Evidence_Type": u.get("Evidence_Type", ""),
                                "Attribution_Level": u.get("Attribution_Level", ""), 
                                "Temporal_Focus": u.get("Temporal_Focus", ""),
                                "Primary_Actor_Type": u.get("Primary_Actor_Type", ""), 
                                "Geographic_Scope": u.get("Geographic_Scope", ""),
                                "Relationship_Model_Definition": u.get("Relationship_Model_Definition", ""),
                                "Discourse_Type": u.get("Discourse_Type", "")
                            })
                            
                            # åºå·é€’å¢
                            unit_counter_in_chunk += 1
        else:
            # å®Œå…¨ç¦ç”¨æ‰¹å¤„ç†ï¼Œæ‰€æœ‰å®è§‚å—å•ç‹¬å¤„ç†
            UX.info(f"ID {original_id}: æ‰¹å¤„ç†å·²ç¦ç”¨æˆ–åªæœ‰å•ä¸ªå®è§‚å—ï¼Œä½¿ç”¨å•ä¸ªå¤„ç†æ¨¡å¼")
            for chunk_data in chunks_to_process:
                chunk_text = chunk_data.get('Macro_Chunk_Text', '').strip()
                if not chunk_text: continue

                macro_chunk_id = chunk_data.get('Macro_Chunk_ID')
                speaker = chunk_data.get('Speaker')
                token_count = count_tokens(chunk_text)
                language, _ = detect_language_and_get_config(chunk_text)

                analysis_model_key = 'INTEGRATED_ANALYSIS'
                if token_count < SHORT_TEXT_THRESHOLD: analysis_model_key = 'INTEGRATED_ANALYSIS_SHORT'
                elif token_count > LONG_TEXT_THRESHOLD: analysis_model_key = 'INTEGRATED_ANALYSIS_LONG'
                
                integrated_prompt = prompts.INTEGRATED_ANALYSIS_V2.replace("{speaker}", speaker).replace("{macro_chunk_text}", chunk_text)
                
                analyzed_Units = await api_service.get_analysis(
                    integrated_prompt, 'analyzed_Units', language,
                    model_name=get_stage_model(analysis_model_key), stage_key=analysis_model_key,
                    context_label=f"{macro_chunk_id}:{analysis_model_key}"
                )
                
                if analyzed_Units is None:
                    failed_unit = create_unified_record(ProcessingStatus.API_FAILED, original_id, source, chunk_text[:200], f"å®è§‚å— {macro_chunk_id} åˆ†æå¤±è´¥")
                    failed_unit["Macro_Chunk_ID"] = macro_chunk_id
                    final_data.append(failed_unit)
                    continue
                
                if isinstance(analyzed_Units, list):
                    unit_counter_in_chunk = 1
                    for u in analyzed_Units:
                        Unit_Text = safe_str_convert(u.get("Unit_Text", ""))
                        if not Unit_Text.strip():
                            failed_unit = create_unified_record(
                                ProcessingStatus.API_FAILED, 
                                original_id, 
                                source, 
                                chunk_text[:200], 
                                f"å®è§‚å— {macro_chunk_id} è¿”å›ç©ºå†…å®¹"
                            )
                            failed_unit["Macro_Chunk_ID"] = macro_chunk_id
                            final_data.append(failed_unit)
                            continue
                        
                        unit_id = f"{macro_chunk_id}-{unit_counter_in_chunk}"
                        norm_text = normalize_text(Unit_Text)
                        Unit_hash = hashlib.sha256(norm_text.encode('utf-8')).hexdigest()
                        
                        final_data.append({
                            "Unit_ID": unit_id,
                            "Source": source,
                            "speaker": speaker, 
                            "Unit_Text": Unit_Text, 
                            "seed_sentence": u.get("seed_sentence", ""),
                            "expansion_logic": u.get("expansion_logic", ""), 
                            "Macro_Chunk_ID": macro_chunk_id,
                            "Unit_Hash": Unit_hash, 
                            "processing_status": ProcessingStatus.SUCCESS, 
                            "Incident": u.get("Incident", ""),
                            "Frame_ProblemDefinition": u.get("Frame_ProblemDefinition", []),
                            "Frame_ResponsibilityAttribution": u.get("Frame_ResponsibilityAttribution", []),
                            "Frame_MoralEvaluation": u.get("Frame_MoralEvaluation", []),
                            "Frame_SolutionRecommendation": u.get("Frame_SolutionRecommendation", []),
                            "Frame_ActionStatement": u.get("Frame_ActionStatement", []),
                            "Frame_CausalExplanation": u.get("Frame_CausalExplanation", []),
                            "Valence": u.get("Valence", ""), 
                            "Evidence_Type": u.get("Evidence_Type", ""),
                            "Attribution_Level": u.get("Attribution_Level", ""), 
                            "Temporal_Focus": u.get("Temporal_Focus", ""),
                            "Primary_Actor_Type": u.get("Primary_Actor_Type", ""), 
                            "Geographic_Scope": u.get("Geographic_Scope", ""),
                            "Relationship_Model_Definition": u.get("Relationship_Model_Definition", ""),
                            "Discourse_Type": u.get("Discourse_Type", "")
                        })
                        
                        unit_counter_in_chunk += 1

        if not final_data:
            no_relevant_record = create_unified_record(ProcessingStatus.NO_RELEVANT, original_id, source)
            if failed_macro_chunk_ids:
                 no_relevant_record['reprocessed_chunks'] = list(failed_macro_chunk_ids)
            return original_id, [no_relevant_record], macro_chunks_info

        return original_id, final_data, macro_chunks_info

    except Exception as e:
        UX.err(f"å¤„ç†ID {original_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return original_id, [create_unified_record(ProcessingStatus.API_FAILED, original_id, source, "", f"é”™è¯¯: {str(e)[:100]}")], None

async def api_worker(name, task_queue, results_queue, api_service, source, pbar, macro_db_path=None, output_file_path=None, macro_chunks_rerun=None, rechunk_article_ids=None):
   while True:
       try:
           item = await task_queue.get()
           if item is None:
               break
           
           try:
               item_with_source = (item[0], item[1], source)
               current_id = safe_str_convert(item[1].get(COLUMN_MAPPING["ID"], "unknown")) if len(item) > 1 else "unknown"
               force_rechunk = (rechunk_article_ids is not None and current_id in rechunk_article_ids)
               original_id, result_Units, macro_chunks_info = await process_row(
                   item_with_source,
                   api_service,
                   macro_db_path,
                   output_file_path,
                   macro_chunks_rerun=macro_chunks_rerun,
                   force_rechunk=force_rechunk
               )
               await results_queue.put((original_id, result_Units, macro_chunks_info))
           except Exception as e:
               UX.warn(f"å·¥ä½œè€… {name} å¤„ç†å¤±è´¥: {e}")
               original_id = safe_str_convert(item[1].get(COLUMN_MAPPING["ID"], "unknown")) if len(item) > 1 else "unknown"
               await results_queue.put((original_id, [create_unified_record(ProcessingStatus.API_FAILED, original_id, source, "", f"Workeré”™è¯¯: {str(e)[:100]}")], None))
           finally:
               # ä¿è¯æ— è®ºæˆåŠŸå¤±è´¥ï¼Œä»»åŠ¡å®Œæˆåéƒ½æ›´æ–°è¿›åº¦æ¡
               pbar.update(1)
               task_queue.task_done()
       
       except asyncio.CancelledError:
           break
       except Exception as e:
           UX.err(f"å·¥ä½œè€… {name} å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

async def saver_worker(results_queue, df_input, output_file_path, macro_db_path, total_tasks=None):
    """
    [ä¼˜åŒ–å] æ‰¹é‡ä¿å­˜workerï¼š
    - ç¼“å†²å¹¶åˆ†æ‰¹ä¿å­˜"åˆ†æç»“æœ"
    - ç¼“å†²å¹¶åˆ†æ‰¹ä¿å­˜"å®è§‚å—"åˆ°ä¸»æ•°æ®åº“
    """
    analysis_buffer = []
    macro_chunk_buffer = []
    received_count = 0
    
    # ä»é…ç½®è¯»å–ç¼“å†²åŒºå¤§å°é˜ˆå€¼
    ANALYSIS_BUFFER_LIMIT = BUFFER_CONFIG.get('analysis_buffer_limit', 30)    # åˆ†æç»“æœç¼“å†²åŒº
    MACRO_CHUNK_BUFFER_LIMIT = BUFFER_CONFIG.get('macro_chunk_buffer_limit', 80)  # å®è§‚å—ç¼“å†²åŒº

    async def save_analysis_batch(data):
        if not data:
            return
        async with file_write_lock:
            # è°ƒç”¨ç°æœ‰çš„åˆ†æç»“æœä¿å­˜å‡½æ•°
            save_data_to_excel(data, df_input, output_file_path)

    async def save_macro_batch(data):
        if not data:
            return
        async with file_write_lock:
            # è°ƒç”¨æ–°å¢çš„å®è§‚å—æ•°æ®åº“ä¿å­˜å‡½æ•°
            save_macro_chunks_database(data, macro_db_path)

    while True:
        try:
            # ç­‰å¾…ç»“æœï¼Œä»é…ç½®è¯»å–è¶…æ—¶æ—¶é—´
            item = await asyncio.wait_for(results_queue.get(), timeout=QUEUE_TIMEOUT)
            if item is None: # æ”¶åˆ°ç»“æŸä¿¡å·
                await save_analysis_batch(analysis_buffer)
                await save_macro_batch(macro_chunk_buffer)
                break

            original_id, result_Units, macro_chunks_info = item
            received_count += 1

            # æ”¶é›†å®è§‚å—åˆ°ç¼“å†²åŒº
            if macro_chunks_info:
                macro_chunk_buffer.extend(macro_chunks_info)

            # æ”¶é›†åˆ†æç»“æœåˆ°ç¼“å†²åŒº
            if result_Units:
                for unit in result_Units:
                    unit[COLUMN_MAPPING["ID"]] = original_id
                analysis_buffer.extend(result_Units)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç©ºå¹¶ä¿å­˜ç¼“å†²åŒº
            if len(analysis_buffer) >= ANALYSIS_BUFFER_LIMIT:
                await save_analysis_batch(analysis_buffer)
                UX.info(f"æ‰¹é‡ä¿å­˜åˆ†æç»“æœ: {len(analysis_buffer)} æ¡")
                analysis_buffer = []

            if len(macro_chunk_buffer) >= MACRO_CHUNK_BUFFER_LIMIT:
                await save_macro_batch(macro_chunk_buffer)
                UX.info(f"æ‰¹é‡ä¿å­˜å®è§‚å—: {len(macro_chunk_buffer)} æ¡")
                macro_chunk_buffer = []

            results_queue.task_done()
            if total_tasks and received_count >= total_tasks:
                await save_analysis_batch(analysis_buffer)
                await save_macro_batch(macro_chunk_buffer)
                break

        except asyncio.TimeoutError:
            # è¶…æ—¶åä¿å­˜æ‰€æœ‰ç¼“å†²åŒºå†…å®¹ï¼Œé˜²æ­¢åœ¨ä»»åŠ¡é—´éš™ä¸¢å¤±æ•°æ®
            await save_analysis_batch(analysis_buffer)
            analysis_buffer = []
            await save_macro_batch(macro_chunk_buffer)
            macro_chunk_buffer = []
            if total_tasks and received_count >= total_tasks:
                break
            continue

        except asyncio.CancelledError:
            await save_analysis_batch(analysis_buffer)
            await save_macro_batch(macro_chunk_buffer)
            break

        except Exception as e:
            UX.err(f"ä¿å­˜çº¿ç¨‹é”™è¯¯: {e}")

def save_data_to_excel(new_Units_list, df_input, output_file_path):
   try:
       df_existing = pd.DataFrame()
       if os.path.exists(output_file_path):
           try:
               df_existing = pd.read_excel(output_file_path)
           except Exception as e:
               UX.warn(f"è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}")
       df_new_Units = pd.DataFrame(new_Units_list)

       if df_new_Units.empty:
           return

       df_existing = ensure_required_columns(df_existing)
       df_new_Units = ensure_required_columns(df_new_Units)
       
       if COLUMN_MAPPING["ID"] in df_input.columns and COLUMN_MAPPING["ID"] in df_new_Units.columns:
           df_input[COLUMN_MAPPING["ID"]] = df_input[COLUMN_MAPPING["ID"]].astype(str)
           df_new_Units[COLUMN_MAPPING["ID"]] = df_new_Units[COLUMN_MAPPING["ID"]].astype(str)
           new_original_ids = df_new_Units[COLUMN_MAPPING["ID"]].unique()

           input_base_cols = [col for col in df_input.columns if col in df_new_Units.columns and col != COLUMN_MAPPING["ID"]]
           if input_base_cols:
               df_new_Units = df_new_Units.drop(columns=input_base_cols)
           
           # [æœ€ç»ˆä¿®å¤] å…¨æ–°çš„ã€æ›´å¥å£®çš„æ¸…ç†é€»è¾‘
           if not df_existing.empty and 'Macro_Chunk_ID' in df_existing.columns:
               chunks_to_clean_up = set()
               
               # 1. ä»æˆåŠŸå’Œå¤±è´¥çš„æ–°è®°å½•ä¸­ï¼Œç›´æ¥è·å–å®è§‚å—ID
               if 'Macro_Chunk_ID' in df_new_Units.columns:
                   chunks_to_clean_up.update(df_new_Units['Macro_Chunk_ID'].dropna().astype(str))

               # 2. ä»NO_RELEVANTè®°å½•çš„"ä¾¿ç­¾"ä¸­è·å–éœ€è¦æ¸…ç†çš„æ—§å®è§‚å—ID
               if 'reprocessed_chunks' in df_new_Units.columns:
                   no_relevant_rows = df_new_Units[df_new_Units['processing_status'] == ProcessingStatus.NO_RELEVANT]
                   if not no_relevant_rows.empty:
                       for chunk_list in no_relevant_rows['reprocessed_chunks'].dropna():
                           if isinstance(chunk_list, list):
                               chunks_to_clean_up.update(chunk_list)
               
               if chunks_to_clean_up:
                   # æ‰§è¡Œæ¸…ç†
                   initial_rows = len(df_existing)
                   df_existing = df_existing[~df_existing['Macro_Chunk_ID'].astype(str).isin(chunks_to_clean_up)]
                   rows_removed = initial_rows - len(df_existing)
                   if rows_removed > 0:
                       UX.info(f"æ•°æ®æ¸…ç†ï¼šç§»é™¤äº† {rows_removed} æ¡ä¸æœ¬æ¬¡é‡å¤„ç†ç›¸å…³çš„æ—§è®°å½•ã€‚")
           
           df_input_subset = df_input[df_input[COLUMN_MAPPING["ID"]].isin(new_original_ids)].copy()
           df_newly_merged = pd.merge(df_input_subset, df_new_Units, on=COLUMN_MAPPING["ID"], how='left')
       else:
           df_newly_merged = df_new_Units.copy()

       df_final_to_save = pd.concat([df_existing, df_newly_merged], ignore_index=True)

       if 'Unit_ID' in df_final_to_save.columns:
           df_final_to_save = df_final_to_save.drop_duplicates(subset=['Unit_ID'], keep='last')

       df_final_to_save = reorder_columns(df_final_to_save, list(df_input.columns))

       if 'reprocessed_chunks' in df_final_to_save.columns:
           df_final_to_save = df_final_to_save.drop(columns=['reprocessed_chunks'])

       df_final_to_save.to_excel(output_file_path, index=False)
       UX.ok(f"å·²ä¿å­˜: {os.path.basename(output_file_path)} (ç´¯è®¡ {len(df_final_to_save)} æ¡)")

   except Exception as e:
       UX.err(f"Excelä¿å­˜å¤±è´¥: {e}")

async def main_async():
   UX.start_run()
   UX.phase("é•¿æ–‡æœ¬åˆ†æå™¨å¯åŠ¨")
   UX.info(f"ä¿¡åº¦æ£€éªŒæ¨¡å¼: {'å¼€å¯' if RELIABILITY_TEST_MODE else 'å…³é—­'}")

   # éªŒè¯æ¨¡å‹é…ç½®
   required_keys = ["MACRO_CHUNKING", "INTEGRATED_ANALYSIS", "INTEGRATED_ANALYSIS_SHORT"]
   # æ£€æŸ¥æ–°çš„model_poolsé…ç½®
   primary_models = MODEL_POOLS.get('primary_models', {})
   missing = [k for k in required_keys if k not in primary_models]
   if missing:
       raise ValueError(f"ç¼ºå°‘æ¨¡å‹æ± é…ç½®: {missing}")

   # æ£€æŸ¥APIå¯†é’¥
   if not API_CONFIG["API_KEYS"] or not API_CONFIG["API_KEYS"][0]:
       UX.err("æœªæä¾›æœ‰æ•ˆAPIå¯†é’¥")
       return

   # ç¡®å®šè¾“å…¥æ–‡ä»¶
   if os.path.isdir(INPUT_PATH):
       files_to_process = glob.glob(os.path.join(INPUT_PATH, '*.xlsx'))
       is_folder_mode = True
       os.makedirs(OUTPUT_PATH, exist_ok=True)
   elif os.path.isfile(INPUT_PATH):
       files_to_process = [INPUT_PATH]
       is_folder_mode = False
   else:
       UX.err("è¾“å…¥è·¯å¾„æ— æ•ˆ")
       return

   async with aiohttp.ClientSession() as session:
       api_service = APIService(session)

       for file_path in files_to_process:
           file_basename = os.path.basename(file_path)
           UX.phase(f"å¤„ç†æ–‡ä»¶: {file_basename}")

           # è¯†åˆ«ä¿¡æº
           source = identify_source(file_basename)
           UX.info(f"è¯†åˆ«ä¿¡æº: {source}")

           output_file_path = os.path.join(OUTPUT_PATH, f"(ä¸èƒ½åˆ )analyzed_{file_basename}") if is_folder_mode else OUTPUT_PATH

           # è¯»å–è¾“å…¥æ–‡ä»¶
           try:
               df_input = pd.read_excel(file_path)
               if COLUMN_MAPPING["MEDIA_TEXT"] not in df_input.columns:
                   UX.err("æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„æ–‡æœ¬åˆ—")
                   continue
           except Exception as e:
               UX.err(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
               continue

           # å®šä¹‰å®è§‚å—æ•°æ®åº“è·¯å¾„
           macro_db_path = os.path.join(OUTPUT_PATH, '(ä¸èƒ½åˆ )åª’ä½“_å®è§‚å—ä¸»æ•°æ®åº“.xlsx')

           # ğŸ” æ„å»ºæ–­ç‚¹ç»­ä¼ è®¡åˆ’
           UX.info("ğŸ” æ„å»ºæ–­ç‚¹ç»­ä¼ è®¡åˆ’...")
           total_input_articles = len(set(df_input[COLUMN_MAPPING["ID"]].astype(str)))
           never_processed_ids, rechunk_article_ids, macro_chunks_to_rerun = build_resume_plan(
               output_file_path, df_input, COLUMN_MAPPING["ID"]
           )

           ids_to_process = set(never_processed_ids) | set(rechunk_article_ids) | set(macro_chunks_to_rerun.keys())

           UX.info(f"ğŸ“Š è®¡åˆ’ï¼šæ€»æ–‡ç« æ•° {total_input_articles}")
           UX.info(f"   ä»æœªå¤„ç†: {len(never_processed_ids)} ç¯‡")
           UX.info(f"   éœ€é‡åˆ‡åˆ†: {len(rechunk_article_ids)} ç¯‡")
           rerun_chunks_count = sum(len(v) for v in macro_chunks_to_rerun.values())
           UX.info(f"   å®è§‚å—é‡åˆ†æ: {rerun_chunks_count} ä¸ªï¼Œæ¶‰åŠ {len(macro_chunks_to_rerun)} ç¯‡")
           UX.info(f"   æœ¬æ¬¡å¤„ç†æ–‡ç« : {len(ids_to_process)} ç¯‡ï¼Œè·³è¿‡ {total_input_articles - len(ids_to_process)} ç¯‡")

           df_to_process = df_input[df_input[COLUMN_MAPPING["ID"]].astype(str).isin(ids_to_process)].copy()

           if len(df_to_process) > 0:
               UX.info(f"ğŸš€ KISSæ–­ç‚¹ç»­ä¼ : å¤„ç†{len(df_to_process)}ç¯‡æ–‡ç«  (process_rowå°†è‡ªåŠ¨è·³è¿‡å·²æˆåŠŸçš„å®è§‚å—)")

               # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
               task_queue = asyncio.Queue()
               results_queue = asyncio.Queue()
               total_tasks = len(df_to_process)

               # æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—
               for item in df_to_process.iterrows():
                   await task_queue.put(item)

               # åˆ›å»ºè¿›åº¦æ¡
               pbar = aio_tqdm(total=total_tasks, desc=f"å¤„ç†ä¸­ ({file_basename})")

               # [ä¿®æ”¹] åˆ›å»ºä¿å­˜ä»»åŠ¡ - ä¼ å…¥å®è§‚å—æ•°æ®åº“è·¯å¾„
               saver_task = asyncio.create_task(
                   saver_worker(results_queue, df_input, output_file_path, macro_db_path, total_tasks)
               )

               # åˆ›å»ºå·¥ä½œä»»åŠ¡
               worker_tasks = [
                   asyncio.create_task(
                       api_worker(
                           f'worker-{i}',
                           task_queue,
                           results_queue,
                           api_service,
                           source,
                           pbar, # <-- å°†è¿›åº¦æ¡å¯¹è±¡ä¼ é€’ç»™worker
                           macro_db_path,
                           output_file_path,
                           macro_chunks_rerun=macro_chunks_to_rerun,
                           rechunk_article_ids=rechunk_article_ids
                       )
                   )
                   for i in range(MAX_CONCURRENT_REQUESTS)
               ]

               # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡éƒ½è¢«workerå¤„ç†å®Œæ¯•
               await task_queue.join()
               
               pbar.close()
           
           elif len(ids_to_process) == 0:
               UX.ok("ğŸ‰ è¯¥æ–‡ä»¶æ‰€æœ‰æ¡ç›®å·²å®Œç¾å¤„ç†å®Œæ¯•ï¼")
               continue
           
           # ğŸ‰ å¤„ç†å®Œæˆæ€»ç»“
           try:
               df_final_check = pd.read_excel(output_file_path)
               if not df_final_check.empty and 'processing_status' in df_final_check.columns:
                   final_success = (df_final_check['processing_status'] == ProcessingStatus.SUCCESS).sum()
                   final_no_relevant = (df_final_check['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                   final_failed = (df_final_check['processing_status'] == ProcessingStatus.API_FAILED).sum()
                   final_total_units = len(df_final_check)
                   
                   # è®¡ç®—æ–‡ç« çº§åˆ«å®Œæˆåº¦
                   final_processed_ids, final_failed_ids = get_processing_state(df_final_check, COLUMN_MAPPING["ID"])
                   final_completed_articles = len(final_processed_ids - final_failed_ids)
                   final_completion_rate = (final_completed_articles / max(1, total_input_articles)) * 100
                   
                   UX.ok(f"ğŸ“‹ æ–‡ä»¶ {file_basename} å¤„ç†å®Œæˆæ€»ç»“:")
                   UX.ok(f"   ğŸ“Š æ–‡ç« å®Œæˆåº¦: {final_completed_articles}/{total_input_articles} ({final_completion_rate:.1f}%)")
                   UX.ok(f"   ğŸ¯ è®®é¢˜å•å…ƒ: æˆåŠŸ{final_success}æ¡, æ— ç›¸å…³{final_no_relevant}æ¡, å¤±è´¥{final_failed}æ¡")
                   
                   if final_failed_ids:
                       UX.warn(f"   âš ï¸  ä»æœ‰{len(final_failed_ids)}ç¯‡æ–‡ç« å¤„ç†å¤±è´¥ï¼Œå¯å†æ¬¡è¿è¡Œè¿›è¡Œæ™ºèƒ½é‡è¯•")
                   else:
                       UX.ok(f"   âœ¨ å®Œç¾ï¼æ‰€æœ‰æ–‡ç« å‡å·²æˆåŠŸå¤„ç†")
               else:
                   UX.ok(f"æ–‡ä»¶ {file_basename} å¤„ç†å®Œæˆ")
           except Exception:
               UX.ok(f"æ–‡ä»¶ {file_basename} å¤„ç†å®Œæˆ")

   # ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶
   if RELIABILITY_TEST_MODE:
       UX.phase("ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶")

       # å®è§‚å—æ•°æ®åº“è·¯å¾„ï¼ˆç°åœ¨ç”±å®æ—¶ä¿å­˜ç”Ÿæˆï¼‰
       macro_db_path = os.path.join(OUTPUT_PATH, '(ä¸èƒ½åˆ )åª’ä½“_å®è§‚å—ä¸»æ•°æ®åº“.xlsx')

       # åˆå¹¶æ‰€æœ‰ç»“æœæ–‡ä»¶
       final_results_files = glob.glob(os.path.join(OUTPUT_PATH, "*analyzed_*.xlsx"))
       if final_results_files:
           all_results = []
           for file in final_results_files:
               df = pd.read_excel(file)
               df = ensure_required_columns(df)
               source = identify_source(os.path.basename(file))
               df['Source'] = source
               all_results.append(df)

           if all_results:
               df_all_results = pd.concat(all_results, ignore_index=True)
               df_all_results = ensure_required_columns(df_all_results)
               df_all_results = reorder_columns(df_all_results, [])

               combined_results_path = os.path.join(OUTPUT_PATH, 'åª’ä½“_æœ€ç»ˆåˆ†ææ•°æ®åº“.xlsx')
               df_all_results.to_excel(combined_results_path, index=False)

               if 'processing_status' in df_all_results.columns:
                   success_count = (df_all_results['processing_status'] == ProcessingStatus.SUCCESS).sum()
                   no_relevant_count = (df_all_results['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                   failed_count = (df_all_results['processing_status'] == ProcessingStatus.API_FAILED).sum()
                   UX.ok(f"æœ€ç»ˆæ•°æ®åº“å·²ä¿å­˜: {combined_results_path}")
                   UX.info(f"æ€»è®°å½•: {len(df_all_results)}, æˆåŠŸ: {success_count}, æ— ç›¸å…³: {no_relevant_count}, å¤±è´¥: {failed_count}")

               # ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶ - ä½¿ç”¨æ­£ç¡®çš„å®è§‚å—æ•°æ®
               if macro_db_path and os.path.exists(macro_db_path):
                   generate_reliability_files_cn(macro_db_path, combined_results_path, OUTPUT_PATH)
               else:
                   UX.err("å®è§‚å—æ•°æ®åº“ä¸å­˜åœ¨ï¼Œæ— æ³•ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶")

           else:
               UX.warn("åˆå¹¶ç»“æœæ–‡ä»¶å¤±è´¥")
       else:
           UX.warn("æœªæ‰¾åˆ°ä»»ä½•analyzedç»“æœæ–‡ä»¶")

   UX.phase("æ‰€æœ‰ä»»åŠ¡å®Œæˆ")

# ============================================================================
# ä¿¡åº¦æ£€éªŒæ ¸å¿ƒä¿®å¤ä»£ç 
# ============================================================================

def _highlight_Unit_in_parent(parent_text: str, Unit_Text: str) -> str:
    if not parent_text or not Unit_Text:
        return parent_text

    parent_str = str(parent_text).strip()
    unit_str = str(Unit_Text).strip()

    # è§„èŒƒåŒ–ç©ºæ ¼ç”¨äºæ¯”è¾ƒ
    def normalize(s):
        return ' '.join(s.split())

    parent_norm = normalize(parent_str)
    unit_norm = normalize(unit_str)

    # æƒ…å†µ1ï¼šè®®é¢˜å•å…ƒå°±æ˜¯æ•´ä¸ªå®è§‚å—
    if parent_norm == unit_norm or len(parent_norm) - len(unit_norm) < 10:
        return f"ã€{parent_str}ã€‘"

    # æƒ…å†µ2ï¼šè®®é¢˜å•å…ƒæ˜¯å®è§‚å—çš„ä¸€éƒ¨åˆ†
    # ç”±äºè®®é¢˜å•å…ƒæ˜¯ä»å®è§‚å—æå–çš„ï¼Œç†è®ºä¸Šä¸€å®šèƒ½æ‰¾åˆ°
    # å°è¯•ç›´æ¥æŸ¥æ‰¾
    if unit_str in parent_str:
        return parent_str.replace(unit_str, f"ã€{unit_str}ã€‘", 1)

    # å¤„ç†å¯èƒ½çš„ç©ºæ ¼/æ¢è¡Œå·®å¼‚
    if unit_norm in parent_norm:
        # æ‰¾åˆ°å½’ä¸€åŒ–åçš„ä½ç½®
        idx = parent_norm.find(unit_norm)
        # åœ¨åŸæ–‡ä¸­æ‰¾åˆ°å¯¹åº”ä½ç½®ï¼ˆè€ƒè™‘ç©ºæ ¼å·®å¼‚ï¼‰
        words_before = len(parent_norm[:idx].split())
        # åœ¨åŸæ–‡ä¸­å®šä½
        words = parent_str.split()
        if words_before < len(words):
            # é‡å»ºé«˜äº®æ–‡æœ¬
            result_parts = []
            word_count = 0
            in_highlight = False
            unit_words = unit_norm.split()

            for word in parent_str.split():
                if word_count == words_before and not in_highlight:
                    result_parts.append("ã€")
                    in_highlight = True

                result_parts.append(word)

                if in_highlight:
                    if word_count >= words_before + len(unit_words) - 1:
                        result_parts.append("ã€‘")
                        in_highlight = False

                word_count += 1

            return ' '.join(result_parts)

    # æœ€åçš„å¤‡é€‰ï¼šæ¨¡ç³ŠåŒ¹é…
    # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç‰‡æ®µå¹¶é«˜äº®
    import difflib

    # å°†æ–‡æœ¬åˆ†å¥
    sentences = [s.strip() for s in re.split(r'[ã€‚ï¼ï¼Ÿ.!?\n]', parent_str) if s.strip()]
    unit_sentences = [s.strip() for s in re.split(r'[ã€‚ï¼ï¼Ÿ.!?\n]', unit_str) if s.strip()]

    if unit_sentences:
        # æ‰¾åˆ°åŒ…å«è®®é¢˜å•å…ƒç¬¬ä¸€å¥çš„ä½ç½®
        first_unit_sentence = unit_sentences[0]
        best_match_idx = -1
        best_ratio = 0

        for i, sent in enumerate(sentences):
            ratio = difflib.SequenceMatcher(None, sent, first_unit_sentence).ratio()
            if ratio > best_ratio:
                best_ratio = ratio
                best_match_idx = i

        similarity_threshold = QUALITY_THRESHOLDS.get('text_similarity_threshold', 0.7)

        if best_match_idx >= 0 and best_ratio > similarity_threshold:
            # ä»æ‰¾åˆ°çš„ä½ç½®å¼€å§‹é«˜äº®ç›¸åº”é•¿åº¦çš„æ–‡æœ¬
            highlighted_sentences = sentences.copy()
            num_sentences_to_highlight = len(unit_sentences)

            for j in range(best_match_idx, min(best_match_idx + num_sentences_to_highlight, len(sentences))):
                highlighted_sentences[j] = f"ã€{sentences[j]}ã€‘"

            # é‡ç»„æ–‡æœ¬
            result = ""
            for sent in highlighted_sentences:
                if sent:
                    result += sent + "ã€‚"

            return result.rstrip("ã€‚")

    # å¦‚æœå®åœ¨æ‰¾ä¸åˆ°ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œè¿”å›å¸¦æ ‡è®°çš„åŸæ–‡
    UX.warn(f"è­¦å‘Šï¼šæ— æ³•åœ¨å®è§‚å—ä¸­å®šä½è®®é¢˜å•å…ƒ")
    return f"{parent_str}\n\n[è®®é¢˜å•å…ƒï¼šã€{unit_str}ã€‘]"

# æœ¬åœ°åŒ–æ–‡æœ¬ç¼“å­˜
_locale_cache = {}

def _load_locale_mapping(lang: str) -> dict:
    """ä»JSONæ–‡ä»¶åŠ è½½æœ¬åœ°åŒ–æ˜ å°„"""
    if lang in _locale_cache:
        return _locale_cache[lang]

    locales_dir = os.path.join(os.path.dirname(__file__), 'locales')
    file_path = os.path.join(locales_dir, f'{lang}.json')

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            mapping = json.load(f)
            _locale_cache[lang] = mapping
            return mapping
    except Exception as e:
        print(f"åŠ è½½æœ¬åœ°åŒ–æ–‡ä»¶ {lang}.json å¤±è´¥: {e}")
        return {}

def _decorate_headers(df: pd.DataFrame, lang: str) -> pd.DataFrame:
    """è£…é¥°åˆ—åä¸ºè¯­è¨€æ ‡ç­¾æ ¼å¼"""
    mapping = _load_locale_mapping(lang)
    new_cols = []

    for c in list(df.columns):
        if c in mapping:
            new_cols.append(f"{mapping[c]}({c})")
        else:
            new_cols.append(c)

    df_out = df.copy()
    df_out.columns = new_cols
    return df_out

def _save_bilingual(df: pd.DataFrame, zh_path: str, ru_path: str):
    """ä¿å­˜åŒè¯­ç‰ˆæœ¬æ–‡ä»¶"""
    try:
        zh_dir = os.path.dirname(zh_path)
        ru_dir = os.path.dirname(ru_path)

        if zh_dir:
            os.makedirs(zh_dir, exist_ok=True)

        if ru_dir and ru_dir != zh_dir:
            os.makedirs(ru_dir, exist_ok=True)
    except Exception as e:
        UX.warn(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {e}")

    try:
        df_zh = _decorate_headers(df, 'zh')
        df_zh.to_excel(zh_path, index=False)
    except Exception as e:
        UX.warn(f"ä¸­æ–‡ç‰ˆæœ¬å¯¼å‡ºå¤±è´¥: {e}")

    try:
        df_ru = _decorate_headers(df, 'ru')
        df_ru.to_excel(ru_path, index=False)
    except Exception as e:
        UX.warn(f"ä¿„è¯­ç‰ˆæœ¬å¯¼å‡ºå¤±è´¥: {e}")

def generate_reliability_files_cn(macro_db_path: str, final_results_path: str, output_path: str):
    """ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶ - ä¸å†å°è¯•é”™è¯¯çš„é‡å»º"""
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(macro_db_path):
        UX.err(f"å®è§‚å—æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {macro_db_path}")
        UX.err("è¯·ç¡®ä¿ç¨‹åºæ­£ç¡®ä¿å­˜äº†å®è§‚å—æ•°æ®")
        return

    if not os.path.exists(final_results_path):
        UX.err(f"æœ€ç»ˆç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {final_results_path}")
        return

    try:
        os.makedirs(output_path, exist_ok=True)
    except Exception as e:
        UX.warn(f"åˆ›å»ºä¿¡åº¦è¾“å‡ºç›®å½•å¤±è´¥: {e}")
        return

    UX.info("å¼€å§‹ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶ï¼ˆä¸­æ–‡+ä¿„è¯­åŒè¯­ç‰ˆï¼‰...")

    # è¯»å–æ•°æ®
    try:
        df_macro = pd.read_excel(macro_db_path)
        df_results = pd.read_excel(final_results_path)

        UX.info(f"åŠ è½½å®è§‚å—æ•°æ®: {len(df_macro)} æ¡")
        UX.info(f"åŠ è½½åˆ†æç»“æœ: {len(df_results)} æ¡")

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        if 'Macro_Chunk_Text' not in df_macro.columns:
            UX.err("å®è§‚å—æ•°æ®ç¼ºå°‘Macro_Chunk_Textåˆ— - æ•°æ®ä¸å®Œæ•´")
            return

        # æ£€æŸ¥å®è§‚å—æ–‡æœ¬é•¿åº¦ï¼ˆå­—ç¬¦æ•°å’Œtokenæ•°ï¼‰
        macro_texts = df_macro['Macro_Chunk_Text'].astype(str)
        avg_char_len = macro_texts.str.len().mean()
        avg_token_len = macro_texts.apply(count_tokens).mean()

        UX.info(f"å®è§‚å—å¹³å‡é•¿åº¦: {avg_char_len:.0f} å­—ç¬¦, {avg_token_len:.0f} tokens")

        min_chars_threshold = QUALITY_THRESHOLDS.get('min_macro_chunk_chars', 50)
        if avg_char_len < min_chars_threshold:
            UX.warn(f"å®è§‚å—æ–‡æœ¬å¼‚å¸¸çŸ­ï¼ˆå¹³å‡{avg_char_len:.0f}å­—ç¬¦ < {min_chars_threshold}å­—ç¬¦ï¼‰ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®é—®é¢˜")

    except Exception as e:
        UX.err(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return

    # =========== 1. è¯»å–å’Œå‡†å¤‡æ•°æ® ===========
    import pickle

    sampled_cache = os.path.join(output_path, '.sampled_ids.pkl')
    sampled_records = {'recall': set(), 'precision': set()}

    if os.path.exists(sampled_cache):
        try:
            with open(sampled_cache, 'rb') as f:
                loaded = pickle.load(f)
                if isinstance(loaded, dict):
                    sampled_records.update(loaded)
                    UX.info(f"è¯»å–å·²æŠ½æ ·è®°å½•: å¬å›{len(sampled_records.get('recall', set()))}æ¡, "
                           f"ç²¾ç¡®{len(sampled_records.get('precision', set()))}æ¡")
        except Exception as e:
            UX.warn(f"è¯»å–æŠ½æ ·è®°å½•å¤±è´¥: {e}")

    # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
    if 'Macro_Chunk_ID' not in df_macro.columns:
        UX.err("å®è§‚å—æ•°æ®ç¼ºå°‘Macro_Chunk_IDåˆ—")
        return

    if 'processing_status' not in df_results.columns:
        UX.warn("ç»“æœæ•°æ®ç¼ºå°‘processing_statusåˆ—ï¼Œå°è¯•å…¼å®¹æ—§ç‰ˆæœ¬")
        # å°è¯•é€šè¿‡speakerå­—æ®µåˆ¤æ–­çŠ¶æ€
        if 'speaker' in df_results.columns:
            df_results['processing_status'] = df_results.apply(
                lambda x: ProcessingStatus.API_FAILED if 'API_CALL_FAILED' in str(x.get('speaker', ''))
                else ProcessingStatus.SUCCESS, axis=1
            )
        else:
            UX.err("æ— æ³•ç¡®å®šå¤„ç†çŠ¶æ€")
            return

    # æ ‡è®°ä½¿ç”¨æƒ…å†µ
    used_macro_ids = set()
    excluded_original_ids = set()  # è¢«åˆ¤å®šä¸ºNO_RELEVANTçš„åŸå§‹ID

    if 'processing_status' in df_results.columns:
        # æ‰¾å‡ºæˆåŠŸå¤„ç†çš„å®è§‚å—
        success_mask = df_results['processing_status'] == ProcessingStatus.SUCCESS
        if 'Macro_Chunk_ID' in df_results.columns:
            used_macro_ids = set(df_results[success_mask]['Macro_Chunk_ID'].dropna().astype(str).unique())

        # æ‰¾å‡ºè¢«åˆ¤å®šä¸ºNO_RELEVANTçš„åŸå§‹æ–‡ç« ID
        no_relevant_mask = df_results['processing_status'] == ProcessingStatus.NO_RELEVANT
        if no_relevant_mask.any() and COLUMN_MAPPING["ID"] in df_results.columns:
            excluded_original_ids = set(df_results[no_relevant_mask][COLUMN_MAPPING["ID"]].dropna().astype(str).unique())

    # åœ¨å®è§‚å—æ•°æ®ä¸­æ ‡è®°
    df_macro['Was_Used'] = df_macro['Macro_Chunk_ID'].astype(str).isin(used_macro_ids)
    df_macro['Article_Was_Excluded'] = df_macro['Original_ID'].astype(str).isin(excluded_original_ids)

    # =========== 2. åå‘æ£€éªŒï¼ˆå¬å›ç‡ï¼‰===========
    UX.info("ç”Ÿæˆåå‘æ£€éªŒæ ·æœ¬...")

    negative_samples = []
    new_recall_ids = set()

    for source, cfg in RELIABILITY_SAMPLING_CONFIG.items():
        # é€‰æ‹©æœªä½¿ç”¨æˆ–è¢«æ’é™¤çš„å®è§‚å—
        mask_source = df_macro['Source'] == source
        mask_not_used = (~df_macro['Was_Used']) | df_macro['Article_Was_Excluded']
        mask_not_sampled = ~df_macro['Macro_Chunk_ID'].astype(str).isin(sampled_records['recall'])

        candidates = df_macro[mask_source & mask_not_used & mask_not_sampled]
        n_sample = min(int(cfg.get('recall', 0)), len(candidates))

        if n_sample > 0:
            random_seed = RANDOMIZATION_CONFIG.get('random_seed', 2025)
            sample_df = candidates.sample(n=n_sample, replace=False, random_state=random_seed)
            negative_samples.append(sample_df)
            new_recall_ids.update(sample_df['Macro_Chunk_ID'].astype(str).tolist())

    if negative_samples:
        df_neg = pd.concat(negative_samples, ignore_index=True)
        df_neg = df_neg.drop_duplicates(subset=['Macro_Chunk_ID'], keep='first')

        # æ„å»ºè¾“å‡ºæ•°æ®
        df_neg_out = pd.DataFrame()
        df_neg_out['Macro_Chunk_ID'] = df_neg['Macro_Chunk_ID']
        df_neg_out['Source'] = df_neg['Source']
        df_neg_out['Original_ID'] = df_neg['Original_ID']
        df_neg_out['Article_Title'] = df_neg.get('Article_Title', '')

        # æ·»åŠ AIå†³ç­–æ ‡è®°
        df_neg_out['AI_Decision'] = df_neg.apply(
            lambda x: 'AIåˆ¤å®šæ•´ç¯‡æ–‡ç« æ— å…³' if x['Article_Was_Excluded'] else 'æœªè¢«ä½¿ç”¨çš„å®è§‚å—',
            axis=1
        )

        df_neg_out['Speaker'] = df_neg['Speaker']
        df_neg_out['Macro_Chunk_Text'] = df_neg['Macro_Chunk_Text']

        # æ£€éªŒå‘˜å­—æ®µ
        df_neg_out['Inspector_Is_CN_RU_Related'] = ''  # æ˜¯/å¦
        df_neg_out['Inspector_Should_Include'] = ''    # åº”è¯¥åŒ…å«/ä¸åº”è¯¥åŒ…å«
        df_neg_out['Inspector_Comments'] = ''           # å¤‡æ³¨

        # ä¿å­˜æ–‡ä»¶
        zh_neg = os.path.join(output_path, 'åå‘æ£€éªŒ_å¬å›ç‡æ ·æœ¬.xlsx')
        ru_neg = os.path.join(output_path, 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°_Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ_Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°(Recall).xlsx')
        _save_bilingual(df_neg_out, zh_neg, ru_neg)

        UX.ok(f"åå‘æ£€éªŒæ ·æœ¬å·²ç”Ÿæˆ: {len(df_neg_out)} æ¡")
        ai_excluded = (df_neg_out['AI_Decision'] == 'AIåˆ¤å®šæ•´ç¯‡æ–‡ç« æ— å…³').sum()
        UX.info(f"å…¶ä¸­ {ai_excluded} æ¡æ¥è‡ªAIåˆ¤å®šæ— å…³çš„æ–‡ç« ")

    # =========== 3. æ­£å‘æ£€éªŒï¼ˆç²¾ç¡®ç‡ä¸è¾¹ç•Œï¼‰===========
    UX.info("ç”Ÿæˆæ­£å‘æ£€éªŒæ ·æœ¬...")

    # ç­›é€‰æˆåŠŸå¤„ç†çš„è®®é¢˜å•å…ƒ
    df_pos_pool = df_results[df_results['processing_status'] == ProcessingStatus.SUCCESS].copy()

    positive_samples = []
    new_precision_ids = set()

    for source, cfg in RELIABILITY_SAMPLING_CONFIG.items():
        mask_source = df_pos_pool['Source'] == source
        mask_not_sampled = ~df_pos_pool['Unit_ID'].astype(str).isin(sampled_records['precision'])

        candidates = df_pos_pool[mask_source & mask_not_sampled]
        n_sample = min(int(cfg.get('precision', 0)), len(candidates))

        if n_sample > 0:
            random_seed = RANDOMIZATION_CONFIG.get('random_seed', 2025)
            sample_df = candidates.sample(n=n_sample, replace=False, random_state=random_seed)
            positive_samples.append(sample_df)
            new_precision_ids.update(sample_df['Unit_ID'].astype(str).tolist())

    if positive_samples:
        df_pos = pd.concat(positive_samples, ignore_index=True)
        df_pos = df_pos.drop_duplicates(subset=['Unit_ID'], keep='first')

        # åˆ›å»ºå®è§‚å—ç´¢å¼•
        df_macro_dict = df_macro.set_index('Macro_Chunk_ID').to_dict('index')

        # ç”Ÿæˆé«˜äº®æ–‡æœ¬
        highlighted_texts = []

        for _, row in df_pos.iterrows():
            macro_id = str(row.get('Macro_Chunk_ID', ''))
            unit_text = str(row.get('Unit_Text', ''))

            if macro_id in df_macro_dict:
                parent_text = str(df_macro_dict[macro_id].get('Macro_Chunk_Text', ''))
                highlighted = _highlight_Unit_in_parent(parent_text, unit_text)
            else:
                highlighted = f"[æœªæ‰¾åˆ°å®è§‚å—]\nè®®é¢˜å•å…ƒï¼šã€{unit_text}ã€‘"

            highlighted_texts.append(highlighted)

        # æ„å»ºè¾“å‡ºæ•°æ®
        df_pos_out = pd.DataFrame()
        df_pos_out['Unit_ID'] = df_pos['Unit_ID']
        df_pos_out['Source'] = df_pos['Source']
        df_pos_out['Macro_Chunk_ID'] = df_pos['Macro_Chunk_ID']
        df_pos_out['Parent_Macro_Chunk_Text_Highlighted'] = highlighted_texts
        df_pos_out['Unit_Text'] = df_pos['Unit_Text']

        # æ£€éªŒå‘˜å­—æ®µ
        df_pos_out['Inspector_Is_CN_RU_Related'] = ''  # æ˜¯/å¦
        df_pos_out['Inspector_Boundary'] = ''           # åˆé€‚/åå°/åå¤§
        df_pos_out['Inspector_Comments'] = ''           # å¤‡æ³¨

        # ä¿å­˜æ–‡ä»¶
        zh_pos = os.path.join(output_path, 'æ­£å‘æ£€éªŒ_ç²¾ç¡®ç‡ä¸è¾¹ç•Œæ ·æœ¬.xlsx')
        ru_pos = os.path.join(output_path, 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°_Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ_Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°(Precision_Ğ¸_Ğ“Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹).xlsx')
        _save_bilingual(df_pos_out, zh_pos, ru_pos)

        UX.ok(f"æ­£å‘æ£€éªŒæ ·æœ¬å·²ç”Ÿæˆ: {len(df_pos_out)} æ¡")

        # =========== 4. æ¡†æ¶ç»´åº¦æ£€éªŒ ===========
        UX.info("ç”Ÿæˆæ¡†æ¶ç»´åº¦æ£€éªŒæ ·æœ¬...")

        # æ¡†æ¶å­—æ®µæ˜ å°„ï¼ˆæ›´æ–°ä¸ºæ–°çš„å¯¹è±¡æ ¼å¼ï¼‰
        frame_fields = [
            ('ProblemDefinition', 'Frame_ProblemDefinition'),
            ('ResponsibilityAttribution', 'Frame_ResponsibilityAttribution'),
            ('MoralEvaluation', 'Frame_MoralEvaluation'),
            ('SolutionRecommendation', 'Frame_SolutionRecommendation'),
            ('ActionStatement', 'Frame_ActionStatement'),
            ('CausalExplanation', 'Frame_CausalExplanation')
        ]

        # ç»´åº¦å­—æ®µ
        dimension_fields = [
            'Valence', 'Evidence_Type', 'Attribution_Level', 'Temporal_Focus',
            'Primary_Actor_Type', 'Geographic_Scope', 'Relationship_Model_Definition', 'Discourse_Type'
        ]

        # æ„å»ºæ¡†æ¶æ£€éªŒæ•°æ®
        df_frame = pd.DataFrame()
        df_frame['Unit_ID'] = df_pos['Unit_ID']
        df_frame['Source'] = df_pos['Source']
        df_frame['Macro_Chunk_ID'] = df_pos['Macro_Chunk_ID']
        df_frame['Parent_Macro_Chunk_Text_Highlighted'] = highlighted_texts
        df_frame['Unit_Text'] = df_pos['Unit_Text']

        # å¤„ç†æ¡†æ¶å­—æ®µ
        for display_name, field_name in frame_fields:
            if field_name in df_pos.columns:
                # AIè¯†åˆ«ç»“æœ
                ai_values = []
                ai_quotes = []
                inspector_col = f'Inspector_Frame_{display_name}_Present'

                for _, row in df_pos.iterrows():
                    val = row.get(field_name, [])

                    # å¤„ç†æ–°çš„å¯¹è±¡æ ¼å¼ [{"quote": "...", "reason": "...", "reasoning_pattern": "..."}]
                    if isinstance(val, list):
                        has_frame = len(val) > 0
                        if val and isinstance(val[0], dict):
                            # æ–°çš„å¯¹è±¡æ ¼å¼ï¼šæå–æ‰€æœ‰quoteå­—æ®µ
                            quotes = [item.get('quote', '') for item in val if isinstance(item, dict) and item.get('quote')]
                            quote_text = '; '.join(quotes) if quotes else ''
                        else:
                            # å…¼å®¹æ—§æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²åˆ—è¡¨
                            quote_text = '; '.join([str(q) for q in val]) if val else ''
                    elif isinstance(val, str):
                        val_str = str(val).strip()
                        has_frame = len(val_str) > 2 and val_str != '[]'
                        quote_text = val_str if has_frame else ''
                    else:
                        has_frame = False
                        quote_text = ''

                    ai_values.append(1 if has_frame else 0)
                    ai_quotes.append(quote_text)

                # æ·»åŠ åˆ°æ•°æ®æ¡†
                df_frame[f'AI_Frame_{display_name}_Present'] = ai_values
                df_frame[f'AI_Frame_{display_name}_Quotes'] = ai_quotes  # æ·»åŠ å¼•æ–‡å†…å®¹
                df_frame[inspector_col] = ''  # æ£€éªŒå‘˜åˆ¤æ–­

        # å¤„ç†ç»´åº¦å­—æ®µ
        for dim in dimension_fields:
            if dim in df_pos.columns:
                df_frame[f'AI_{dim}'] = df_pos[dim]  # AIçš„åˆ†ç±»ç»“æœ
                df_frame[f'Inspector_{dim}_Correct'] = ''  # æ£€éªŒå‘˜åˆ¤æ–­æ˜¯å¦æ­£ç¡®

        # ä¿å­˜æ¡†æ¶ç»´åº¦æ£€éªŒæ–‡ä»¶
        zh_frame = os.path.join(output_path, 'æ¡†æ¶ç»´åº¦æ£€éªŒ_å•æ£€éªŒå‘˜.xlsx')
        ru_frame = os.path.join(output_path, 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°_Ğ Ğ°Ğ¼ĞºĞ¸_Ğ¸_Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸_Ğ¾Ğ´Ğ½Ğ¸Ğ¼_Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‰Ğ¸Ğ¼.xlsx')
        _save_bilingual(df_frame, zh_frame, ru_frame)

        UX.ok(f"æ¡†æ¶ç»´åº¦æ£€éªŒæ ·æœ¬å·²ç”Ÿæˆ: {len(df_frame)} æ¡")

    # =========== 5. ä¿å­˜æŠ½æ ·è®°å½• ===========
    if new_recall_ids or new_precision_ids:
        sampled_records['recall'].update(new_recall_ids)
        sampled_records['precision'].update(new_precision_ids)

        try:
            with open(sampled_cache, 'wb') as f:
                pickle.dump(sampled_records, f)
            UX.info(f"æ›´æ–°æŠ½æ ·è®°å½•: ç´¯è®¡å¬å›{len(sampled_records['recall'])}æ¡, "
                   f"ç²¾ç¡®{len(sampled_records['precision'])}æ¡")
        except Exception as e:
            UX.warn(f"ä¿å­˜æŠ½æ ·è®°å½•å¤±è´¥: {e}")

    UX.ok("æ‰€æœ‰ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*60)
    print("ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆç»Ÿè®¡ï¼š")
    print("-"*60)

    if negative_samples:
        print(f"åå‘æ£€éªŒï¼ˆå¬å›ç‡ï¼‰: {len(df_neg_out)} æ¡æ ·æœ¬")
        print(f"  - AIåˆ¤å®šæ— å…³: {(df_neg_out['AI_Decision']=='AIåˆ¤å®šæ•´ç¯‡æ–‡ç« æ— å…³').sum()} æ¡")
        print(f"  - æœªè¢«ä½¿ç”¨: {(df_neg_out['AI_Decision']=='æœªè¢«ä½¿ç”¨çš„å®è§‚å—').sum()} æ¡")

    if positive_samples:
        print(f"æ­£å‘æ£€éªŒï¼ˆç²¾ç¡®ç‡ï¼‰: {len(df_pos_out)} æ¡æ ·æœ¬")
        print(f"æ¡†æ¶ç»´åº¦æ£€éªŒ: {len(df_frame)} æ¡æ ·æœ¬")

    print("="*60)

# ============================================================================
# ä¿¡åº¦æ£€éªŒéªŒè¯å’Œè¯Šæ–­å·¥å…·
# ============================================================================

def verify_macro_chunks_quality(macro_chunks):
    """éªŒè¯å®è§‚å—æ•°æ®è´¨é‡"""
    if not macro_chunks:
        return False, "æ²¡æœ‰å®è§‚å—æ•°æ®"

    issues = []

    # æ£€æŸ¥å¿…è¦å­—æ®µ
    sample = macro_chunks[0]
    required = ['Macro_Chunk_ID', 'Macro_Chunk_Text', 'Speaker', 'Source']
    missing = [f for f in required if f not in sample]

    if missing:
        issues.append(f"ç¼ºå°‘å­—æ®µ: {missing}")

    # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼ˆå­—ç¬¦æ•°å’Œtokenæ•°ï¼‰
    char_lengths = [len(str(m.get('Macro_Chunk_Text', ''))) for m in macro_chunks]
    token_lengths = [count_tokens(str(m.get('Macro_Chunk_Text', ''))) for m in macro_chunks]

    avg_char_len = sum(char_lengths) / len(char_lengths) if char_lengths else 0
    avg_token_len = sum(token_lengths) / len(token_lengths) if token_lengths else 0

    min_chars_strict = QUALITY_THRESHOLDS.get('min_macro_chunk_chars_strict', 100)
    if avg_char_len < min_chars_strict:
        issues.append(f"å®è§‚å—å¹³å‡é•¿åº¦è¿‡çŸ­: {avg_char_len:.0f} å­—ç¬¦ (< {min_chars_strict})")

    # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºæ–‡æœ¬
    empty_count = sum(1 for m in macro_chunks if not str(m.get('Macro_Chunk_Text', '')).strip())
    if empty_count > 0:
        issues.append(f"æœ‰ {empty_count} ä¸ªç©ºå®è§‚å—")

    if issues:
        return False, "; ".join(issues)

    return True, f"é€šè¿‡éªŒè¯: {len(macro_chunks)} ä¸ªå®è§‚å—ï¼Œå¹³å‡é•¿åº¦ {avg_char_len:.0f} å­—ç¬¦, {avg_token_len:.0f} tokens"

if __name__ == "__main__":
    asyncio.run(main_async())