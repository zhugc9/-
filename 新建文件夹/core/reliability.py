"""æ¨¡å—åŒ–çš„ä¿¡åº¦æ£€éªŒç”Ÿæˆå™¨"""

from __future__ import annotations

import os
import json
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Tuple

import pandas as pd

try:
    from rapidfuzz import fuzz, utils
except ImportError:  # pragma: no cover
    from thefuzz import fuzz, utils  # type: ignore

from . import UX
from .utils import ProcessingStatus

# ---------------------------------------------------------------------------
# é€šç”¨å·¥å…·
# ---------------------------------------------------------------------------

_UNIT_TEXT_CANDIDATES = [
    "Unit_Text",
    "unit_text",
    "UnitText",
    "comment_text",
    "è¯„è®ºå†…å®¹",
    "å›ç­”å†…å®¹",
    "Answer_Text",
    "text",
    "æ­£æ–‡",
]

_ID_CANDIDATES = [
    "åºå·",       # åª’ä½“æ–‡æœ¬å’ŒçŸ¥ä¹çš„æ–‡ç« /å›ç­”ID
    "Unit_ID",    # å•å…ƒå”¯ä¸€æ ‡è¯†
    "comment_id", # VKè¯„è®ºID
]


def _analysis_columns() -> List[str]:
    return [
        "Incident",
        "Frame_SolutionRecommendation",
        "Frame_ResponsibilityAttribution",
        "Frame_CausalExplanation",
        "Frame_MoralEvaluation",
        "Frame_ProblemDefinition",
        "Frame_ActionStatement",
        "Valence",
        "Evidence_Type",
        "Attribution_Level",
        "Temporal_Focus",
        "Primary_Actor_Type",
        "Geographic_Scope",
        "Relationship_Model_Definition",
        "Discourse_Type",
    ]


def _locate_unit(unit_text: str, full_text: str) -> Optional[Tuple[int, int]]:
    if not unit_text or not full_text:
        return None

    try:
        start = full_text.index(unit_text)
        return start, start + len(unit_text)
    except ValueError:
        pass

    processed_unit = utils.default_process(unit_text)
    processed_full = utils.default_process(full_text)
    alignment = fuzz.partial_ratio_alignment(processed_unit, processed_full, score_cutoff=85)
    if alignment:
        return alignment.dest_start, alignment.dest_end
    return None


def _get_text(row: pd.Series, preferred: str) -> str:
    if preferred in row and pd.notna(row[preferred]):
        text = str(row[preferred]).strip()
        if text and text.lower() != "nan":
            return text
    for col in _UNIT_TEXT_CANDIDATES:
        if col in row and pd.notna(row[col]):
            text = str(row[col]).strip()
            if text and text.lower() != "nan":
                return text
    return ""


def _get_id(row: pd.Series, preferred: str) -> str:
    if preferred in row and pd.notna(row[preferred]):
        val = str(row[preferred]).strip()
        if val:
            return val
    for col in _ID_CANDIDATES:
        if col in row and pd.notna(row[col]):
            val = str(row[col]).strip()
            if val:
                return val
    return ""


# ---------------------------------------------------------------------------
# æŠ½è±¡ç­–ç•¥
# ---------------------------------------------------------------------------


class BaseSampler(ABC):
    """æ‰€æœ‰æŠ½æ ·å™¨çš„ç»Ÿä¸€æ¥å£"""

    def __init__(self, sampling_config: Dict[str, Dict], random_seed: int = 42):
        self.sampling_config = sampling_config or {}
        self.random_seed = random_seed

    @abstractmethod
    def sample(self, df_results: pd.DataFrame, df_input: pd.DataFrame) -> List[dict]:
        raise NotImplementedError


# ---------------------------------------------------------------------------
# å…·ä½“æŠ½æ ·ç­–ç•¥
# ---------------------------------------------------------------------------


class PrecisionSampler(BaseSampler):
    """æ­£å‘ï¼ˆå‡†ç¡®ç‡ï¼‰æŠ½æ ·"""

    def sample(self, df_results: pd.DataFrame, df_input: pd.DataFrame) -> List[dict]:
        samples: List[dict] = []
        
        # åŒºåˆ†åª’ä½“æ–‡æœ¬å’Œç¤¾äº¤åª’ä½“
        SOCIAL_MEDIA_SOURCES = {'vk', 'çŸ¥ä¹', 'zhihu'}

        # ä¼˜åŒ–ï¼šåªå¤„ç†å®é™…å­˜åœ¨çš„ä¿¡æºï¼Œé¿å…æ— ç”¨æ—¥å¿—
        if "Source" not in df_results.columns:
            UX.warn("ç»“æœæ•°æ®ä¸­ç¼ºå°‘Sourceåˆ—ï¼Œæ— æ³•è¿›è¡Œæ­£å‘æŠ½æ ·")
            return samples
        if "processing_status" not in df_results.columns:
            UX.warn("ç»“æœæ•°æ®ä¸­ç¼ºå°‘processing_statusåˆ—ï¼Œæ— æ³•è¿›è¡Œæ­£å‘æŠ½æ ·")
            return samples
        
        actual_sources = set(df_results["Source"].dropna().unique())
        
        for source, config in self.sampling_config.items():
            precision = config.get("precision", 0)
            if precision <= 0:
                continue
            
            # è·³è¿‡ä¸å­˜åœ¨çš„ä¿¡æºï¼ˆæ— æ—¥å¿—ï¼‰
            if source not in actual_sources:
                continue
                
            source_results = df_results[
                (df_results["Source"] == source)
                & (df_results["processing_status"] == ProcessingStatus.SUCCESS)
            ]
            if source_results.empty:
                UX.info(f"{source} æ²¡æœ‰æˆåŠŸå¤„ç†çš„è®°å½•ï¼Œè·³è¿‡æ­£å‘æŠ½æ ·")
                continue

            take = min(len(source_results), precision)
            sampled = source_results.sample(n=take, random_state=self.random_seed)

            for _, row in sampled.iterrows():
                unit_text = _get_text(row, "Unit_Text")
                
                # ç¤¾äº¤åª’ä½“ï¼šä¸éœ€è¦é«˜äº®ï¼Œç›´æ¥ä½¿ç”¨Unit_Text
                if source in SOCIAL_MEDIA_SOURCES:
                    # VKéœ€è¦é™„åŠ post_textä¸Šä¸‹æ–‡
                    display_text = unit_text
                    if source == 'vk':
                        post_text = row.get("post_text", "") or row.get("Post_Text", "")
                        if post_text:
                            display_text = f"ã€å¸–å­åŸæ–‡ã€‘\n{post_text}\n\nã€è¯„è®ºå†…å®¹ã€‘\n{unit_text}"
                    
                    record = {col: row.get(col, "") for col in _analysis_columns()}
                    record.update(
                        {
                            "Unit_ID": row.get("Unit_ID", ""),
                            "Source": source,
                            "Unit_Text": display_text,
                            "Inspector_Is_Relevant": "",
                            "Inspector_Comments": "",
                        }
                    )
                    for col in _analysis_columns():
                        record[f"Inspector_{col}"] = ""
                    samples.append(record)
                    continue
                
                # åª’ä½“æ–‡æœ¬ï¼šéœ€è¦é«˜äº®
                # ä½¿ç”¨åºå·åˆ—ä½œä¸ºæ–‡ç« IDï¼ˆä¼šè‡ªåŠ¨fallbackåˆ°_ID_CANDIDATESï¼‰
                article_id = _get_id(row, "åºå·")
                if not article_id:
                    continue

                # ä¿®å¤ï¼šä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒï¼Œç¡®ä¿IDç±»å‹ä¸€è‡´
                article_id_str = str(article_id)
                matching = df_input[
                    df_input.apply(lambda r: str(_get_id(r, "åºå·")), axis=1)
                    == article_id_str
                ]
                if matching.empty:
                    UX.warn(f"æ­£å‘æŠ½æ ·ï¼šå•å…ƒ {row.get('Unit_ID', 'unknown')} çš„åŸæ–‡ç« æœªæ‰¾åˆ°")
                    continue

                full_text = _get_text(matching.iloc[0], "text")
                location = _locate_unit(unit_text, full_text)
                if location:
                    start, end = location
                    highlighted = (
                        full_text[:start]
                        + "\nã€ğŸŒŸ===é«˜äº®æ®µè½å¼€å§‹===ğŸŒŸã€‘\n"
                        + full_text[start:end]
                        + "\nã€ğŸŒŸ===é«˜äº®æ®µè½ç»“æŸ===ğŸŒŸã€‘\n"
                        + full_text[end:]
                    )
                else:
                    highlighted = f"ã€å®šä½å¤±è´¥ã€‘{unit_text}"

                record = {col: row.get(col, "") for col in _analysis_columns()}
                record.update(
                    {
                        "Unit_ID": row.get("Unit_ID", ""),
                        "Article_ID": article_id,
                        "Source": source,
                        "Unit_Text": unit_text,
                        "Highlighted_Full_Text": highlighted,
                        "Inspector_Is_Relevant": "",
                        "Inspector_Boundary_Quality": "",
                        "Inspector_Comments": "",
                    }
                )
                for col in _analysis_columns():
                    record[f"Inspector_{col}"] = ""

                samples.append(record)

        return samples


class RecallByExclusionSampler(BaseSampler):
    """åª’ä½“æ–‡æœ¬ï¼šæŒ–é™¤ç‰ˆå¬å›ç‡"""

    def sample(self, df_results: pd.DataFrame, df_input: pd.DataFrame) -> List[dict]:
        samples: List[dict] = []
        
        # åªå¤„ç†åª’ä½“æ–‡æœ¬ç±»å‹çš„sourceï¼Œä¸å¤„ç†ç¤¾äº¤åª’ä½“
        SOCIAL_MEDIA_SOURCES = {'vk', 'çŸ¥ä¹', 'zhihu'}

        # ä¼˜åŒ–ï¼šåªå¤„ç†å®é™…å­˜åœ¨çš„ä¿¡æºï¼Œé¿å…æ— ç”¨æ—¥å¿—
        if "Source" not in df_input.columns:
            UX.warn("è¾“å…¥æ•°æ®ä¸­ç¼ºå°‘Sourceåˆ—ï¼Œæ— æ³•è¿›è¡Œåå‘æŠ½æ ·")
            return samples
        
        actual_sources = set(df_input["Source"].dropna().unique())

        for source, config in self.sampling_config.items():
            recall = config.get("recall", 0)
            if recall <= 0:
                continue
            
            # è·³è¿‡ç¤¾äº¤åª’ä½“sourceï¼ˆå®ƒä»¬ç”±RecallByRejectionSamplerå¤„ç†ï¼‰
            if source in SOCIAL_MEDIA_SOURCES:
                continue
            
            # è·³è¿‡ä¸å­˜åœ¨çš„ä¿¡æºï¼ˆæ— æ—¥å¿—ï¼‰
            if source not in actual_sources:
                continue
                
            source_inputs = df_input[df_input["Source"] == source]
            if source_inputs.empty:
                UX.info(f"{source} åœ¨è¾“å…¥æ•°æ®ä¸­æ²¡æœ‰è®°å½•ï¼Œè·³è¿‡åå‘æŠ½æ ·")
                continue

            take = min(len(source_inputs), recall)
            sampled = source_inputs.sample(n=take, random_state=self.random_seed)

            for _, article in sampled.iterrows():
                # ä½¿ç”¨åºå·åˆ—ä½œä¸ºæ–‡ç« ID
                article_id = _get_id(article, "åºå·")
                full_text = _get_text(article, "text")
                if not full_text:
                    continue

                # ä¿®å¤ï¼šä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒï¼Œç¡®ä¿IDç±»å‹ä¸€è‡´
                article_id_str = str(article_id)
                units = df_results[
                    df_results.apply(lambda r: str(_get_id(r, "åºå·")), axis=1) == article_id_str
                ]
                
                if units.empty:
                    UX.info(f"æ–‡ç«  {article_id} æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„æå–å•å…ƒï¼Œå¯èƒ½å…¨éƒ¨è¢«åˆ¤å®šä¸ºä¸ç›¸å…³")
                
                unit_texts = [_get_text(r, "Unit_Text") for _, r in units.iterrows() if _get_text(r, "Unit_Text")]

                positions: List[Tuple[int, int]] = []
                failed_units: List[str] = []  # è®°å½•å®šä½å¤±è´¥çš„å•å…ƒ
                
                for unit_text in unit_texts:
                    location = _locate_unit(unit_text, full_text)
                    if location:
                        positions.append(location)
                    else:
                        # è®°å½•å®šä½å¤±è´¥çš„å•å…ƒï¼ˆæˆªå–å‰80å­—ç¬¦ä½œä¸ºæ ‡è¯†ï¼‰
                        preview = unit_text[:80] + "..." if len(unit_text) > 80 else unit_text
                        failed_units.append(preview)

                positions.sort(key=lambda x: x[0], reverse=True)
                modified = full_text
                for start, end in positions:
                    modified = modified[:start] + "ã€å·²æå–ã€‘" + modified[end:]

                # å¦‚æœæœ‰å®šä½å¤±è´¥çš„å•å…ƒï¼Œåœ¨æœ«å°¾æ·»åŠ è¯´æ˜
                if failed_units:
                    modified += "\n\n" + "="*50 + "\n"
                    modified += "âš ï¸ ä»¥ä¸‹å•å…ƒå·²è¢«AIæå–ï¼Œä½†åœ¨åŸæ–‡ä¸­å®šä½å¤±è´¥ï¼ˆå¯èƒ½å› æ–‡æœ¬ç»†å¾®å·®å¼‚ï¼‰ï¼š\n"
                    modified += "ï¼ˆè¿™äº›å†…å®¹å·²è¢«æå–ï¼Œéé—æ¼ã€‚è¯·æ£€æŸ¥AIçš„Unit_Textåˆ—ã€‚ï¼‰\n"
                    modified += "="*50 + "\n"
                    for i, unit in enumerate(failed_units, 1):
                        modified += f"\n{i}. {unit}\n"

                if modified.replace("ã€å·²æå–ã€‘", "").strip():
                    samples.append(
                        {
                            "Article_ID": article_id,
                            "Source": source,
                            "Extracted_Units_Count": len(positions),
                            "Failed_Locate_Count": len(failed_units),  # æ–°å¢ï¼šè®°å½•å®šä½å¤±è´¥æ•°é‡
                            "Remaining_Text": modified,
                            "Inspector_Has_Missed_Content": "",
                            "Inspector_Missed_Content_Type": "",
                            "Inspector_Comments": "",
                        }
                    )

        return samples


class RecallByRejectionSampler(BaseSampler):
    """ç¤¾äº¤åª’ä½“ï¼šæŠ½æ ·è¢«åˆ¤ä¸ç›¸å…³çš„å•å…ƒï¼ˆä»…å¤„ç†æœ‰ç›¸å…³æ€§åˆ¤æ–­çš„ç¤¾äº¤åª’ä½“ï¼‰"""

    def sample(self, df_results: pd.DataFrame, df_input: pd.DataFrame) -> List[dict]:
        samples: List[dict] = []
        
        # åªå¤„ç†æœ‰ç›¸å…³æ€§åˆ¤æ–­çš„ç¤¾äº¤åª’ä½“ï¼ˆVKï¼‰
        # çŸ¥ä¹ï¼šä½¿ç”¨ZHIHU_CHUNKINGï¼Œæ²¡æœ‰ç›¸å…³æ€§åˆ¤æ–­ï¼Œä¸ä¼šæœ‰NO_RELEVANT
        # åª’ä½“æ–‡æœ¬ï¼šNO_RELEVANTæå°‘ï¼Œç”±RecallByExclusionSamplerç»Ÿä¸€å¤„ç†ï¼Œæˆ–äººå·¥ç­›é€‰
        SOCIAL_MEDIA_WITH_RELEVANCE_CHECK = {'vk'}

        # ä¼˜åŒ–ï¼šåªå¤„ç†å®é™…å­˜åœ¨çš„ä¿¡æºï¼Œé¿å…æ— ç”¨æ—¥å¿—
        if "Source" not in df_results.columns:
            UX.warn("ç»“æœæ•°æ®ä¸­ç¼ºå°‘Sourceåˆ—ï¼Œæ— æ³•è¿›è¡Œå¬å›ç‡æŠ½æ ·")
            return samples
        if "processing_status" not in df_results.columns:
            UX.warn("ç»“æœæ•°æ®ä¸­ç¼ºå°‘processing_statusåˆ—ï¼Œæ— æ³•è¿›è¡Œå¬å›ç‡æŠ½æ ·")
            return samples
        
        actual_sources = set(df_results["Source"].dropna().unique())

        for source, config in self.sampling_config.items():
            recall = config.get("recall", 0)
            if recall <= 0:
                continue
            
            # è·³è¿‡éç¤¾äº¤åª’ä½“ä¿¡æº
            if source not in SOCIAL_MEDIA_WITH_RELEVANCE_CHECK:
                continue
            
            # è·³è¿‡ä¸å­˜åœ¨çš„ä¿¡æºï¼ˆæ— æ—¥å¿—ï¼‰
            if source not in actual_sources:
                continue
                
            rejected = df_results[
                (df_results["Source"] == source)
                & (df_results["processing_status"] == ProcessingStatus.NO_RELEVANT)
            ]
            if rejected.empty:
                UX.info(f"{source} æ²¡æœ‰è¢«æ‹’ç»çš„è®°å½•ï¼Œè·³è¿‡å¬å›ç‡æŠ½æ ·")
                continue

            take = min(len(rejected), recall)
            sampled = rejected.sample(n=take, random_state=self.random_seed)

            for _, row in sampled.iterrows():
                unit_text = _get_text(row, "Unit_Text")
                
                # VKéœ€è¦é™„åŠ post_textä¸Šä¸‹æ–‡
                if source == 'vk':
                    post_text = row.get("post_text", "") or row.get("Post_Text", "")
                    if post_text:
                        unit_text = f"ã€å¸–å­åŸæ–‡ã€‘\n{post_text}\n\nã€è¯„è®ºå†…å®¹ã€‘\n{unit_text}"
                
                samples.append(
                    {
                        "Unit_ID": row.get("Unit_ID", ""),
                        "Source": source,
                        "Unit_Text": unit_text,
                        "Inspector_Has_Missed_Content": "",
                        "Inspector_Missed_Content_Type": "",
                        "Inspector_Comments": "",
                    }
                )

        return samples


# ---------------------------------------------------------------------------
# åè°ƒå™¨
# ---------------------------------------------------------------------------


class ReliabilityTestModule:
    def __init__(self, output_path: str, sampling_config: dict, random_seed: int = 42):
        self.output_path = output_path
        self.sampling_config = sampling_config or {}
        self.random_seed = random_seed
        self._locale_cache: Dict[str, Dict[str, str]] = {}

        self.precision_sampler = PrecisionSampler(self.sampling_config, random_seed)
        self.recall_exclusion_sampler = RecallByExclusionSampler(self.sampling_config, random_seed)
        self.recall_rejection_sampler = RecallByRejectionSampler(self.sampling_config, random_seed)

    def generate_files(self, df_results: pd.DataFrame, df_input: pd.DataFrame) -> None:
        UX.info("å¼€å§‹ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶")

        precision_samples = self.precision_sampler.sample(df_results, df_input)
        recall_exclusion_samples = self.recall_exclusion_sampler.sample(df_results, df_input)
        recall_rejection_samples = self.recall_rejection_sampler.sample(df_results, df_input)

        if precision_samples:
            self._save_positive(precision_samples)
        else:
            UX.warn("æ­£å‘æ£€éªŒæœªæŠ½åˆ°æ ·æœ¬")

        negative_samples = recall_exclusion_samples + recall_rejection_samples
        if negative_samples:
            self._save_negative(negative_samples)
        else:
            UX.warn("åå‘æ£€éªŒæœªæŠ½åˆ°æ ·æœ¬")

        UX.ok("ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆå®Œæˆ")

    # ------------------------------------------------------------------
    # ä¿å­˜é€»è¾‘
    # ------------------------------------------------------------------

    def _save_positive(self, samples: List[dict]) -> None:
        df_output = pd.DataFrame(samples)
        order = ["Unit_ID", "Article_ID", "Source", "Unit_Text", "Highlighted_Full_Text"]
        for col in _analysis_columns():
            if col in df_output.columns:
                order.extend([col, f"Inspector_{col}"])
        order.extend(["Inspector_Is_Relevant", "Inspector_Boundary_Quality", "Inspector_Comments"])
        order = [col for col in order if col in df_output.columns]
        df_output = df_output.reindex(columns=order)

        zh_path = os.path.join(self.output_path, "æ­£å‘æ£€éªŒ_é«˜äº®ç‰ˆ.xlsx")
        ru_path = os.path.join(self.output_path, "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°_Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ.xlsx")
        self._save_bilingual(df_output, zh_path, ru_path)
        UX.ok(f"æ­£å‘æ£€éªŒæ–‡ä»¶å·²ä¿å­˜ï¼š{len(df_output)} æ¡æ ·æœ¬")

    def _save_negative(self, samples: List[dict]) -> None:
        df_output = pd.DataFrame(samples)
        
        # å®šä¹‰åå‘æ£€éªŒçš„åˆ—é¡ºåºï¼ˆåŒ…å«åª’ä½“æ–‡æœ¬å’Œç¤¾äº¤åª’ä½“ä¸¤ç§æ•°æ®ç»“æ„ï¼‰
        order = [
            # IDåˆ—ï¼ˆæœ‰çš„æ•°æ®æœ‰Unit_IDï¼Œæœ‰çš„æœ‰Article_IDï¼‰
            "Unit_ID", 
            "Article_ID", 
            "Source",
            # å†…å®¹åˆ—
            "Unit_Text",           # VKè¢«æ‹’ç»è¯„è®ºä½¿ç”¨
            "Remaining_Text",      # åª’ä½“æ–‡æœ¬æŒ–é™¤æ³•ä½¿ç”¨
            # ç»Ÿè®¡åˆ—ï¼ˆä»…åª’ä½“æ–‡æœ¬æœ‰ï¼‰
            "Extracted_Units_Count",
            "Failed_Locate_Count",
            # æ£€æŸ¥å‘˜åˆ—
            "Inspector_Has_Missed_Content",
            "Inspector_Missed_Content_Type",
            "Inspector_Comments"
        ]
        # åªä¿ç•™å®é™…å­˜åœ¨çš„åˆ—ï¼Œé¿å…æŠ¥é”™
        order = [col for col in order if col in df_output.columns]
        df_output = df_output.reindex(columns=order)
        
        zh_path = os.path.join(self.output_path, "åå‘æ£€éªŒ.xlsx")
        ru_path = os.path.join(self.output_path, "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°_Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ.xlsx")
        self._save_bilingual(df_output, zh_path, ru_path)
        UX.ok(f"åå‘æ£€éªŒæ–‡ä»¶å·²ä¿å­˜ï¼š{len(df_output)} æ¡æ ·æœ¬")

    # ------------------------------------------------------------------
    # åŒè¯­å¯¼å‡º
    # ------------------------------------------------------------------

    def _save_bilingual(self, df: pd.DataFrame, zh_path: str, ru_path: str) -> None:
        self._ensure_dirs([zh_path, ru_path])
        try:
            df_zh = self._decorate_headers(df, "zh")
            df_zh.to_excel(zh_path, index=False)
        except Exception as e:  # pragma: no cover
            UX.warn(f"ä¸­æ–‡å¯¼å‡ºå¤±è´¥: {e}")
        try:
            df_ru = self._decorate_headers(df, "ru")
            df_ru.to_excel(ru_path, index=False)
        except Exception as e:  # pragma: no cover
            UX.warn(f"ä¿„æ–‡å¯¼å‡ºå¤±è´¥: {e}")

    def _decorate_headers(self, df: pd.DataFrame, lang: str) -> pd.DataFrame:
        mapping = self._load_locale(lang)
        new_cols = [f"{mapping.get(col, col)}({col})" if col in mapping else col for col in df.columns]
        df_out = df.copy()
        df_out.columns = new_cols
        return df_out

    def _load_locale(self, lang: str) -> Dict[str, str]:
        if lang in self._locale_cache:
            return self._locale_cache[lang]
        locales_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "locales")
        path = os.path.join(locales_dir, f"{lang}.json")
        try:
            with open(path, "r", encoding="utf-8") as f:
                mapping = json.load(f)
                self._locale_cache[lang] = mapping
                return mapping
        except Exception:  # pragma: no cover
            UX.warn(f"æœ¬åœ°åŒ–æ–‡ä»¶ç¼ºå¤±: {path}")
            return {}

    @staticmethod
    def _ensure_dirs(paths: List[str]) -> None:
        for path in paths:
            directory = os.path.dirname(path)
            if directory:
                os.makedirs(directory, exist_ok=True)


def create_reliability_test_module(output_path: str, sampling_config: dict, random_seed: int = 42) -> ReliabilityTestModule:
    """åˆ›å»ºä¿¡åº¦æ£€éªŒæ¨¡å—å®ä¾‹"""
    return ReliabilityTestModule(output_path, sampling_config, random_seed)
