# -*- coding: utf-8 -*-
"""
åª’ä½“æ–‡æœ¬åˆ†æä¸»å…¥å£ - ç§å­æ‰©å±•ä¸¤é˜¶æ®µåˆ†æ
"""
import os
import glob
import asyncio
import aiohttp
import pandas as pd
from asyncio import Lock
from tqdm.asyncio import tqdm as aio_tqdm

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core import (
    UX,
    load_config,
    APIService,
    ReliabilityTestModule,
    safe_str_convert,
    identify_source,
    create_auto_retry_manager,
)
from core.media_text_processor import MediaTextProcessor
from core.utils import ProcessingStatus, get_processing_state

# åŠ è½½é…ç½®
CONFIG = load_config()
if CONFIG is None:
    raise RuntimeError("æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")

# æ–°çš„ç»“æ„åŒ–é…ç½®è¯»å–
# APIé…ç½®
api_config = CONFIG['api']
api_strategy = api_config['strategy']
MAX_CONCURRENT_REQUESTS = api_strategy['max_concurrent_requests']
QUEUE_TIMEOUT = api_strategy['queue_timeout_sec']

# åª’ä½“æ–‡æœ¬æµç¨‹é…ç½®
media_config = CONFIG.get('media_text', {})

# è¾“å…¥è¾“å‡ºè·¯å¾„é…ç½®
INPUT_PATH = media_config['paths']['input']
OUTPUT_PATH = media_config['paths']['output']

# é€šç”¨è‡ªåŠ¨é‡è¯•é…ç½®
AUTO_RETRY_CONFIG = CONFIG['processing']['auto_retry']
AUTO_RETRY_ENABLED = AUTO_RETRY_CONFIG['enabled']
MAX_RETRY_ROUNDS = AUTO_RETRY_CONFIG['max_rounds']
RETRY_DELAY_MINUTES = AUTO_RETRY_CONFIG['delay_minutes']
MIN_FAILED_THRESHOLD = AUTO_RETRY_CONFIG['min_failed_threshold']

# ä¿¡åº¦æ£€éªŒé…ç½®
RELIABILITY_TEST_CONFIG = CONFIG['reliability_test']
RELIABILITY_TEST_MODE = RELIABILITY_TEST_CONFIG['enabled']
RELIABILITY_SAMPLING_CONFIG = RELIABILITY_TEST_CONFIG['sampling_config']

# åˆ—æ˜ å°„å’Œè¾“å‡ºåˆ—é…ç½®
COLUMN_MAPPING = CONFIG.get('media_text', {}).get('column_mapping', {})
REQUIRED_OUTPUT_COLUMNS = CONFIG.get('media_text', {}).get('required_output_columns', CONFIG.get('required_output_columns', []))

# æ¨¡å‹æ± é…ç½®ï¼ˆå…¨å±€ç»Ÿä¸€ï¼‰
MODEL_POOLS = CONFIG.get('model_pools', {})
API_CONFIG = api_config

# å…¨å±€é”
file_write_lock = Lock()

# æç¤ºè¯ç±»
class Prompts:
    def __init__(self):
        self.prompts_dir = os.path.join(os.path.dirname(__file__), 'prompts')
        self._load_prompts()

    def _load_prompts(self):
        prompt_files = {
            'UNIT_EXTRACTION': 'SEED.txt',
            'UNIT_ANALYSIS': 'ANALYSIS.txt'
        }
        for attr_name, filename in prompt_files.items():
            file_path = os.path.join(self.prompts_dir, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    setattr(self, attr_name, f.read())
            except Exception as e:
                UX.err(f"åŠ è½½æç¤ºè¯æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                setattr(self, attr_name, "")

# åˆ›å»ºæç¤ºè¯å®ä¾‹
prompts = Prompts()

async def api_worker(name, task_queue, results_queue, api_service, source, pbar, processor, output_file_path=None, failed_unit_ids=None, is_unit_mode=False):
   while True:
       try:
           item = await task_queue.get()
           if item is None:
               break
           
           # æ¯10ä¸ªä»»åŠ¡æ£€æŸ¥ä¸€æ¬¡æ´»åŠ¨çŠ¶æ€
           if pbar.n % 10 == 0:
               UX.check_activity()
           
           try:
               item_with_source = (item[0], item[1], source)
               current_id = safe_str_convert(item[1].get(COLUMN_MAPPING["ID"], "unknown")) if len(item) > 1 else "unknown"
               original_id, result_Units = await processor.process_row(
                   item_with_source,
                   output_file_path=output_file_path,
                   failed_unit_ids=failed_unit_ids
               )
               await results_queue.put((original_id, result_Units, None))  # ä¸¤é˜¶æ®µæ¨¡å‹ï¼šåªè¿”å›è®®é¢˜å•å…ƒ
           except Exception as e:
               error_brief = str(e)[:30] + "..." if len(str(e)) > 30 else str(e)
               UX.api_failed(f"Worker-{name}", error_brief)
               original_id = safe_str_convert(item[1].get(COLUMN_MAPPING["ID"], "unknown")) if len(item) > 1 else "unknown"
               from core.utils import create_unified_record
               await results_queue.put((original_id, [create_unified_record(ProcessingStatus.STAGE_2_FAILED, original_id, source, "", f"Workeré”™è¯¯: {str(e)[:100]}")], None))
           finally:
               # ä¿è¯æ— è®ºæˆåŠŸå¤±è´¥ï¼Œä»»åŠ¡å®Œæˆåéƒ½æ›´æ–°è¿›åº¦æ¡
               # å•å…ƒç»­ä¼ æ¨¡å¼ï¼šæŒ‰å½“å‰æ–‡ç« çš„å¤±è´¥å•å…ƒæ•°æ›´æ–°ï¼›æ™®é€šæ¨¡å¼ï¼šæŒ‰æ–‡ç« æ•°æ›´æ–°
               if is_unit_mode and failed_unit_ids and current_id in failed_unit_ids:
                   pbar.update(len(failed_unit_ids[current_id]))
               else:
                   pbar.update(1)
               task_queue.task_done()
       
       except asyncio.CancelledError:
           break
       except Exception as e:
           error_brief = str(e)[:30] + "..." if len(str(e)) > 30 else str(e)
           UX.api_failed(f"Worker-{name}-Critical", error_brief)

async def saver_worker(results_queue, df_input, output_file_path, total_tasks=None):
    """
    [ä¸¤é˜¶æ®µæ¨¡å‹] æ‰¹é‡ä¿å­˜workerï¼š
    - ç¼“å†²å¹¶åˆ†æ‰¹ä¿å­˜è®®é¢˜å•å…ƒåˆ†æç»“æœ
    """
    analysis_buffer = []
    received_count = 0
    
    # ä»é…ç½®è¯»å–ç¼“å†²åŒºå¤§å°é˜ˆå€¼
    general_config = CONFIG.get('processing', {}).get('general', {})
    ANALYSIS_BUFFER_LIMIT = general_config.get('buffer_limit', 20)    # åˆ†æç»“æœç¼“å†²åŒº

    async def save_analysis_batch(data):
        if not data:
            return
        async with file_write_lock:
            # è°ƒç”¨ç°æœ‰çš„åˆ†æç»“æœä¿å­˜å‡½æ•°
            save_data_to_excel(data, df_input, output_file_path)

    while True:
        try:
            # ç­‰å¾…ç»“æœï¼Œä»é…ç½®è¯»å–è¶…æ—¶æ—¶é—´
            item = await asyncio.wait_for(results_queue.get(), timeout=QUEUE_TIMEOUT)
            if item is None: # æ”¶åˆ°ç»“æŸä¿¡å·
                await save_analysis_batch(analysis_buffer)
                break

            original_id, result_Units, _ = item  # ç¬¬ä¸‰ä¸ªå‚æ•°ä¸ºå…¼å®¹æ€§ä¿ç•™ï¼Œä¸¤é˜¶æ®µæ¨¡å‹ä¸­æœªä½¿ç”¨
            received_count += 1

            # æ”¶é›†åˆ†æç»“æœåˆ°ç¼“å†²åŒº
            if result_Units:
                for unit in result_Units:
                    unit[COLUMN_MAPPING["ID"]] = original_id
                analysis_buffer.extend(result_Units)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç©ºå¹¶ä¿å­˜ç¼“å†²åŒº
            if len(analysis_buffer) >= ANALYSIS_BUFFER_LIMIT:
                await save_analysis_batch(analysis_buffer)
                analysis_buffer = []

            results_queue.task_done()
            if total_tasks and received_count >= total_tasks:
                await save_analysis_batch(analysis_buffer)
                break

        except asyncio.TimeoutError:
            # è¶…æ—¶åä¿å­˜æ‰€æœ‰ç¼“å†²åŒºå†…å®¹ï¼Œé˜²æ­¢åœ¨ä»»åŠ¡é—´éš™ä¸¢å¤±æ•°æ®
            await save_analysis_batch(analysis_buffer)
            analysis_buffer = []
            if total_tasks and received_count >= total_tasks:
                break
            continue

        except asyncio.CancelledError:
            await save_analysis_batch(analysis_buffer)
            break

        except Exception as e:
            UX.err(f"ä¿å­˜çº¿ç¨‹é”™è¯¯: {e}")

def save_data_to_excel(new_Units_list, df_input, output_file_path):
   try:
       df_existing = pd.DataFrame()
       if os.path.exists(output_file_path):
           try:
               df_existing = pd.read_excel(output_file_path)
           except Exception as e:
               UX.warn(f"è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}")
       df_new_Units = pd.DataFrame(new_Units_list)

       if df_new_Units.empty:
           return

       df_existing = ensure_required_columns(df_existing)
       df_new_Units = ensure_required_columns(df_new_Units)
       
       if COLUMN_MAPPING["ID"] in df_input.columns and COLUMN_MAPPING["ID"] in df_new_Units.columns:
           df_input[COLUMN_MAPPING["ID"]] = df_input[COLUMN_MAPPING["ID"]].astype(str)
           df_new_Units[COLUMN_MAPPING["ID"]] = df_new_Units[COLUMN_MAPPING["ID"]].astype(str)
           new_original_ids = df_new_Units[COLUMN_MAPPING["ID"]].unique()
           
           # ğŸ”§ ä¿®å¤æ ¸å¿ƒé—®é¢˜ï¼šæ¸…ç†æ—§çš„"æ¦‚è¦çº§"å¤±è´¥è®°å½•
           # å½“ä¸€ç¯‡æ–‡ç« è¢«é‡æ–°å¤„ç†æˆåŠŸæ—¶ï¼Œåˆ é™¤å®ƒçš„æ—§å¤±è´¥è®°å½•
           if not df_existing.empty and 'Unit_ID' in df_existing.columns:
               # è¯†åˆ«"æ¦‚è¦çº§"è®°å½•ï¼šUnit_IDä¸åŒ…å«"-Unit-"çš„è®°å½•
               # ä¾‹å¦‚ï¼š"(3)-RU1012-API_FAILED" æˆ– "(3)-RU1012-NO_RELEVANT"
               summary_mask = (
                   df_existing[COLUMN_MAPPING["ID"]].isin(new_original_ids) & 
                   ~df_existing['Unit_ID'].str.contains('-Unit-', na=False)
               )
               
               removed_count = summary_mask.sum()
               if removed_count > 0:
                   df_existing = df_existing[~summary_mask].copy()

           input_base_cols = [col for col in df_input.columns if col in df_new_Units.columns and col != COLUMN_MAPPING["ID"]]
           if input_base_cols:
               df_new_Units = df_new_Units.drop(columns=input_base_cols)
           
           df_input_subset = df_input[df_input[COLUMN_MAPPING["ID"]].isin(new_original_ids)].copy()
           df_newly_merged = pd.merge(df_input_subset, df_new_Units, on=COLUMN_MAPPING["ID"], how='left')
       else:
           df_newly_merged = df_new_Units.copy()

       df_final_to_save = pd.concat([df_existing, df_newly_merged], ignore_index=True)

       if 'Unit_ID' in df_final_to_save.columns:
           df_final_to_save = df_final_to_save.drop_duplicates(subset=['Unit_ID'], keep='last')

       df_final_to_save = reorder_columns(df_final_to_save, list(df_input.columns))
       df_final_to_save.to_excel(output_file_path, index=False)
       UX.ok(f"å·²ä¿å­˜: {os.path.basename(output_file_path)} (ç´¯è®¡ {len(df_final_to_save)} æ¡è®°å½•)")

   except Exception as e:
       UX.err(f"Excelä¿å­˜å¤±è´¥: {e}")

async def main_async():
   UX.start_run()
   UX.phase("é•¿æ–‡æœ¬åˆ†æå™¨å¯åŠ¨")
   UX.info(f"ä¿¡åº¦æ£€éªŒæ¨¡å¼: {'å¼€å¯' if RELIABILITY_TEST_MODE else 'å…³é—­'}")
   
   # å¯åŠ¨æ´»åŠ¨ç›‘æ§ä»»åŠ¡ï¼ˆè¦†ç›–æ•´ä¸ªå¤„ç†æµç¨‹ï¼‰
   monitor_task = asyncio.create_task(activity_monitor())
   
   try:
       # åˆ›å»ºè‡ªåŠ¨é‡è¯•ç®¡ç†å™¨
       retry_manager = create_auto_retry_manager(CONFIG, OUTPUT_PATH)
       
       # ä½¿ç”¨è‡ªåŠ¨é‡è¯•ç®¡ç†å™¨æ‰§è¡Œä¸»å¤„ç†é€»è¾‘
       await retry_manager.run_with_auto_retry(main_processing_logic)
   finally:
       # å–æ¶ˆæ´»åŠ¨ç›‘æ§ä»»åŠ¡
       monitor_task.cancel()
       try:
           await monitor_task
       except asyncio.CancelledError:
           pass

async def activity_monitor():
    """æ´»åŠ¨ç›‘æ§ä»»åŠ¡ï¼Œæ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡"""
    while True:
        await asyncio.sleep(300)  # 5åˆ†é’Ÿ
        UX.check_activity()

async def main_processing_logic():
    """ä¸»å¤„ç†é€»è¾‘ï¼ˆä»åŸmain_asyncå‡½æ•°ç§»åŠ¨è€Œæ¥ï¼‰"""
    
    # æ´»åŠ¨ç›‘æ§ä»»åŠ¡ç°åœ¨åœ¨main_asyncä¸­ç®¡ç†ï¼Œæ— éœ€åœ¨æ­¤åˆ›å»º
    
    # éªŒè¯ä¸¤é˜¶æ®µæ¨¡å‹é…ç½®
    required_keys = ["media_text_extraction", "media_text_analysis"]
    missing = [k for k in required_keys if k not in MODEL_POOLS]
    if missing:
        raise ValueError(f"ç¼ºå°‘ä¸¤é˜¶æ®µæ¨¡å‹æ± é…ç½®: {missing}")

    # æ£€æŸ¥APIå¯†é’¥
    api_keys = CONFIG['api']['credentials']['keys']
    if not api_keys or not api_keys[0]:
        UX.err("æœªæä¾›æœ‰æ•ˆAPIå¯†é’¥")
        return

    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    if os.path.isdir(INPUT_PATH):
        files_to_process = glob.glob(os.path.join(INPUT_PATH, '*.xlsx'))
        is_folder_mode = True
        os.makedirs(OUTPUT_PATH, exist_ok=True)
    elif os.path.isfile(INPUT_PATH):
        files_to_process = [INPUT_PATH]
        is_folder_mode = False
    else:
        UX.err("è¾“å…¥è·¯å¾„æ— æ•ˆ")
        return

    async with aiohttp.ClientSession() as session:
        api_service = APIService(session, CONFIG)
        # åˆ›å»ºåª’ä½“æ–‡æœ¬å¤„ç†å™¨
        processor = MediaTextProcessor(api_service, CONFIG, prompts)

        for file_path in files_to_process:
            file_basename = os.path.basename(file_path)
            
            # æ–‡ä»¶å¤„ç†å¼€å§‹æ ‡è®°
            UX.phase(f"å¤„ç†æ–‡ä»¶: {file_basename}")

            # è¯†åˆ«ä¿¡æº
            source = identify_source(file_basename)
            UX.info(f"ä¿¡æº: {source}")

            output_file_path = os.path.join(OUTPUT_PATH, f"(ä¸èƒ½åˆ )analyzed_{file_basename}") if is_folder_mode else OUTPUT_PATH

            # è¯»å–è¾“å…¥æ–‡ä»¶
            try:
                df_input = pd.read_excel(file_path)
                if COLUMN_MAPPING.get("MEDIA_TEXT", "text") not in df_input.columns:
                    UX.err("æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„æ–‡æœ¬åˆ—")
                    continue
            except Exception as e:
                UX.err(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                continue

            # ä¸¤é˜¶æ®µæ¨¡å‹ï¼šç›´æ¥å¤„ç†è®®é¢˜å•å…ƒï¼Œæ— éœ€ä¸­é—´æ•°æ®åº“

            # ğŸ” æ„å»ºæ–­ç‚¹ç»­ä¼ è®¡åˆ’
            UX.resume_plan(file_basename)
            total_input_articles = len(set(df_input[COLUMN_MAPPING["ID"]].astype(str)))
            
            # ä½¿ç”¨é€šç”¨æ–¹æ³•è·å–æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
            never_processed_ids = processor.get_never_processed_ids(
                output_file_path, df_input, COLUMN_MAPPING["ID"]
            )
            failed_stage1_ids = processor.load_failed_stage1_ids(
                output_file_path, COLUMN_MAPPING["ID"]
            )
            failed_units_raw = processor.load_failed_units(output_file_path)
            
            # æ„å»ºfailed_unit_idså­—å…¸ï¼ˆæŒ‰æ–‡ç« IDåˆ†ç»„ï¼‰
            failed_unit_ids = {}
            for unit in failed_units_raw:
                article_id = str(unit.get(COLUMN_MAPPING["ID"], ''))
                unit_id = str(unit.get('Unit_ID', ''))
                if article_id and unit_id:
                    failed_unit_ids.setdefault(article_id, set()).add(unit_id)
            
            # åˆå¹¶ç¬¬ä¸€é˜¶æ®µå¤±è´¥çš„IDåˆ°never_processed
            never_processed_ids = never_processed_ids | failed_stage1_ids
            
            ids_to_process = set(never_processed_ids) | set(failed_unit_ids.keys())
            failed_units_count = sum(len(v) for v in failed_unit_ids.values())
            
            # è®¡ç®—å·²å®Œæˆå•å…ƒæ•°
            completed_units = 0
            if os.path.exists(output_file_path):
                try:
                    df_existing = pd.read_excel(output_file_path)
                    if 'processing_status' in df_existing.columns:
                        completed_units = (df_existing['processing_status'] == ProcessingStatus.SUCCESS).sum()
                except:
                    pass

            # æ¸…æ™°åŒºåˆ†æ–‡ç« çº§å’Œå•å…ƒçº§ç»Ÿè®¡
            UX.info(f"æ€»æ–‡ç« æ•°: {total_input_articles} | å·²å®Œæˆå•å…ƒ: {completed_units}")
            
            # äº’æ–¥ç»Ÿè®¡ï¼šéœ€è¦é˜¶æ®µ1çš„æ–‡ç«  vs åªéœ€é˜¶æ®µ2çš„å¤±è´¥å•å…ƒ
            stage1_articles = len(never_processed_ids)  # éœ€è¦åˆ‡åˆ†çš„æ–‡ç« 
            retry_units = failed_units_count  # åªéœ€é‡æ–°åˆ†æçš„å¤±è´¥å•å…ƒ
            
            if stage1_articles > 0 and retry_units > 0:
                UX.info(f"æœ¬æ¬¡å¤„ç†: {stage1_articles} ç¯‡æ–‡ç« ï¼ˆéœ€åˆ‡åˆ†ï¼‰ + {retry_units} ä¸ªå¤±è´¥å•å…ƒï¼ˆä»…åˆ†æï¼‰")
            elif stage1_articles > 0:
                UX.info(f"æœ¬æ¬¡å¤„ç†: {stage1_articles} ç¯‡æ–‡ç« ")
            elif retry_units > 0:
                UX.info(f"æœ¬æ¬¡å¤„ç†: {retry_units} ä¸ªå¤±è´¥å•å…ƒ")
            
            UX.resume_end()

            df_to_process = df_input[df_input[COLUMN_MAPPING["ID"]].astype(str).isin(ids_to_process)].copy()

            if len(df_to_process) > 0:
                UX.phase(f"ä¸¤é˜¶æ®µå¤„ç†: å•å…ƒæå– â†’ æ·±åº¦åˆ†æ")

                # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
                task_queue = asyncio.Queue()
                results_queue = asyncio.Queue()
                total_tasks = len(df_to_process)

                # æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—
                for item in df_to_process.iterrows():
                    await task_queue.put(item)

                # åˆ›å»ºè¿›åº¦æ¡ - æ™ºèƒ½é€‰æ‹©è®¡æ•°å•ä½
                is_unit_retry_only = len(never_processed_ids) == 0 and failed_units_count > 0
                if is_unit_retry_only:
                    # çº¯å•å…ƒç»­ä¼ ï¼šæŒ‰å•å…ƒæ•°è®¡æ•°
                    progress_total = failed_units_count
                    progress_desc = f"ç»­ä¼  {failed_units_count} ä¸ªå¤±è´¥å•å…ƒ"
                else:
                    # åŒ…å«æ–°æ–‡ç« ï¼šæŒ‰æ–‡ç« æ•°è®¡æ•°
                    progress_total = total_tasks
                    progress_desc = f"å¤„ç† {len(df_to_process)} ç¯‡æ–‡ç« "
                
                pbar = aio_tqdm(total=progress_total, desc=progress_desc, 
                               bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]")

                # åˆ›å»ºä¿å­˜ä»»åŠ¡ 
                saver_task = asyncio.create_task(
                    saver_worker(results_queue, df_input, output_file_path, total_tasks)
                )

                # åˆ›å»ºå·¥ä½œä»»åŠ¡
                worker_tasks = [
                    asyncio.create_task(
                        api_worker(
                            f'worker-{i}',
                            task_queue,
                            results_queue,
                            api_service,
                            source,
                            pbar,
                            processor,
                            output_file_path=output_file_path,
                            failed_unit_ids=failed_unit_ids,
                            is_unit_mode=is_unit_retry_only
                        )
                    )
                    for i in range(MAX_CONCURRENT_REQUESTS)
                ]

                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡éƒ½è¢«workerå¤„ç†å®Œæ¯•
                await task_queue.join()
                
                # å‘æ‰€æœ‰workerå‘é€ç»“æŸä¿¡å·
                for _ in range(MAX_CONCURRENT_REQUESTS):
                    await task_queue.put(None)
                
                # ç­‰å¾…æ‰€æœ‰workeræ­£ç¡®å…³é—­
                await asyncio.gather(*worker_tasks)
                
                # å‘saverå‘é€ç»“æŸä¿¡å·å¹¶ç­‰å¾…ä¿å­˜å®Œæˆ
                await results_queue.put(None)
                await saver_task
                
                pbar.close()
            
            elif len(ids_to_process) == 0:
                UX.ok(f"æ–‡ä»¶ {file_basename} | æ‰€æœ‰æ–‡ç« å·²å¤„ç†å®Œæ¯•")
                continue
       
            # ğŸ‰ å¤„ç†å®Œæˆæ€»ç»“
            try:
                df_final_check = pd.read_excel(output_file_path)
                if not df_final_check.empty and 'processing_status' in df_final_check.columns:
                    final_success = (df_final_check['processing_status'] == ProcessingStatus.SUCCESS).sum()
                    final_no_relevant = (df_final_check['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                    final_failed = df_final_check['processing_status'].isin([ProcessingStatus.STAGE_1_FAILED, ProcessingStatus.STAGE_2_FAILED]).sum()
                    final_total_units = len(df_final_check)
                    
                    # è®¡ç®—æ–‡ç« çº§åˆ«å®Œæˆåº¦
                    final_processed_ids, final_failed_ids = get_processing_state(df_final_check, COLUMN_MAPPING["ID"])
                    final_completed_articles = len(final_processed_ids - final_failed_ids)
                    final_completion_rate = (final_completed_articles / max(1, total_input_articles)) * 100
                    
                    # æ–‡ä»¶å¤„ç†ç»“æœæ€»ç»“
                    UX.phase(f"æ–‡ä»¶ {file_basename} å¤„ç†å®Œæˆ")
                    UX.ok(f"æ–‡ç« ç»Ÿè®¡ | æ€»è®¡ {total_input_articles} | å·²å®Œæˆ {final_completed_articles} | å®Œæˆç‡ {final_completion_rate:.1f}%")
                    UX.ok(f"å•å…ƒç»Ÿè®¡ | æ€»è®¡ {final_total_units} | æˆåŠŸ {final_success} | å¤±è´¥ {final_failed} | æ— ç›¸å…³ {final_no_relevant}")
                    
                    if final_failed > 0:
                        UX.warn(f"  å¤±è´¥ {final_failed} ä¸ªå•å…ƒï¼Œå¯å†æ¬¡è¿è¡Œé‡è¯•")
                    else:
                        UX.ok(f"  æ‰€æœ‰å•å…ƒå‡å·²æˆåŠŸå¤„ç†")
                else:
                    UX.ok(f"æ–‡ä»¶ {file_basename} å¤„ç†å®Œæˆ")
            except Exception:
                UX.ok(f"æ–‡ä»¶ {file_basename} å¤„ç†å®Œæˆ")

    # ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶
    if RELIABILITY_TEST_MODE:
        UX.phase("ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶")
        import traceback
        try:
            result_files = glob.glob(os.path.join(OUTPUT_PATH, "*analyzed_*.xlsx"))
            input_files = glob.glob(os.path.join(INPUT_PATH, "*.xlsx"))

            if not result_files or not input_files:
                UX.warn("æœªæ‰¾åˆ°ä¿¡åº¦æ£€éªŒæ‰€éœ€çš„è¾“å…¥/ç»“æœæ–‡ä»¶")
            else:
                result_frames = []
                for path in result_files:
                    df_res = pd.read_excel(path)
                    # ä¸è¦†ç›–Excelé‡Œå·²æœ‰çš„Sourceï¼Œä¿¡åº¦æ£€éªŒä½¿ç”¨åŸå§‹Sourceå€¼
                    # source_name = os.path.basename(path).replace('(ä¸èƒ½åˆ )analyzed_', '')
                    # df_res['Source'] = identify_source(source_name)
                    result_frames.append(df_res)
                df_all_results = pd.concat(result_frames, ignore_index=True)

                input_frames = []
                for path in input_files:
                    df_in = pd.read_excel(path)
                    # æ ¹æ®æ–‡ä»¶åè¯†åˆ«å¹¶æ·»åŠ Sourceåˆ—ï¼ˆåª’ä½“æ–‡æœ¬è¾“å…¥æ–‡ä»¶ä¸å«Sourceï¼‰
                    source_name = identify_source(os.path.basename(path))
                    df_in['Source'] = source_name
                    input_frames.append(df_in)
                df_all_input = pd.concat(input_frames, ignore_index=True)

                from core.reliability import create_reliability_test_module

                reliability_module = create_reliability_test_module(
                    output_path=OUTPUT_PATH,
                    sampling_config=RELIABILITY_SAMPLING_CONFIG,
                    random_seed=CONFIG.get('project', {}).get('random_seed', 42)
                )

                reliability_module.generate_files(df_all_results, df_all_input)
                UX.ok("ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆå®Œæˆ")
        except Exception as e:
            UX.err(f"ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
            UX.err(traceback.format_exc())
    
    UX.phase("æ‰€æœ‰ä»»åŠ¡å®Œæˆ")

# æ·»åŠ ç¼ºå¤±çš„è¾…åŠ©å‡½æ•°
def ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ç¡®ä¿ç»“æœè¡¨å…·å¤‡ç»Ÿä¸€åˆ—ï¼šç¼ºå¤±åˆ™ä»¥ç©ºå€¼è¡¥é½ï¼Œä¸ç§»é™¤å·²æœ‰åˆ—ã€‚"""
    if df is None or df.empty:
        return pd.DataFrame(columns=REQUIRED_OUTPUT_COLUMNS)
    
    for col in REQUIRED_OUTPUT_COLUMNS:
        if col not in df.columns:
            df[col] = ""
    return df

def reorder_columns(df: pd.DataFrame, df_input_columns: list) -> pd.DataFrame:
    """å°†åˆ—é¡ºåºæ•´ç†ä¸ºï¼šè¾“å…¥è¡¨åŸæœ‰åˆ—åœ¨å‰ + REQUIRED_OUTPUT_COLUMNSï¼ˆå»é‡ä¿åºï¼‰"""
    if df is None or df.empty:
        return df
    
    preferred = list(dict.fromkeys(list(df_input_columns) + REQUIRED_OUTPUT_COLUMNS))
    # ä¿ç•™è¡¨å†…å·²æœ‰çš„åˆ—ï¼Œä¸”æŒ‰ç…§ preferred é¡ºåºé‡æ’
    ordered_existing = [c for c in preferred if c in df.columns]
    # è¿½åŠ ä»»ä½•æœªåœ¨ preferred ä¸­ä½†å­˜åœ¨äº df çš„åˆ—ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
    tail = [c for c in df.columns if c not in preferred]
    return df.reindex(columns=ordered_existing + tail)

# ç¨‹åºå…¥å£
if __name__ == "__main__":
    import sys
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main_async())