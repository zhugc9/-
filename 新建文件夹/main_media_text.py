# -*- coding: utf-8 -*-
"""
Â™í‰ΩìÊñáÊú¨ÂàÜÊûê‰∏ªÂÖ•Âè£ - ÁßçÂ≠êÊâ©Â±ï‰∏§Èò∂ÊÆµÂàÜÊûê
"""
import os
import glob
import asyncio
import aiohttp
import pandas as pd
from asyncio import Lock
from tqdm.asyncio import tqdm as aio_tqdm

# ÂØºÂÖ•Ê†∏ÂøÉÊ®°Âùó
from core import (
    UX,
    load_config,
    APIService,
    ReliabilityTestModule,
    safe_str_convert,
    identify_source,
    create_auto_retry_manager,
)
from core.media_text_processor import MediaTextProcessor
from core.utils import ProcessingStatus, get_processing_state

# Âä†ËΩΩÈÖçÁΩÆ
CONFIG = load_config()
if CONFIG is None:
    raise RuntimeError("Êó†Ê≥ïÂä†ËΩΩÈÖçÁΩÆÊñá‰ª∂ÔºåÁ®ãÂ∫èÈÄÄÂá∫")

# Êñ∞ÁöÑÁªìÊûÑÂåñÈÖçÁΩÆËØªÂèñ
# APIÈÖçÁΩÆ
api_config = CONFIG['api']
api_strategy = api_config['strategy']
MAX_CONCURRENT_REQUESTS = api_strategy['max_concurrent_requests']
QUEUE_TIMEOUT = api_strategy['queue_timeout_sec']

# Â™í‰ΩìÊñáÊú¨ÊµÅÁ®ãÈÖçÁΩÆ
media_config = CONFIG.get('media_text', {})

# ËæìÂÖ•ËæìÂá∫Ë∑ØÂæÑÈÖçÁΩÆ
INPUT_PATH = media_config['paths']['input']
OUTPUT_PATH = media_config['paths']['output']

# ÈÄöÁî®Ëá™Âä®ÈáçËØïÈÖçÁΩÆ
AUTO_RETRY_CONFIG = CONFIG['processing']['auto_retry']
AUTO_RETRY_ENABLED = AUTO_RETRY_CONFIG['enabled']
MAX_RETRY_ROUNDS = AUTO_RETRY_CONFIG['max_rounds']
RETRY_DELAY_MINUTES = AUTO_RETRY_CONFIG['delay_minutes']
MIN_FAILED_THRESHOLD = AUTO_RETRY_CONFIG['min_failed_threshold']

# ‰ø°Â∫¶Ê£ÄÈ™åÈÖçÁΩÆ
RELIABILITY_TEST_CONFIG = CONFIG['reliability_test']
RELIABILITY_TEST_MODE = RELIABILITY_TEST_CONFIG['enabled']
RELIABILITY_SAMPLING_CONFIG = RELIABILITY_TEST_CONFIG['sampling_config']

# ÂàóÊò†Â∞ÑÂíåËæìÂá∫ÂàóÈÖçÁΩÆ
COLUMN_MAPPING = CONFIG.get('media_text', {}).get('column_mapping', {})
REQUIRED_OUTPUT_COLUMNS = CONFIG.get('media_text', {}).get('required_output_columns', CONFIG.get('required_output_columns', []))

# Ê®°ÂûãÊ±†ÈÖçÁΩÆÔºàÂÖ®Â±ÄÁªü‰∏ÄÔºâ
MODEL_POOLS = CONFIG.get('model_pools', {})
API_CONFIG = api_config

# ÂÖ®Â±ÄÈîÅ
file_write_lock = Lock()

# ÊèêÁ§∫ËØçÁ±ª
class Prompts:
    def __init__(self):
        self.prompts_dir = os.path.join(os.path.dirname(__file__), 'prompts')
        self._load_prompts()

    def _load_prompts(self):
        prompt_files = {
            'UNIT_EXTRACTION': 'SEED.txt',
            'UNIT_ANALYSIS': 'ANALYSIS.txt'
        }
        for attr_name, filename in prompt_files.items():
            file_path = os.path.join(self.prompts_dir, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    setattr(self, attr_name, f.read())
            except Exception as e:
                UX.err(f"Âä†ËΩΩÊèêÁ§∫ËØçÊñá‰ª∂ {filename} Â§±Ë¥•: {e}")
                setattr(self, attr_name, "")

# ÂàõÂª∫ÊèêÁ§∫ËØçÂÆû‰æã
prompts = Prompts()

async def api_worker(name, task_queue, results_queue, api_service, source, pbar, processor, output_file_path=None, failed_unit_ids=None, is_unit_mode=False):
   while True:
       try:
           item = await task_queue.get()
           if item is None:
               break
           
           # ÊØè10‰∏™‰ªªÂä°Ê£ÄÊü•‰∏ÄÊ¨°Ê¥ªÂä®Áä∂ÊÄÅ
           if pbar.n % 10 == 0:
               UX.check_activity()
           
           try:
               item_with_source = (item[0], item[1], source)
               current_id = safe_str_convert(item[1].get(COLUMN_MAPPING["ID"], "unknown")) if len(item) > 1 else "unknown"
               original_id, result_Units = await processor.process_row(
                   item_with_source,
                   output_file_path=output_file_path,
                   failed_unit_ids=failed_unit_ids
               )
               await results_queue.put((original_id, result_Units, None))  # ‰∏§Èò∂ÊÆµÊ®°ÂûãÔºöÂè™ËøîÂõûËÆÆÈ¢òÂçïÂÖÉ
           except Exception as e:
               error_brief = str(e)[:30] + "..." if len(str(e)) > 30 else str(e)
               UX.api_failed(f"Worker-{name}", error_brief)
               original_id = safe_str_convert(item[1].get(COLUMN_MAPPING["ID"], "unknown")) if len(item) > 1 else "unknown"
               from core.utils import create_unified_record
               await results_queue.put((original_id, [create_unified_record(ProcessingStatus.STAGE_2_FAILED, original_id, source, "", f"WorkerÈîôËØØ: {str(e)[:100]}")], None))
           finally:
               # ‰øùËØÅÊó†ËÆ∫ÊàêÂäüÂ§±Ë¥•Ôºå‰ªªÂä°ÂÆåÊàêÂêéÈÉΩÊõ¥Êñ∞ËøõÂ∫¶Êù°
               # ÂçïÂÖÉÁª≠‰º†Ê®°ÂºèÔºöÊåâÂΩìÂâçÊñáÁ´†ÁöÑÂ§±Ë¥•ÂçïÂÖÉÊï∞Êõ¥Êñ∞ÔºõÊôÆÈÄöÊ®°ÂºèÔºöÊåâÊñáÁ´†Êï∞Êõ¥Êñ∞
               if is_unit_mode and failed_unit_ids and current_id in failed_unit_ids:
                   pbar.update(len(failed_unit_ids[current_id]))
               else:
                   pbar.update(1)
               task_queue.task_done()
       
       except asyncio.CancelledError:
           break
       except Exception as e:
           error_brief = str(e)[:30] + "..." if len(str(e)) > 30 else str(e)
           UX.api_failed(f"Worker-{name}-Critical", error_brief)

async def saver_worker(results_queue, df_input, output_file_path, total_tasks=None):
    """
    [‰∏§Èò∂ÊÆµÊ®°Âûã] ÊâπÈáè‰øùÂ≠òworkerÔºö
    - ÁºìÂÜ≤Âπ∂ÂàÜÊâπ‰øùÂ≠òËÆÆÈ¢òÂçïÂÖÉÂàÜÊûêÁªìÊûú
    """
    analysis_buffer = []
    received_count = 0
    
    # ‰ªéÈÖçÁΩÆËØªÂèñÁºìÂÜ≤Âå∫Â§ßÂ∞èÈòàÂÄº
    general_config = CONFIG.get('processing', {}).get('general', {})
    ANALYSIS_BUFFER_LIMIT = general_config.get('buffer_limit', 20)    # ÂàÜÊûêÁªìÊûúÁºìÂÜ≤Âå∫

    async def save_analysis_batch(data):
        if not data:
            return
        async with file_write_lock:
            # Ë∞ÉÁî®Áé∞ÊúâÁöÑÂàÜÊûêÁªìÊûú‰øùÂ≠òÂáΩÊï∞
            save_data_to_excel(data, df_input, output_file_path)

    while True:
        try:
            # Á≠âÂæÖÁªìÊûúÔºå‰ªéÈÖçÁΩÆËØªÂèñË∂ÖÊó∂Êó∂Èó¥
            item = await asyncio.wait_for(results_queue.get(), timeout=QUEUE_TIMEOUT)
            if item is None: # Êî∂Âà∞ÁªìÊùü‰ø°Âè∑
                await save_analysis_batch(analysis_buffer)
                break

            original_id, result_Units, _ = item  # Á¨¨‰∏â‰∏™ÂèÇÊï∞‰∏∫ÂÖºÂÆπÊÄß‰øùÁïôÔºå‰∏§Èò∂ÊÆµÊ®°Âûã‰∏≠Êú™‰ΩøÁî®
            received_count += 1

            # Êî∂ÈõÜÂàÜÊûêÁªìÊûúÂà∞ÁºìÂÜ≤Âå∫
            if result_Units:
                for unit in result_Units:
                    unit[COLUMN_MAPPING["ID"]] = original_id
                analysis_buffer.extend(result_Units)

            # Ê£ÄÊü•ÊòØÂê¶ÈúÄË¶ÅÊ∏ÖÁ©∫Âπ∂‰øùÂ≠òÁºìÂÜ≤Âå∫
            if len(analysis_buffer) >= ANALYSIS_BUFFER_LIMIT:
                await save_analysis_batch(analysis_buffer)
                analysis_buffer = []

            results_queue.task_done()
            if total_tasks and received_count >= total_tasks:
                await save_analysis_batch(analysis_buffer)
                break

        except asyncio.TimeoutError:
            # Ë∂ÖÊó∂Âêé‰øùÂ≠òÊâÄÊúâÁºìÂÜ≤Âå∫ÂÜÖÂÆπÔºåÈò≤Ê≠¢Âú®‰ªªÂä°Èó¥Èöô‰∏¢Â§±Êï∞ÊçÆ
            await save_analysis_batch(analysis_buffer)
            analysis_buffer = []
            if total_tasks and received_count >= total_tasks:
                break
            continue

        except asyncio.CancelledError:
            await save_analysis_batch(analysis_buffer)
            break

        except Exception as e:
            UX.err(f"‰øùÂ≠òÁ∫øÁ®ãÈîôËØØ: {e}")

def save_data_to_excel(new_Units_list, df_input, output_file_path):
   try:
       df_existing = pd.DataFrame()
       if os.path.exists(output_file_path):
           try:
               df_existing = pd.read_excel(output_file_path)
           except Exception as e:
               UX.warn(f"ËØªÂèñÁé∞ÊúâÊñá‰ª∂Â§±Ë¥•: {e}")
       df_new_Units = pd.DataFrame(new_Units_list)

       if df_new_Units.empty:
           return

       df_existing = ensure_required_columns(df_existing)
       df_new_Units = ensure_required_columns(df_new_Units)
       
       if COLUMN_MAPPING["ID"] in df_input.columns and COLUMN_MAPPING["ID"] in df_new_Units.columns:
           df_input[COLUMN_MAPPING["ID"]] = df_input[COLUMN_MAPPING["ID"]].astype(str)
           df_new_Units[COLUMN_MAPPING["ID"]] = df_new_Units[COLUMN_MAPPING["ID"]].astype(str)
           new_original_ids = df_new_Units[COLUMN_MAPPING["ID"]].unique()
           
           # üîß ‰øÆÂ§çÊ†∏ÂøÉÈóÆÈ¢òÔºöÊ∏ÖÁêÜÊóßÁöÑ"Ê¶ÇË¶ÅÁ∫ß"Â§±Ë¥•ËÆ∞ÂΩï
           # ÂΩì‰∏ÄÁØáÊñáÁ´†Ë¢´ÈáçÊñ∞Â§ÑÁêÜÊàêÂäüÊó∂ÔºåÂà†Èô§ÂÆÉÁöÑÊóßÂ§±Ë¥•ËÆ∞ÂΩï
           if not df_existing.empty and 'Unit_ID' in df_existing.columns:
               # ËØÜÂà´"Ê¶ÇË¶ÅÁ∫ß"ËÆ∞ÂΩïÔºöUnit_ID‰∏çÂåÖÂê´"-Unit-"ÁöÑËÆ∞ÂΩï
               # ‰æãÂ¶ÇÔºö"(3)-RU1012-API_FAILED" Êàñ "(3)-RU1012-NO_RELEVANT"
               summary_mask = (
                   df_existing[COLUMN_MAPPING["ID"]].isin(new_original_ids) & 
                   ~df_existing['Unit_ID'].str.contains('-Unit-', na=False)
               )
               
               removed_count = summary_mask.sum()
               if removed_count > 0:
                   df_existing = df_existing[~summary_mask].copy()

           input_base_cols = [col for col in df_input.columns if col in df_new_Units.columns and col != COLUMN_MAPPING["ID"]]
           if input_base_cols:
               df_new_Units = df_new_Units.drop(columns=input_base_cols)
           
           df_input_subset = df_input[df_input[COLUMN_MAPPING["ID"]].isin(new_original_ids)].copy()
           df_newly_merged = pd.merge(df_input_subset, df_new_Units, on=COLUMN_MAPPING["ID"], how='left')
       else:
           df_newly_merged = df_new_Units.copy()

       df_final_to_save = pd.concat([df_existing, df_newly_merged], ignore_index=True)

       if 'Unit_ID' in df_final_to_save.columns:
           df_final_to_save = df_final_to_save.drop_duplicates(subset=['Unit_ID'], keep='last')

       df_final_to_save = reorder_columns(df_final_to_save, list(df_input.columns))
       df_final_to_save.to_excel(output_file_path, index=False)
       UX.ok(f"Â∑≤‰øùÂ≠ò: {os.path.basename(output_file_path)} (Á¥ØËÆ° {len(df_final_to_save)} Êù°ËÆ∞ÂΩï)")

   except Exception as e:
       UX.err(f"Excel‰øùÂ≠òÂ§±Ë¥•: {e}")

async def main_async():
   UX.start_run()
   UX.phase("ÈïøÊñáÊú¨ÂàÜÊûêÂô®ÂêØÂä®")
   UX.info(f"‰ø°Â∫¶Ê£ÄÈ™åÊ®°Âºè: {'ÂºÄÂêØ' if RELIABILITY_TEST_MODE else 'ÂÖ≥Èó≠'}")
   
   # ÂêØÂä®Ê¥ªÂä®ÁõëÊéß‰ªªÂä°ÔºàË¶ÜÁõñÊï¥‰∏™Â§ÑÁêÜÊµÅÁ®ãÔºâ
   monitor_task = asyncio.create_task(activity_monitor())
   
   try:
       # ÂàõÂª∫Ëá™Âä®ÈáçËØïÁÆ°ÁêÜÂô®
       retry_manager = create_auto_retry_manager(CONFIG, OUTPUT_PATH)
       
       # ‰ΩøÁî®Ëá™Âä®ÈáçËØïÁÆ°ÁêÜÂô®ÊâßË°å‰∏ªÂ§ÑÁêÜÈÄªËæë
       await retry_manager.run_with_auto_retry(main_processing_logic)
   finally:
       # ÂèñÊ∂àÊ¥ªÂä®ÁõëÊéß‰ªªÂä°
       monitor_task.cancel()
       try:
           await monitor_task
       except asyncio.CancelledError:
           pass

async def activity_monitor():
    """Ê¥ªÂä®ÁõëÊéß‰ªªÂä°ÔºåÊØè5ÂàÜÈíüÊ£ÄÊü•‰∏ÄÊ¨°"""
    while True:
        await asyncio.sleep(300)  # 5ÂàÜÈíü
        UX.check_activity()

async def main_processing_logic():
    """‰∏ªÂ§ÑÁêÜÈÄªËæëÔºà‰ªéÂéümain_asyncÂáΩÊï∞ÁßªÂä®ËÄåÊù•Ôºâ"""
    
    # Ê¥ªÂä®ÁõëÊéß‰ªªÂä°Áé∞Âú®Âú®main_async‰∏≠ÁÆ°ÁêÜÔºåÊó†ÈúÄÂú®Ê≠§ÂàõÂª∫
    
    # È™åËØÅ‰∏§Èò∂ÊÆµÊ®°ÂûãÈÖçÁΩÆ
    required_keys = ["media_text_extraction", "media_text_analysis"]
    missing = [k for k in required_keys if k not in MODEL_POOLS]
    if missing:
        raise ValueError(f"Áº∫Â∞ë‰∏§Èò∂ÊÆµÊ®°ÂûãÊ±†ÈÖçÁΩÆ: {missing}")

    # Ê£ÄÊü•APIÂØÜÈí•
    api_keys = CONFIG['api']['credentials']['keys']
    if not api_keys or not api_keys[0]:
        UX.err("Êú™Êèê‰æõÊúâÊïàAPIÂØÜÈí•")
        return

    # Á°ÆÂÆöËæìÂÖ•Êñá‰ª∂
    if os.path.isdir(INPUT_PATH):
        files_to_process = glob.glob(os.path.join(INPUT_PATH, '*.xlsx'))
        is_folder_mode = True
        os.makedirs(OUTPUT_PATH, exist_ok=True)
    elif os.path.isfile(INPUT_PATH):
        files_to_process = [INPUT_PATH]
        is_folder_mode = False
    else:
        UX.err("ËæìÂÖ•Ë∑ØÂæÑÊó†Êïà")
        return

    async with aiohttp.ClientSession() as session:
        api_service = APIService(session, CONFIG)
        # ÂàõÂª∫Â™í‰ΩìÊñáÊú¨Â§ÑÁêÜÂô®
        processor = MediaTextProcessor(api_service, CONFIG, prompts)

        for file_path in files_to_process:
            file_basename = os.path.basename(file_path)
            
            # Êñá‰ª∂Â§ÑÁêÜÂºÄÂßãÊ†áËÆ∞
            UX.phase(f"Â§ÑÁêÜÊñá‰ª∂: {file_basename}")

            # ËØÜÂà´‰ø°Ê∫ê
            source = identify_source(file_basename)
            UX.info(f"‰ø°Ê∫ê: {source}")

            output_file_path = os.path.join(OUTPUT_PATH, f"(‰∏çËÉΩÂà†)analyzed_{file_basename}") if is_folder_mode else OUTPUT_PATH

            # ËØªÂèñËæìÂÖ•Êñá‰ª∂
            try:
                df_input = pd.read_excel(file_path)
                if COLUMN_MAPPING.get("MEDIA_TEXT", "text") not in df_input.columns:
                    UX.err("Êñá‰ª∂Áº∫Â∞ëÂøÖË¶ÅÁöÑÊñáÊú¨Âàó")
                    continue
            except Exception as e:
                UX.err(f"ËØªÂèñÊñá‰ª∂Â§±Ë¥•: {e}")
                continue

            # ‰∏§Èò∂ÊÆµÊ®°ÂûãÔºöÁõ¥Êé•Â§ÑÁêÜËÆÆÈ¢òÂçïÂÖÉÔºåÊó†ÈúÄ‰∏≠Èó¥Êï∞ÊçÆÂ∫ì

            # üîç ÊûÑÂª∫Êñ≠ÁÇπÁª≠‰º†ËÆ°Âàí
            UX.resume_plan(file_basename)
            total_input_articles = len(set(df_input[COLUMN_MAPPING["ID"]].astype(str)))
            
            # ‰ΩøÁî®ÈÄöÁî®ÊñπÊ≥ïËé∑ÂèñÊñ≠ÁÇπÁª≠‰º†‰ø°ÊÅØ
            never_processed_ids = processor.get_never_processed_ids(
                output_file_path, df_input, COLUMN_MAPPING["ID"]
            )
            failed_stage1_ids = processor.load_failed_stage1_ids(
                output_file_path, COLUMN_MAPPING["ID"]
            )
            failed_units_raw = processor.load_failed_units(output_file_path)
            
            # ÊûÑÂª∫failed_unit_idsÂ≠óÂÖ∏ÔºàÊåâÊñáÁ´†IDÂàÜÁªÑÔºâ
            failed_unit_ids = {}
            for unit in failed_units_raw:
                article_id = str(unit.get(COLUMN_MAPPING["ID"], ''))
                unit_id = str(unit.get('Unit_ID', ''))
                if article_id and unit_id:
                    failed_unit_ids.setdefault(article_id, set()).add(unit_id)
            
            # ÂêàÂπ∂Á¨¨‰∏ÄÈò∂ÊÆµÂ§±Ë¥•ÁöÑIDÂà∞never_processed
            never_processed_ids = never_processed_ids | failed_stage1_ids
            
            ids_to_process = set(never_processed_ids) | set(failed_unit_ids.keys())
            failed_units_count = sum(len(v) for v in failed_unit_ids.values())
            
            # ËÆ°ÁÆóÂ∑≤ÂÆåÊàêÂçïÂÖÉÊï∞
            completed_units = 0
            if os.path.exists(output_file_path):
                try:
                    df_existing = pd.read_excel(output_file_path)
                    if 'processing_status' in df_existing.columns:
                        completed_units = (df_existing['processing_status'] == ProcessingStatus.SUCCESS).sum()
                except:
                    pass

            # Ê∏ÖÊô∞Âå∫ÂàÜÊñáÁ´†Á∫ßÂíåÂçïÂÖÉÁ∫ßÁªüËÆ°
            UX.info(f"ÊÄªÊñáÁ´†Êï∞: {total_input_articles} | Â∑≤ÂÆåÊàêÂçïÂÖÉ: {completed_units}")
            
            # ‰∫íÊñ•ÁªüËÆ°ÔºöÈúÄË¶ÅÈò∂ÊÆµ1ÁöÑÊñáÁ´† vs Âè™ÈúÄÈò∂ÊÆµ2ÁöÑÂ§±Ë¥•ÂçïÂÖÉ
            stage1_articles = len(never_processed_ids)  # ÈúÄË¶ÅÂàáÂàÜÁöÑÊñáÁ´†
            retry_units = failed_units_count  # Âè™ÈúÄÈáçÊñ∞ÂàÜÊûêÁöÑÂ§±Ë¥•ÂçïÂÖÉ
            
            if stage1_articles > 0 and retry_units > 0:
                UX.info(f"Êú¨Ê¨°Â§ÑÁêÜ: {stage1_articles} ÁØáÊñáÁ´†ÔºàÈúÄÂàáÂàÜÔºâ + {retry_units} ‰∏™Â§±Ë¥•ÂçïÂÖÉÔºà‰ªÖÂàÜÊûêÔºâ")
            elif stage1_articles > 0:
                UX.info(f"Êú¨Ê¨°Â§ÑÁêÜ: {stage1_articles} ÁØáÊñáÁ´†")
            elif retry_units > 0:
                UX.info(f"Êú¨Ê¨°Â§ÑÁêÜ: {retry_units} ‰∏™Â§±Ë¥•ÂçïÂÖÉ")
            
            UX.resume_end()

            df_to_process = df_input[df_input[COLUMN_MAPPING["ID"]].astype(str).isin(ids_to_process)].copy()

            if len(df_to_process) > 0:
                UX.phase(f"‰∏§Èò∂ÊÆµÂ§ÑÁêÜ: ÂçïÂÖÉÊèêÂèñ ‚Üí Ê∑±Â∫¶ÂàÜÊûê")

                # ÂàõÂª∫‰ªªÂä°ÈòüÂàó
                task_queue = asyncio.Queue()
                results_queue = asyncio.Queue()
                total_tasks = len(df_to_process)

                # Ê∑ªÂä†‰ªªÂä°Âà∞ÈòüÂàó
                for item in df_to_process.iterrows():
                    await task_queue.put(item)

                # ÂàõÂª∫ËøõÂ∫¶Êù° - Êô∫ËÉΩÈÄâÊã©ËÆ°Êï∞Âçï‰Ωç
                is_unit_retry_only = len(never_processed_ids) == 0 and failed_units_count > 0
                if is_unit_retry_only:
                    # Á∫ØÂçïÂÖÉÁª≠‰º†ÔºöÊåâÂçïÂÖÉÊï∞ËÆ°Êï∞
                    progress_total = failed_units_count
                    progress_desc = f"Áª≠‰º† {failed_units_count} ‰∏™Â§±Ë¥•ÂçïÂÖÉ"
                else:
                    # ÂåÖÂê´Êñ∞ÊñáÁ´†ÔºöÊåâÊñáÁ´†Êï∞ËÆ°Êï∞
                    progress_total = total_tasks
                    progress_desc = f"Â§ÑÁêÜ {len(df_to_process)} ÁØáÊñáÁ´†"
                
                pbar = aio_tqdm(total=progress_total, desc=progress_desc, 
                               bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]")

                # ÂàõÂª∫‰øùÂ≠ò‰ªªÂä° 
                saver_task = asyncio.create_task(
                    saver_worker(results_queue, df_input, output_file_path, total_tasks)
                )

                # ÂàõÂª∫Â∑•‰Ωú‰ªªÂä°
                worker_tasks = [
                    asyncio.create_task(
                        api_worker(
                            f'worker-{i}',
                            task_queue,
                            results_queue,
                            api_service,
                            source,
                            pbar,
                            processor,
                            output_file_path=output_file_path,
                            failed_unit_ids=failed_unit_ids,
                            is_unit_mode=is_unit_retry_only
                        )
                    )
                    for i in range(MAX_CONCURRENT_REQUESTS)
                ]

                # Á≠âÂæÖÊâÄÊúâ‰ªªÂä°ÈÉΩË¢´workerÂ§ÑÁêÜÂÆåÊØï
                await task_queue.join()
                
                # ÂêëÊâÄÊúâworkerÂèëÈÄÅÁªìÊùü‰ø°Âè∑
                for _ in range(MAX_CONCURRENT_REQUESTS):
                    await task_queue.put(None)
                
                # Á≠âÂæÖÊâÄÊúâworkerÊ≠£Á°ÆÂÖ≥Èó≠
                await asyncio.gather(*worker_tasks)
                
                # ÂêësaverÂèëÈÄÅÁªìÊùü‰ø°Âè∑Âπ∂Á≠âÂæÖ‰øùÂ≠òÂÆåÊàê
                await results_queue.put(None)
                await saver_task
                
                pbar.close()
            
            elif len(ids_to_process) == 0:
                UX.ok(f"Êñá‰ª∂ {file_basename} | ÊâÄÊúâÊñáÁ´†Â∑≤Â§ÑÁêÜÂÆåÊØï")
                continue
       
            # üéâ Â§ÑÁêÜÂÆåÊàêÊÄªÁªì
            try:
                df_final_check = pd.read_excel(output_file_path)
                if not df_final_check.empty and 'processing_status' in df_final_check.columns:
                    final_success = (df_final_check['processing_status'] == ProcessingStatus.SUCCESS).sum()
                    final_no_relevant = (df_final_check['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                    final_failed = df_final_check['processing_status'].isin([ProcessingStatus.STAGE_1_FAILED, ProcessingStatus.STAGE_2_FAILED]).sum()
                    final_total_units = len(df_final_check)
                    
                    # ËÆ°ÁÆóÊñáÁ´†Á∫ßÂà´ÂÆåÊàêÂ∫¶
                    final_processed_ids, final_failed_ids = get_processing_state(df_final_check, COLUMN_MAPPING["ID"])
                    final_completed_articles = len(final_processed_ids - final_failed_ids)
                    final_completion_rate = (final_completed_articles / max(1, total_input_articles)) * 100
                    
                    # Êñá‰ª∂Â§ÑÁêÜÁªìÊûúÊÄªÁªì
                    UX.phase(f"Êñá‰ª∂ {file_basename} Â§ÑÁêÜÂÆåÊàê")
                    UX.ok(f"ÊñáÁ´†ÁªüËÆ° | ÊÄªËÆ° {total_input_articles} | Â∑≤ÂÆåÊàê {final_completed_articles} | ÂÆåÊàêÁéá {final_completion_rate:.1f}%")
                    UX.ok(f"ÂçïÂÖÉÁªüËÆ° | ÊÄªËÆ° {final_total_units} | ÊàêÂäü {final_success} | Â§±Ë¥• {final_failed} | Êó†Áõ∏ÂÖ≥ {final_no_relevant}")
                    
                    if final_failed > 0:
                        UX.warn(f"  Â§±Ë¥• {final_failed} ‰∏™ÂçïÂÖÉÔºåÂèØÂÜçÊ¨°ËøêË°åÈáçËØï")
                    else:
                        UX.ok(f"  ÊâÄÊúâÂçïÂÖÉÂùáÂ∑≤ÊàêÂäüÂ§ÑÁêÜ")
                else:
                    UX.ok(f"Êñá‰ª∂ {file_basename} Â§ÑÁêÜÂÆåÊàê")
            except Exception:
                UX.ok(f"Êñá‰ª∂ {file_basename} Â§ÑÁêÜÂÆåÊàê")

    # ÁîüÊàê‰ø°Â∫¶Ê£ÄÈ™åÊñá‰ª∂
    if RELIABILITY_TEST_MODE:
        UX.phase("ÁîüÊàê‰ø°Â∫¶Ê£ÄÈ™åÊñá‰ª∂")
        import traceback
        try:
            result_files = glob.glob(os.path.join(OUTPUT_PATH, "*analyzed_*.xlsx"))
            input_files = glob.glob(os.path.join(INPUT_PATH, "*.xlsx"))

            if not result_files or not input_files:
                UX.warn("Êú™ÊâæÂà∞‰ø°Â∫¶Ê£ÄÈ™åÊâÄÈúÄÁöÑËæìÂÖ•/ÁªìÊûúÊñá‰ª∂")
            else:
                result_frames = []
                for path in result_files:
                    df_res = pd.read_excel(path)
                    # ‰∏çË¶ÜÁõñExcelÈáåÂ∑≤ÊúâÁöÑSourceÔºå‰ø°Â∫¶Ê£ÄÈ™å‰ΩøÁî®ÂéüÂßãSourceÂÄº
                    # source_name = os.path.basename(path).replace('(‰∏çËÉΩÂà†)analyzed_', '')
                    # df_res['Source'] = identify_source(source_name)
                    result_frames.append(df_res)
                df_all_results = pd.concat(result_frames, ignore_index=True)

                input_frames = []
                for path in input_files:
                    df_in = pd.read_excel(path)
                    # Ê†πÊçÆÊñá‰ª∂ÂêçËØÜÂà´Âπ∂Ê∑ªÂä†SourceÂàóÔºàÂ™í‰ΩìÊñáÊú¨ËæìÂÖ•Êñá‰ª∂‰∏çÂê´SourceÔºâ
                    source_name = identify_source(os.path.basename(path))
                    df_in['Source'] = source_name
                    input_frames.append(df_in)
                df_all_input = pd.concat(input_frames, ignore_index=True)

                from core.reliability import create_reliability_test_module

                reliability_module = create_reliability_test_module(
                    output_path=OUTPUT_PATH,
                    sampling_config=RELIABILITY_SAMPLING_CONFIG,
                    random_seed=CONFIG.get('project', {}).get('random_seed', 42)
                )

                reliability_module.generate_files(df_all_results, df_all_input)
                UX.ok("‰ø°Â∫¶Ê£ÄÈ™åÊñá‰ª∂ÁîüÊàêÂÆåÊàê")
        except Exception as e:
            UX.err(f"‰ø°Â∫¶Ê£ÄÈ™åÊñá‰ª∂ÁîüÊàêÂ§±Ë¥•: {e}")
            UX.err(traceback.format_exc())
    
    UX.phase("ÊâÄÊúâ‰ªªÂä°ÂÆåÊàê")

# Ê∑ªÂä†Áº∫Â§±ÁöÑËæÖÂä©ÂáΩÊï∞
def ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Á°Æ‰øùÁªìÊûúË°®ÂÖ∑Â§áÁªü‰∏ÄÂàóÔºöÁº∫Â§±Âàô‰ª•Á©∫ÂÄºË°•ÈΩêÔºå‰∏çÁßªÈô§Â∑≤ÊúâÂàó„ÄÇ"""
    if df is None or df.empty:
        return pd.DataFrame(columns=REQUIRED_OUTPUT_COLUMNS)
    
    for col in REQUIRED_OUTPUT_COLUMNS:
        if col not in df.columns:
            df[col] = ""
    return df

def reorder_columns(df: pd.DataFrame, df_input_columns: list) -> pd.DataFrame:
    """Â∞ÜÂàóÈ°∫Â∫èÊï¥ÁêÜ‰∏∫ÔºöËæìÂÖ•Ë°®ÂéüÊúâÂàóÂú®Ââç + REQUIRED_OUTPUT_COLUMNSÔºàÂéªÈáç‰øùÂ∫èÔºâ"""
    if df is None or df.empty:
        return df
    
    preferred = list(dict.fromkeys(list(df_input_columns) + REQUIRED_OUTPUT_COLUMNS))
    # ‰øùÁïôË°®ÂÜÖÂ∑≤ÊúâÁöÑÂàóÔºå‰∏îÊåâÁÖß preferred È°∫Â∫èÈáçÊéí
    ordered_existing = [c for c in preferred if c in df.columns]
    # ËøΩÂä†‰ªª‰ΩïÊú™Âú® preferred ‰∏≠‰ΩÜÂ≠òÂú®‰∫é df ÁöÑÂàóÔºåÈÅøÂÖç‰ø°ÊÅØ‰∏¢Â§±
    tail = [c for c in df.columns if c not in preferred]
    return df.reindex(columns=ordered_existing + tail)

# Á®ãÂ∫èÂÖ•Âè£
if __name__ == "__main__":
    import sys
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main_async())