# -*- coding: utf-8 -*-
"""
åª’ä½“æ–‡æœ¬åˆ†æä¸»å…¥å£ - ç§å­æ‰©å±•ä¸¤é˜¶æ®µåˆ†æ
"""
import os
import glob
import asyncio
import aiohttp
import pandas as pd
from asyncio import Lock
from tqdm.asyncio import tqdm as aio_tqdm

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core import (
    UX, load_config, APIService, MediaTextProcessor, 
    ReliabilityTestModule, safe_str_convert, identify_source,
    create_auto_retry_manager
)
from core.utils import ProcessingStatus, get_processing_state

# åŠ è½½é…ç½®
CONFIG = load_config()
if CONFIG is None:
    raise RuntimeError("æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")

# æ–°çš„ç»“æ„åŒ–é…ç½®è¯»å–
# APIé…ç½®
api_config = CONFIG['api']
api_strategy = api_config['strategy']
MAX_CONCURRENT_REQUESTS = api_strategy['max_concurrent_requests']
QUEUE_TIMEOUT = api_strategy['queue_timeout_sec']

# åª’ä½“æ–‡æœ¬æµç¨‹é…ç½®
media_config = CONFIG.get('media_text', {})

# è¾“å…¥è¾“å‡ºè·¯å¾„é…ç½®
INPUT_PATH = media_config['paths']['input']
OUTPUT_PATH = media_config['paths']['output']

# é€šç”¨è‡ªåŠ¨é‡è¯•é…ç½®
AUTO_RETRY_CONFIG = CONFIG['processing']['auto_retry']
AUTO_RETRY_ENABLED = AUTO_RETRY_CONFIG['enabled']
MAX_RETRY_ROUNDS = AUTO_RETRY_CONFIG['max_rounds']
RETRY_DELAY_MINUTES = AUTO_RETRY_CONFIG['delay_minutes']
MIN_FAILED_THRESHOLD = AUTO_RETRY_CONFIG['min_failed_threshold']

# ä¿¡åº¦æ£€éªŒé…ç½®
RELIABILITY_TEST_CONFIG = CONFIG['reliability_test']
RELIABILITY_TEST_MODE = RELIABILITY_TEST_CONFIG['enabled']
RELIABILITY_SAMPLING_CONFIG = RELIABILITY_TEST_CONFIG['sampling_config']

# åˆ—æ˜ å°„å’Œè¾“å‡ºåˆ—é…ç½®
COLUMN_MAPPING = CONFIG.get('media_text', {}).get('column_mapping', {})
REQUIRED_OUTPUT_COLUMNS = CONFIG.get('media_text', {}).get('required_output_columns', CONFIG.get('required_output_columns', []))

# æ¨¡å‹æ± é…ç½®ï¼ˆå…¨å±€ç»Ÿä¸€ï¼‰
MODEL_POOLS = CONFIG.get('model_pools', {})
API_CONFIG = api_config

# å…¨å±€é”
file_write_lock = Lock()

# æç¤ºè¯ç±»
class Prompts:
    def __init__(self):
        self.prompts_dir = os.path.join(os.path.dirname(__file__), 'prompts')
        self._load_prompts()

    def _load_prompts(self):
        prompt_files = {
            'UNIT_EXTRACTION': 'SEED.txt',
            'UNIT_ANALYSIS': 'ANALYSIS.txt'
        }
        for attr_name, filename in prompt_files.items():
            file_path = os.path.join(self.prompts_dir, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    setattr(self, attr_name, f.read())
            except Exception as e:
                UX.err(f"åŠ è½½æç¤ºè¯æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                setattr(self, attr_name, "")

# åˆ›å»ºæç¤ºè¯å®ä¾‹
prompts = Prompts()

async def api_worker(name, task_queue, results_queue, api_service, source, pbar, processor, output_file_path=None, failed_unit_ids=None):
   while True:
       try:
           item = await task_queue.get()
           if item is None:
               break
           
           # æ¯10ä¸ªä»»åŠ¡æ£€æŸ¥ä¸€æ¬¡æ´»åŠ¨çŠ¶æ€
           if pbar.n % 10 == 0:
               UX.check_activity()
           
           try:
               item_with_source = (item[0], item[1], source)
               current_id = safe_str_convert(item[1].get(COLUMN_MAPPING["ID"], "unknown")) if len(item) > 1 else "unknown"
               original_id, result_Units = await processor.process_row(
                   item_with_source,
                   output_file_path=output_file_path,
                   failed_unit_ids=failed_unit_ids
               )
               await results_queue.put((original_id, result_Units, None))  # ä¸¤é˜¶æ®µæ¨¡å‹ï¼šåªè¿”å›è®®é¢˜å•å…ƒ
           except Exception as e:
               error_brief = str(e)[:30] + "..." if len(str(e)) > 30 else str(e)
               UX.api_failed(f"Worker-{name}", error_brief)
               original_id = safe_str_convert(item[1].get(COLUMN_MAPPING["ID"], "unknown")) if len(item) > 1 else "unknown"
               from core.utils import create_unified_record
               await results_queue.put((original_id, [create_unified_record(ProcessingStatus.API_FAILED, original_id, source, "", f"Workeré”™è¯¯: {str(e)[:100]}")], None))
           finally:
               # ä¿è¯æ— è®ºæˆåŠŸå¤±è´¥ï¼Œä»»åŠ¡å®Œæˆåéƒ½æ›´æ–°è¿›åº¦æ¡
               pbar.update(1)
               task_queue.task_done()
       
       except asyncio.CancelledError:
           break
       except Exception as e:
           error_brief = str(e)[:30] + "..." if len(str(e)) > 30 else str(e)
           UX.api_failed(f"Worker-{name}-Critical", error_brief)

async def saver_worker(results_queue, df_input, output_file_path, total_tasks=None):
    """
    [ä¸¤é˜¶æ®µæ¨¡å‹] æ‰¹é‡ä¿å­˜workerï¼š
    - ç¼“å†²å¹¶åˆ†æ‰¹ä¿å­˜è®®é¢˜å•å…ƒåˆ†æç»“æœ
    """
    analysis_buffer = []
    received_count = 0
    
    # ä»é…ç½®è¯»å–ç¼“å†²åŒºå¤§å°é˜ˆå€¼
    general_config = CONFIG.get('processing', {}).get('general', {})
    ANALYSIS_BUFFER_LIMIT = general_config.get('buffer_limit', 20)    # åˆ†æç»“æœç¼“å†²åŒº

    async def save_analysis_batch(data):
        if not data:
            return
        async with file_write_lock:
            # è°ƒç”¨ç°æœ‰çš„åˆ†æç»“æœä¿å­˜å‡½æ•°
            save_data_to_excel(data, df_input, output_file_path)

    while True:
        try:
            # ç­‰å¾…ç»“æœï¼Œä»é…ç½®è¯»å–è¶…æ—¶æ—¶é—´
            item = await asyncio.wait_for(results_queue.get(), timeout=QUEUE_TIMEOUT)
            if item is None: # æ”¶åˆ°ç»“æŸä¿¡å·
                await save_analysis_batch(analysis_buffer)
                break

            original_id, result_Units, _ = item  # ç¬¬ä¸‰ä¸ªå‚æ•°ä¸ºå…¼å®¹æ€§ä¿ç•™ï¼Œä¸¤é˜¶æ®µæ¨¡å‹ä¸­æœªä½¿ç”¨
            received_count += 1

            # æ”¶é›†åˆ†æç»“æœåˆ°ç¼“å†²åŒº
            if result_Units:
                for unit in result_Units:
                    unit[COLUMN_MAPPING["ID"]] = original_id
                analysis_buffer.extend(result_Units)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç©ºå¹¶ä¿å­˜ç¼“å†²åŒº
            if len(analysis_buffer) >= ANALYSIS_BUFFER_LIMIT:
                await save_analysis_batch(analysis_buffer)
                UX.info(f"ğŸ’¾ æ‰¹é‡ä¿å­˜: {len(analysis_buffer)} æ¡è®®é¢˜å•å…ƒåˆ†æç»“æœ")
                analysis_buffer = []

            results_queue.task_done()
            if total_tasks and received_count >= total_tasks:
                await save_analysis_batch(analysis_buffer)
                break

        except asyncio.TimeoutError:
            # è¶…æ—¶åä¿å­˜æ‰€æœ‰ç¼“å†²åŒºå†…å®¹ï¼Œé˜²æ­¢åœ¨ä»»åŠ¡é—´éš™ä¸¢å¤±æ•°æ®
            await save_analysis_batch(analysis_buffer)
            analysis_buffer = []
            if total_tasks and received_count >= total_tasks:
                break
            continue

        except asyncio.CancelledError:
            await save_analysis_batch(analysis_buffer)
            break

        except Exception as e:
            UX.err(f"ä¿å­˜çº¿ç¨‹é”™è¯¯: {e}")

def save_data_to_excel(new_Units_list, df_input, output_file_path):
   try:
       df_existing = pd.DataFrame()
       if os.path.exists(output_file_path):
           try:
               df_existing = pd.read_excel(output_file_path)
           except Exception as e:
               UX.warn(f"è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}")
       df_new_Units = pd.DataFrame(new_Units_list)

       if df_new_Units.empty:
           return

       df_existing = ensure_required_columns(df_existing)
       df_new_Units = ensure_required_columns(df_new_Units)
       
       if COLUMN_MAPPING["ID"] in df_input.columns and COLUMN_MAPPING["ID"] in df_new_Units.columns:
           df_input[COLUMN_MAPPING["ID"]] = df_input[COLUMN_MAPPING["ID"]].astype(str)
           df_new_Units[COLUMN_MAPPING["ID"]] = df_new_Units[COLUMN_MAPPING["ID"]].astype(str)
           new_original_ids = df_new_Units[COLUMN_MAPPING["ID"]].unique()
           
           # ğŸ”§ ä¿®å¤æ ¸å¿ƒé—®é¢˜ï¼šæ¸…ç†æ—§çš„"æ¦‚è¦çº§"å¤±è´¥è®°å½•
           # å½“ä¸€ç¯‡æ–‡ç« è¢«é‡æ–°å¤„ç†æˆåŠŸæ—¶ï¼Œåˆ é™¤å®ƒçš„æ—§å¤±è´¥è®°å½•
           if not df_existing.empty and 'Unit_ID' in df_existing.columns:
               # è¯†åˆ«"æ¦‚è¦çº§"è®°å½•ï¼šUnit_IDä¸åŒ…å«"-Unit-"çš„è®°å½•
               # ä¾‹å¦‚ï¼š"(3)-RU1012-API_FAILED" æˆ– "(3)-RU1012-NO_RELEVANT"
               summary_mask = (
                   df_existing[COLUMN_MAPPING["ID"]].isin(new_original_ids) & 
                   ~df_existing['Unit_ID'].str.contains('-Unit-', na=False)
               )
               
               removed_count = summary_mask.sum()
               if removed_count > 0:
                   UX.info(f"ğŸ§¹ æ¸…ç† {removed_count} æ¡æ—§çš„æ¦‚è¦çº§è®°å½•ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰")
                   df_existing = df_existing[~summary_mask].copy()

           input_base_cols = [col for col in df_input.columns if col in df_new_Units.columns and col != COLUMN_MAPPING["ID"]]
           if input_base_cols:
               df_new_Units = df_new_Units.drop(columns=input_base_cols)
           
           df_input_subset = df_input[df_input[COLUMN_MAPPING["ID"]].isin(new_original_ids)].copy()
           df_newly_merged = pd.merge(df_input_subset, df_new_Units, on=COLUMN_MAPPING["ID"], how='left')
       else:
           df_newly_merged = df_new_Units.copy()

       df_final_to_save = pd.concat([df_existing, df_newly_merged], ignore_index=True)

       if 'Unit_ID' in df_final_to_save.columns:
           df_final_to_save = df_final_to_save.drop_duplicates(subset=['Unit_ID'], keep='last')

       df_final_to_save = reorder_columns(df_final_to_save, list(df_input.columns))
       df_final_to_save.to_excel(output_file_path, index=False)
       UX.ok(f"ğŸ’¾ å·²ä¿å­˜: {os.path.basename(output_file_path)} (ç´¯è®¡ {len(df_final_to_save)} æ¡è®°å½•)")

   except Exception as e:
       UX.err(f"Excelä¿å­˜å¤±è´¥: {e}")

async def main_async():
   UX.start_run()
   UX.phase("é•¿æ–‡æœ¬åˆ†æå™¨å¯åŠ¨")
   UX.info(f"ä¿¡åº¦æ£€éªŒæ¨¡å¼: {'å¼€å¯' if RELIABILITY_TEST_MODE else 'å…³é—­'}")
   
   # åˆ›å»ºè‡ªåŠ¨é‡è¯•ç®¡ç†å™¨
   retry_manager = create_auto_retry_manager(CONFIG, OUTPUT_PATH)
   
   # ä½¿ç”¨è‡ªåŠ¨é‡è¯•ç®¡ç†å™¨æ‰§è¡Œä¸»å¤„ç†é€»è¾‘
   await retry_manager.run_with_auto_retry(main_processing_logic)


async def activity_monitor():
    """æ´»åŠ¨ç›‘æ§ä»»åŠ¡ï¼Œæ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡"""
    while True:
        await asyncio.sleep(300)  # 5åˆ†é’Ÿ
        UX.check_activity()

async def main_processing_logic():
    """ä¸»å¤„ç†é€»è¾‘ï¼ˆä»åŸmain_asyncå‡½æ•°ç§»åŠ¨è€Œæ¥ï¼‰"""
    
    # å¯åŠ¨æ´»åŠ¨ç›‘æ§ä»»åŠ¡
    monitor_task = asyncio.create_task(activity_monitor())
    
    try:
        # éªŒè¯ä¸¤é˜¶æ®µæ¨¡å‹é…ç½®
        required_keys = ["media_text_extraction", "media_text_analysis"]
        missing = [k for k in required_keys if k not in MODEL_POOLS]
        if missing:
            raise ValueError(f"ç¼ºå°‘ä¸¤é˜¶æ®µæ¨¡å‹æ± é…ç½®: {missing}")

        # æ£€æŸ¥APIå¯†é’¥
        api_keys = CONFIG['api']['credentials']['keys']
        if not api_keys or not api_keys[0]:
            UX.err("æœªæä¾›æœ‰æ•ˆAPIå¯†é’¥")
            return

        # ç¡®å®šè¾“å…¥æ–‡ä»¶
        if os.path.isdir(INPUT_PATH):
            files_to_process = glob.glob(os.path.join(INPUT_PATH, '*.xlsx'))
            is_folder_mode = True
            os.makedirs(OUTPUT_PATH, exist_ok=True)
        elif os.path.isfile(INPUT_PATH):
            files_to_process = [INPUT_PATH]
            is_folder_mode = False
        else:
            UX.err("è¾“å…¥è·¯å¾„æ— æ•ˆ")
            return

        async with aiohttp.ClientSession() as session:
            api_service = APIService(session, CONFIG)
            # åˆ›å»ºåª’ä½“æ–‡æœ¬å¤„ç†å™¨
            processor = MediaTextProcessor(api_service, CONFIG, prompts)

            for file_path in files_to_process:
                file_basename = os.path.basename(file_path)
                
                # æ–‡ä»¶å¤„ç†å¼€å§‹æ ‡è®°
                UX.phase(f"ğŸ“ å¤„ç†æ–‡ä»¶: {file_basename}")
                UX.info("="*100)

                # è¯†åˆ«ä¿¡æº
                source = identify_source(file_basename)
                UX.info(f"ğŸŒ è¯†åˆ«ä¿¡æº: {source}")

                output_file_path = os.path.join(OUTPUT_PATH, f"(ä¸èƒ½åˆ )analyzed_{file_basename}") if is_folder_mode else OUTPUT_PATH

                # è¯»å–è¾“å…¥æ–‡ä»¶
                try:
                    df_input = pd.read_excel(file_path)
                    if COLUMN_MAPPING.get("MEDIA_TEXT", "text") not in df_input.columns:
                        UX.err("æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„æ–‡æœ¬åˆ—")
                        continue
                except Exception as e:
                    UX.err(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                    continue

                # ä¸¤é˜¶æ®µæ¨¡å‹ï¼šç›´æ¥å¤„ç†è®®é¢˜å•å…ƒï¼Œæ— éœ€ä¸­é—´æ•°æ®åº“

                # ğŸ” æ„å»ºæ–­ç‚¹ç»­ä¼ è®¡åˆ’
                UX.resume_plan(file_basename)
                total_input_articles = len(set(df_input[COLUMN_MAPPING["ID"]].astype(str)))
                never_processed_ids, failed_unit_ids = build_resume_plan(
                    output_file_path, df_input, COLUMN_MAPPING["ID"]
                )

                ids_to_process = set(never_processed_ids) | set(failed_unit_ids.keys())
                failed_units_count = sum(len(v) for v in failed_unit_ids.values())

                UX.info(f"ğŸ“„ æ€»æ–‡ç« æ•°: {total_input_articles}")
                UX.info(f"ğŸ†• ä»æœªå¤„ç†: {len(never_processed_ids)} ç¯‡")
                UX.info(f"ğŸ”„ å¤±è´¥é‡è¯•: {len(failed_unit_ids)} ç¯‡ ({failed_units_count} ä¸ªå•å…ƒ)")
                UX.info(f"ğŸ¯ æœ¬æ¬¡å¤„ç†: {len(ids_to_process)} ç¯‡ | è·³è¿‡: {total_input_articles - len(ids_to_process)} ç¯‡")
                UX.resume_end()

                df_to_process = df_input[df_input[COLUMN_MAPPING["ID"]].astype(str).isin(ids_to_process)].copy()

                if len(df_to_process) > 0:
                    UX.phase(f"å¼€å§‹å¤„ç† {len(df_to_process)} ç¯‡æ–‡ç« ")
                    UX.info(f"ğŸš€ ä¸¤é˜¶æ®µæ¨¡å‹å¤„ç†: ç¬¬ä¸€é˜¶æ®µ(å•å…ƒæå–) â†’ ç¬¬äºŒé˜¶æ®µ(æ·±åº¦åˆ†æ)")

                    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
                    task_queue = asyncio.Queue()
                    results_queue = asyncio.Queue()
                    total_tasks = len(df_to_process)

                    # æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—
                    for item in df_to_process.iterrows():
                        await task_queue.put(item)

                    # åˆ›å»ºè¿›åº¦æ¡
                    pbar = aio_tqdm(total=total_tasks, desc=f"ğŸ”„ å¤„ç†æ–‡ç«  ({file_basename})", 
                                   bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]")

                    # åˆ›å»ºä¿å­˜ä»»åŠ¡ 
                    saver_task = asyncio.create_task(
                        saver_worker(results_queue, df_input, output_file_path, total_tasks)
                    )

                    # åˆ›å»ºå·¥ä½œä»»åŠ¡
                    worker_tasks = [
                        asyncio.create_task(
                            api_worker(
                                f'worker-{i}',
                                task_queue,
                                results_queue,
                                api_service,
                                source,
                                pbar,
                                processor,
                                output_file_path=output_file_path,
                                failed_unit_ids=failed_unit_ids
                            )
                        )
                        for i in range(MAX_CONCURRENT_REQUESTS)
                    ]

                    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡éƒ½è¢«workerå¤„ç†å®Œæ¯•
                    await task_queue.join()
                    
                    # å‘æ‰€æœ‰workerå‘é€ç»“æŸä¿¡å·
                    for _ in range(MAX_CONCURRENT_REQUESTS):
                        await task_queue.put(None)
                    
                    # ç­‰å¾…æ‰€æœ‰workeræ­£ç¡®å…³é—­
                    await asyncio.gather(*worker_tasks)
                    
                    # å‘saverå‘é€ç»“æŸä¿¡å·å¹¶ç­‰å¾…ä¿å­˜å®Œæˆ
                    await results_queue.put(None)
                    await saver_task
                    
                    pbar.close()
                
                elif len(ids_to_process) == 0:
                    UX.ok(f"ğŸ‰ æ–‡ä»¶ {file_basename}: æ‰€æœ‰æ¡ç›®å·²å®Œç¾å¤„ç†å®Œæ¯•ï¼")
                    continue
           
                # ğŸ‰ å¤„ç†å®Œæˆæ€»ç»“
                try:
                    df_final_check = pd.read_excel(output_file_path)
                    if not df_final_check.empty and 'processing_status' in df_final_check.columns:
                        final_success = (df_final_check['processing_status'] == ProcessingStatus.SUCCESS).sum()
                        final_no_relevant = (df_final_check['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                        final_failed = (df_final_check['processing_status'] == ProcessingStatus.API_FAILED).sum()
                        final_total_units = len(df_final_check)
                        
                        # è®¡ç®—æ–‡ç« çº§åˆ«å®Œæˆåº¦
                        final_processed_ids, final_failed_ids = get_processing_state(df_final_check, COLUMN_MAPPING["ID"])
                        final_completed_articles = len(final_processed_ids - final_failed_ids)
                        final_completion_rate = (final_completed_articles / max(1, total_input_articles)) * 100
                        
                        # æ–‡ä»¶å¤„ç†ç»“æœæ€»ç»“
                        UX.phase(f"ğŸ“Š æ–‡ä»¶ {file_basename} å¤„ç†å®Œæˆæ€»ç»“")
                        UX.info("-"*80)
                        UX.ok(f"ğŸ“Š æ–‡ç« å®Œæˆåº¦: {final_completed_articles}/{total_input_articles} ({final_completion_rate:.1f}%)")
                        UX.ok(f"ğŸ¯ è®®é¢˜å•å…ƒç»Ÿè®¡: âœ…æˆåŠŸ {final_success}æ¡ | ğŸ“æ— ç›¸å…³ {final_no_relevant}æ¡ | âŒå¤±è´¥ {final_failed}æ¡")
                        
                        if final_failed_ids:
                            UX.warn(f"âš ï¸  ä»æœ‰ {len(final_failed_ids)} ç¯‡æ–‡ç« å¤„ç†å¤±è´¥ï¼Œå¯å†æ¬¡è¿è¡Œè¿›è¡Œæ™ºèƒ½é‡è¯•")
                        else:
                            UX.ok(f"âœ¨ å®Œç¾ï¼æ‰€æœ‰æ–‡ç« å‡å·²æˆåŠŸå¤„ç†")
                        UX.info("-"*80)
                    else:
                        UX.ok(f"âœ… æ–‡ä»¶ {file_basename} å¤„ç†å®Œæˆ")
                except Exception:
                    UX.ok(f"âœ… æ–‡ä»¶ {file_basename} å¤„ç†å®Œæˆ")

        # ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶
        if RELIABILITY_TEST_MODE:
            UX.phase("ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶")

            # åˆå¹¶æ‰€æœ‰ç»“æœæ–‡ä»¶
            final_results_files = glob.glob(os.path.join(OUTPUT_PATH, "*analyzed_*.xlsx"))
            if final_results_files:
                all_results = []
                for file in final_results_files:
                    df = pd.read_excel(file)
                    df = ensure_required_columns(df)
                    source = identify_source(os.path.basename(file))
                    df['Source'] = source
                    all_results.append(df)

                if all_results:
                    df_all_results = pd.concat(all_results, ignore_index=True)
                    df_all_results = ensure_required_columns(df_all_results)
                    df_all_results = reorder_columns(df_all_results, [])

                    combined_results_path = os.path.join(OUTPUT_PATH, 'åª’ä½“_æœ€ç»ˆåˆ†ææ•°æ®åº“.xlsx')
                    df_all_results.to_excel(combined_results_path, index=False)

                    if 'processing_status' in df_all_results.columns:
                        success_count = (df_all_results['processing_status'] == ProcessingStatus.SUCCESS).sum()
                        no_relevant_count = (df_all_results['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                        failed_count = (df_all_results['processing_status'] == ProcessingStatus.API_FAILED).sum()
                        UX.ok(f"ğŸ“Š æœ€ç»ˆæ•°æ®åº“å·²ä¿å­˜: {combined_results_path}")
                        UX.info(f"   ğŸ“ˆ æ€»è®°å½•: {len(df_all_results)} | âœ…æˆåŠŸ: {success_count} | ğŸ“æ— ç›¸å…³: {no_relevant_count} | âŒå¤±è´¥: {failed_count}")

                    # ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶
                    try:
                        UX.info("ğŸ” å¼€å§‹ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶...")
                        from core.reliability import create_reliability_test_module
                        
                        reliability_module = create_reliability_test_module(
                            input_path=INPUT_PATH,
                            output_path=OUTPUT_PATH,
                            sampling_config=RELIABILITY_SAMPLING_CONFIG,
                            id_column=COLUMN_MAPPING.get("ID", "åºå·"),
                            text_column=COLUMN_MAPPING.get("MEDIA_TEXT", "text"),
                            random_seed=CONFIG.get('project', {}).get('random_seed', 42)
                        )
                        
                        reliability_module.generate_reliability_files()
                        UX.ok("âœ… ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆå®Œæˆ")
                    except Exception as e:
                        UX.err(f"âŒ ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
                        UX.info("â­ï¸  å°†ç»§ç»­å…¶ä»–ä»»åŠ¡...")

                else:
                    UX.warn("åˆå¹¶ç»“æœæ–‡ä»¶å¤±è´¥")
            else:
                UX.warn("æœªæ‰¾åˆ°ä»»ä½•analyzedç»“æœæ–‡ä»¶")

    finally:
        # å–æ¶ˆæ´»åŠ¨ç›‘æ§ä»»åŠ¡
        monitor_task.cancel()
        try:
            await monitor_task
        except asyncio.CancelledError:
            pass
    
    UX.phase("æ‰€æœ‰ä»»åŠ¡å®Œæˆ")

# æ·»åŠ ç¼ºå¤±çš„è¾…åŠ©å‡½æ•°
def ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ç¡®ä¿ç»“æœè¡¨å…·å¤‡ç»Ÿä¸€åˆ—ï¼šç¼ºå¤±åˆ™ä»¥ç©ºå€¼è¡¥é½ï¼Œä¸ç§»é™¤å·²æœ‰åˆ—ã€‚"""
    if df is None or df.empty:
        return pd.DataFrame(columns=REQUIRED_OUTPUT_COLUMNS)
    
    for col in REQUIRED_OUTPUT_COLUMNS:
        if col not in df.columns:
            df[col] = ""
    return df

def reorder_columns(df: pd.DataFrame, df_input_columns: list) -> pd.DataFrame:
    """å°†åˆ—é¡ºåºæ•´ç†ä¸ºï¼šè¾“å…¥è¡¨åŸæœ‰åˆ—åœ¨å‰ + REQUIRED_OUTPUT_COLUMNSï¼ˆå»é‡ä¿åºï¼‰"""
    if df is None or df.empty:
        return df
    
    preferred = list(dict.fromkeys(list(df_input_columns) + REQUIRED_OUTPUT_COLUMNS))
    # ä¿ç•™è¡¨å†…å·²æœ‰çš„åˆ—ï¼Œä¸”æŒ‰ç…§ preferred é¡ºåºé‡æ’
    ordered_existing = [c for c in preferred if c in df.columns]
    # è¿½åŠ ä»»ä½•æœªåœ¨ preferred ä¸­ä½†å­˜åœ¨äº df çš„åˆ—ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
    tail = [c for c in df.columns if c not in preferred]
    return df.reindex(columns=ordered_existing + tail)

def build_resume_plan(output_file_path: str, df_input: pd.DataFrame, id_col: str):
    """æ„å»ºä¸¤é˜¶æ®µæ¨¡å‹çš„æ–­ç‚¹ç»­ä¼ è®¡åˆ’"""
    all_input_ids = set(df_input[id_col].astype(str)) if (df_input is not None and id_col in df_input.columns) else set()

    never_processed_ids = set(all_input_ids)
    failed_unit_ids = {}

    if os.path.exists(output_file_path):
        try:
            df_existing = pd.read_excel(output_file_path)
            if not df_existing.empty and id_col in df_existing.columns:
                df_existing[id_col] = df_existing[id_col].astype(str)
                processed_article_ids = set(df_existing[id_col].unique())
                never_processed_ids = all_input_ids - processed_article_ids

                if 'processing_status' in df_existing.columns:
                    stage1_failed_records = df_existing[df_existing['processing_status'] == ProcessingStatus.STAGE_1_FAILED]
                    stage2_failed_records = df_existing[df_existing['processing_status'] == ProcessingStatus.STAGE_2_FAILED]

                    for _, record in stage1_failed_records.iterrows():
                        article_id = str(record.get(id_col, '')).strip()
                        if article_id:
                            never_processed_ids.add(article_id)

                    for _, record in stage2_failed_records.iterrows():
                        article_id = str(record.get(id_col, '')).strip()
                        unit_id = str(record.get('Unit_ID', '')).strip()
                        if article_id and unit_id:
                            s = failed_unit_ids.setdefault(article_id, set())
                            s.add(unit_id)
        except Exception as e:
            UX.warn(f"è¯»å–å·²å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")

    return never_processed_ids, failed_unit_ids

# ç¨‹åºå…¥å£
if __name__ == "__main__":
    import sys
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main_async())