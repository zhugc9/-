# -*- coding: utf-8 -*-
"""ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆæ¨¡å—"""

import os
import glob
import pandas as pd
from typing import Tuple, Optional
import re
import json
try:
    from rapidfuzz import fuzz, process, utils
except ImportError:
    # é™çº§åˆ°thefuzzä½œä¸ºå¤‡é€‰
    from thefuzz import fuzz, process
    from thefuzz import utils

# ä»utilsæ¨¡å—å¯¼å…¥ProcessingStatus
from .utils import ProcessingStatus

class ReliabilityTestModule:
    
    def __init__(self, input_path: str, output_path: str, sampling_config: dict, 
                 id_column: str = "åºå·", text_column: str = "text", random_seed: int = 42):
        self.input_path = input_path
        self.output_path = output_path
        self.sampling_config = sampling_config
        self.id_column = id_column
        self.text_column = text_column
        self.random_seed = random_seed
        self._locale_cache = {}
        
    def locate_unit(self, unit_text: str, full_text: str) -> Optional[Tuple[int, int]]:
        """
        ã€SOTA æœ€ç»ˆç‰ˆã€‘æ–‡æœ¬å®šä½ç®—æ³•
        ä½¿ç”¨ fuzz.partial_ratio_alignment å®ç°ç²¾ç¡®ã€é«˜æ•ˆçš„æ¨¡ç³Šå®šä½
        """
        if not unit_text or not full_text:
            return None
            
        # ç¬¬ä¸€çº§ï¼šç²¾ç¡®åŒ¹é… (æœ€å¿«ï¼Œä¿æŒä¸å˜)
        try:
            start_idx = full_text.index(unit_text)
            return (start_idx, start_idx + len(unit_text))
        except ValueError:
            pass  # ç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œè¿›å…¥ä¸‹ä¸€çº§
        
        # ç¬¬äºŒçº§ï¼šSOTA æ¨¡ç³Šå®šä½ (ä½¿ç”¨ alignment)
        # å‡†å¤‡æ–‡æœ¬ï¼šå¯¹äºå­—æ¯è¯­è¨€ï¼Œç»Ÿä¸€å°å†™ï¼›å¯¹äºCJKï¼Œå¯ä»¥ä¸å¤„ç†ï¼Œä½†ä¸ºäº†ç»Ÿä¸€ï¼Œè¿™é‡Œä¹Ÿå¤„ç†
        # utils.default_process ä¼šæ™ºèƒ½å¤„ç†ï¼Œæ¯” .lower() æ›´å¥å£®
        processed_unit = utils.default_process(unit_text)
        processed_full = utils.default_process(full_text)
        
        # æ ¸å¿ƒï¼šä½¿ç”¨ partial_ratio_alignment ç›´æ¥è·å–ä½ç½®å’Œåˆ†æ•°
        # score_cutoff çš„å€¼å¯ä»¥æ ¹æ®ç»éªŒè°ƒæ•´ï¼Œ75-85 é€šå¸¸æ˜¯æ¯”è¾ƒåˆç†çš„èŒƒå›´
        alignment = fuzz.partial_ratio_alignment(
            processed_unit, 
            processed_full, 
            score_cutoff=85
        )
        
        if alignment:
            # alignment å¯¹è±¡ç›´æ¥åŒ…å«äº†æˆ‘ä»¬éœ€è¦çš„åœ¨ full_text ä¸­çš„åæ ‡
            # alignment.dest_start, alignment.dest_end
            return (alignment.dest_start, alignment.dest_end)
        
        # å¦‚æœä¸¤çº§éƒ½å¤±è´¥ï¼Œåˆ™è¿”å› None
        return None
    
    
    def generate_positive_test_file(self, df_all_results: pd.DataFrame, df_all_input: pd.DataFrame) -> None:
        print("ğŸ¯ å¼€å§‹ç”Ÿæˆæ­£å‘æ£€éªŒæ–‡ä»¶ï¼ˆé«˜äº®ç‰ˆï¼‰...")
        sampled_units = []
        
        for source, config in self.sampling_config.items():
            precision_count = config.get('precision', 0)
            if precision_count <= 0:
                continue
                
            source_results = df_all_results[
                (df_all_results.get('Source', '') == source) & 
                (df_all_results.get('processing_status', '') == ProcessingStatus.SUCCESS)
            ]
            if len(source_results) == 0:
                continue
            sample_size = min(precision_count, len(source_results))
            sampled = source_results.sample(n=sample_size, random_state=self.random_seed)
            sampled_units.append(sampled)
        
        if not sampled_units:
            print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨äºæ­£å‘æ£€éªŒçš„æ ·æœ¬")
            return
            
        df_sampled = pd.concat(sampled_units, ignore_index=True)
        results = []
        for _, row in df_sampled.iterrows():
            unit_text = str(row.get('Unit_Text', ''))
            article_id = str(row.get(self.id_column, ''))
            
            full_text = ""
            matching_input = df_all_input[df_all_input[self.id_column].astype(str) == article_id]
            if not matching_input.empty:
                full_text = str(matching_input.iloc[0][self.text_column])
            
            location = self.locate_unit(unit_text, full_text)
            if location and full_text:
                start, end = location
                highlight_head = "\nã€ğŸŒŸé«˜äº®æ®µè½å¼€å§‹ğŸŒŸã€‘\n"
                highlight_tail = "\nã€ğŸŒŸé«˜äº®æ®µè½ç»“æŸğŸŒŸã€‘\n"
                highlighted_text = (
                    full_text[:start] +
                    highlight_head +
                    full_text[start:end] +
                    highlight_tail +
                    full_text[end:]
                )
            else:
                highlighted_text = f"ã€å®šä½å¤±è´¥ã€‘{unit_text}"
            # æ­¥éª¤ä¸€ï¼šå…ˆå°†åŸå§‹åˆ†æç»“æœçš„æ‰€æœ‰ç›¸å…³åˆ—å¤åˆ¶è¿‡æ¥
            # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰åˆ†æåˆ—çš„åˆ—è¡¨
            analysis_columns = [
                "Incident", "Frame_SolutionRecommendation", "Frame_ResponsibilityAttribution",
                "Frame_CausalExplanation", "Frame_MoralEvaluation", "Frame_ProblemDefinition",
                "Frame_ActionStatement", "Valence", "Evidence_Type", "Attribution_Level",
                "Temporal_Focus", "Primary_Actor_Type", "Geographic_Scope",
                "Relationship_Model_Definition", "Discourse_Type"
            ]

            # ä»åŸå§‹è¡Œ(row)ä¸­æå–æ‰€æœ‰åˆ†æåˆ—çš„æ•°æ®ï¼Œå­˜å…¥ä¸€ä¸ªæ–°å­—å…¸
            result_record = {}
            for col in analysis_columns:
                if col in row:
                    result_record[col] = row[col]

            # æ­¥éª¤äºŒï¼šå†æ·»åŠ å’Œæ›´æ–°æ£€éªŒæ–‡ä»¶ç‰¹æœ‰çš„åˆ—
            result_record.update({
                'Unit_ID': row.get('Unit_ID', ''),
                'Article_ID': article_id,
                'Source': row.get('Source', ''),
                'Unit_Text': unit_text,
                'Highlighted_Full_Text': highlighted_text,
                'Inspector_Is_Relevant': '',          # æ£€éªŒå‘˜å¡«å†™çš„åˆ—
                'Inspector_Boundary_Quality': '',     # æ£€éªŒå‘˜å¡«å†™çš„åˆ—
                'Inspector_Comments': ''              # æ£€éªŒå‘˜å¡«å†™çš„åˆ—
            })

            # ã€æ–°å¢é€»è¾‘å¼€å§‹ã€‘
            # å†æ¬¡å®šä¹‰éœ€è¦è¢«æ£€éªŒçš„AIåˆ†æåˆ—
            analysis_columns_to_inspect = [
                "Incident", "Valence", "Evidence_Type", "Attribution_Level",
                "Temporal_Focus", "Primary_Actor_Type", "Geographic_Scope",
                "Relationship_Model_Definition", "Discourse_Type",
                "Frame_SolutionRecommendation", "Frame_ResponsibilityAttribution",
                "Frame_CausalExplanation", "Frame_MoralEvaluation", "Frame_ProblemDefinition",
                "Frame_ActionStatement"
            ]

            # ä¸ºæ¯ä¸€ä¸ªéœ€è¦æ£€éªŒçš„åˆ—ï¼ŒåŠ¨æ€æ·»åŠ ä¸€ä¸ªå¯¹åº”çš„ `Inspector_` ç©ºç™½åˆ—
            inspector_fields = {}
            for col in analysis_columns_to_inspect:
                inspector_fields[f"Inspector_{col}"] = ''

            # å°†ç©ºç™½æ£€éªŒåˆ—æ›´æ–°åˆ°è®°å½•ä¸­
            result_record.update(inspector_fields)
            # ã€æ–°å¢é€»è¾‘ç»“æŸã€‘

            # å°†æ­¤æ›´æ–°åçš„result_recordæ·»åŠ åˆ°resultsåˆ—è¡¨ä¸­
            results.append(result_record)
        
        # ä¿å­˜æ–‡ä»¶
        if results:
            df_output = pd.DataFrame(results)

            # ã€æ–°å¢é€»è¾‘å¼€å§‹ã€‘
            # å®šä¹‰æœ€ç»ˆçš„åˆ—é¡ºåº
            final_column_order = [
                'Unit_ID', 'Article_ID', 'Source', 'Unit_Text', 'Highlighted_Full_Text'
            ]

            # å®šä¹‰éœ€è¦è¢«æ£€éªŒçš„AIåˆ†æåˆ—ï¼ˆä¸ä¸Šé¢ä¿æŒä¸€è‡´ï¼‰
            analysis_columns_to_inspect = [
                "Incident", "Valence", "Evidence_Type", "Attribution_Level",
                "Temporal_Focus", "Primary_Actor_Type", "Geographic_Scope",
                "Relationship_Model_Definition", "Discourse_Type",
                "Frame_SolutionRecommendation", "Frame_ResponsibilityAttribution",
                "Frame_CausalExplanation", "Frame_MoralEvaluation", "Frame_ProblemDefinition",
                "Frame_ActionStatement"
            ]

            # åŠ¨æ€æ„å»º"AIåˆ†æåˆ—"å’Œ"æ£€éªŒåˆ—"å¹¶æ’çš„é¡ºåº
            for col in analysis_columns_to_inspect:
                if col in df_output.columns:
                    final_column_order.append(col)
                    final_column_order.append(f"Inspector_{col}")

            # æ·»åŠ é—ç•™çš„æ£€éªŒå‘˜è¯„è®ºåˆ—
            final_column_order.extend(['Inspector_Is_Relevant', 'Inspector_Boundary_Quality', 'Inspector_Comments'])

            # è¿‡æ»¤æ‰ä¸å­˜åœ¨äºDataFrameä¸­çš„åˆ—åï¼Œå¹¶é‡æ’
            existing_columns_ordered = [col for col in final_column_order if col in df_output.columns]
            df_output = df_output.reindex(columns=existing_columns_ordered)
            # ã€æ–°å¢é€»è¾‘ç»“æŸã€‘

            zh_positive_path = os.path.join(self.output_path, 'æ­£å‘æ£€éªŒåŠæ¡†æ¶ç»´åº¦æ£€éªŒ_é«˜äº®ç‰ˆ.xlsx')
            ru_positive_path = os.path.join(self.output_path, 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°_Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ_Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°(Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ_Ğ¸_Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹).xlsx')
            self._save_bilingual(df_output, zh_positive_path, ru_positive_path)
            print(f"æ­£å‘æ£€éªŒåŠæ¡†æ¶ç»´åº¦æ£€éªŒæ–‡ä»¶å·²ç”Ÿæˆ: {zh_positive_path}")
            print(f"   æ ·æœ¬æ•°é‡: {len(results)}")
        else:
            print("æ²¡æœ‰ç”Ÿæˆæ­£å‘æ£€éªŒæ ·æœ¬")
    
    def generate_negative_test_file(self, df_all_results: pd.DataFrame, df_all_input: pd.DataFrame) -> None:
        print("ğŸ” å¼€å§‹ç”Ÿæˆåå‘æ£€éªŒæ–‡ä»¶ï¼ˆæŒ–é™¤ç‰ˆï¼‰...")
        
        # æŒ‰æ¥æºæŠ½æ ·æ–‡ç« 
        sampled_articles = []
        for source, config in self.sampling_config.items():
            recall_count = config.get('recall', 0)
            if recall_count <= 0:
                continue
            
            source_input = df_all_input[df_all_input.get('Source', '') == source]
            if len(source_input) == 0:
                continue
                
            sample_size = min(recall_count, len(source_input))
            sampled = source_input.sample(n=sample_size, random_state=self.random_seed)
            sampled_articles.append(sampled)
        
        if not sampled_articles:
            print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨äºåå‘æ£€éªŒçš„æ–‡ç« ")
            return
            
        df_sampled_articles = pd.concat(sampled_articles, ignore_index=True)
        results = []
        
        for _, article_row in df_sampled_articles.iterrows():
            article_id_str = str(article_row[self.id_column])
            full_text = str(article_row[self.text_column])
            
            # è·å–è¯¥æ–‡ç« çš„æ‰€æœ‰è®®é¢˜å•å…ƒ
            article_units = df_all_results[df_all_results[self.id_column].astype(str) == article_id_str]
            unit_texts = article_units['Unit_Text'].dropna().tolist() if not article_units.empty else []
            
            # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„ä½ç½®ä¿¡æ¯
            positions = []
            for unit_text in unit_texts:
                location = self.locate_unit(str(unit_text), full_text)
                if location:
                    positions.append(location)
            
            # æŒ‰start_indexé™åºæ’åºï¼ˆä»å¤§åˆ°å°ï¼‰
            positions.sort(key=lambda x: x[0], reverse=True)
            
            # åˆ›å»ºfull_textçš„å¯å˜å‰¯æœ¬å¹¶æ‰§è¡Œæ›¿æ¢
            modified_text = full_text
            for start, end in positions:
                modified_text = modified_text[:start] + "ã€å·²æå–ã€‘" + modified_text[end:]
            
            # æ£€æŸ¥æŒ–é™¤åæ˜¯å¦è¿˜æœ‰å‰©ä½™çš„æœ‰æ•ˆæ–‡æœ¬
            check_text = modified_text.replace("ã€å·²æå–ã€‘", "").strip()
            
            # åªæœ‰å½“å‰©ä½™æ–‡æœ¬ä¸ä¸ºç©ºæ—¶ï¼Œæ‰å°†è¯¥è®°å½•æ·»åŠ åˆ°ç»“æœä¸­
            if check_text:
                result_record = {
                    'Article_ID': article_id_str,
                    'Source': article_row.get('Source', ''),
                    'Extracted_Units_Count': len(positions),
                    'Remaining_Text': modified_text,
                    'Inspector_Has_Missed_Content': '',
                    'Inspector_Missed_Content_Type': '',
                    'Inspector_Comments': ''
                }
                results.append(result_record)
        
        # ä¿å­˜æ–‡ä»¶
        if results:
            df_output = pd.DataFrame(results)
            zh_negative_path = os.path.join(self.output_path, 'åå‘æ£€éªŒ_æŒ–é™¤ç‰ˆ.xlsx')
            ru_negative_path = os.path.join(self.output_path, 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°_Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹_Ñ‚ĞµĞºÑÑ‚(Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ°).xlsx')
            self._save_bilingual(df_output, zh_negative_path, ru_negative_path)
            print(f"åå‘æ£€éªŒæ–‡ä»¶å·²ç”Ÿæˆ: {zh_negative_path}")
            print(f"   æ–‡ç« æ•°é‡: {len(results)}")
        else:
            print("æ²¡æœ‰ç”Ÿæˆåå‘æ£€éªŒæ ·æœ¬")
    
    def generate_reliability_files(self) -> None:
        print("å¯åŠ¨ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆæ¨¡å—ï¼ˆSOTAæµå¼å¤„ç†ç‰ˆï¼‰")
        print("=" * 60)
        
        # ã€å†…å­˜ä¼˜åŒ–ã€‘ç¬¬ä¸€æ­¥ï¼šåªè·å–æ–‡ä»¶è·¯å¾„ï¼Œä¸åŠ è½½å†…å®¹
        analyzed_pattern = os.path.join(self.output_path, "*analyzed_*.xlsx")
        analyzed_files = glob.glob(analyzed_pattern)
        
        if not analyzed_files:
            print(f"æœªæ‰¾åˆ°åˆ†æç»“æœæ–‡ä»¶ï¼Œæœç´¢æ¨¡å¼: {analyzed_pattern}")
            return
            
        print(f"æ‰¾åˆ° {len(analyzed_files)} ä¸ªåˆ†æç»“æœæ–‡ä»¶:")
        for file in analyzed_files:
            print(f"   - {os.path.basename(file)}")
        
        input_files = []
        if os.path.isdir(self.input_path):
            input_pattern = os.path.join(self.input_path, "*.xlsx")
            input_files = glob.glob(input_pattern)
        elif os.path.isfile(self.input_path) and self.input_path.lower().endswith('.xlsx'):
            input_files = [self.input_path]
        else:
            print(f"è¾“å…¥è·¯å¾„æ— æ•ˆ: {self.input_path}")
        
        temp_input_dir = os.path.join(self.output_path, "_reliability_inputs")
        if os.path.isdir(temp_input_dir):
            temp_files = glob.glob(os.path.join(temp_input_dir, "*.xlsx"))
            input_files.extend(temp_files)

        if not input_files:
            print(f"æœªæ‰¾åˆ°åŸå§‹è¾“å…¥æ–‡ä»¶ï¼Œæœç´¢è·¯å¾„: {self.input_path}")
            return
            
        print(f"æ‰¾åˆ° {len(input_files)} ä¸ªåŸå§‹è¾“å…¥æ–‡ä»¶:")
        for file in input_files:
            print(f"   - {os.path.basename(file)}")
        
        # ã€å†…å­˜ä¼˜åŒ–ã€‘ç¬¬äºŒæ­¥ï¼šé€æ–‡ä»¶æµå¼å¤„ç†
        print(f"\né‡‡ç”¨æµå¼å¤„ç†æ¨¡å¼ï¼Œé€æ–‡ä»¶å¤„ç†ä»¥èŠ‚çœå†…å­˜")
        os.makedirs(self.output_path, exist_ok=True)
        
        # æ”¶é›†æ ·æœ¬æ•°æ®çš„å®¹å™¨ï¼ˆåªä¿å­˜æŠ½æ ·ç»“æœï¼Œä¸æ˜¯å…¨é‡æ•°æ®ï¼‰
        positive_samples = []
        negative_samples = []
        
        # ä¸»å¾ªç¯ï¼šé€ä¸ªå¤„ç†è¾“å…¥æ–‡ä»¶
        for input_file in input_files:
            print(f"\næ­£åœ¨å¤„ç†: {os.path.basename(input_file)}")
            
            try:
                # åªåŠ è½½å½“å‰ä¸€ä¸ªè¾“å…¥æ–‡ä»¶
                df_input_single = pd.read_excel(input_file)
                if not self._validate_input_columns(df_input_single):
                    print(f"   è·³è¿‡æ–‡ä»¶ï¼ˆç¼ºå°‘å¿…è¦åˆ—ï¼‰: {os.path.basename(input_file)}")
                    continue
                
                input_source = self._identify_source(os.path.basename(input_file))
                df_input_single['Source'] = input_source
                print(f"   è¾“å…¥æ–‡ä»¶åŠ è½½æˆåŠŸ: {len(df_input_single)} æ¡è®°å½•")
                
                # æ‰¾åˆ°å¯¹åº”çš„åˆ†æç»“æœæ–‡ä»¶
                corresponding_analyzed = None
                for analyzed_file in analyzed_files:
                    if self._files_match(input_file, analyzed_file):
                        corresponding_analyzed = analyzed_file
                        break
                
                if not corresponding_analyzed:
                    print(f"   æœªæ‰¾åˆ°å¯¹åº”çš„åˆ†æç»“æœæ–‡ä»¶ï¼Œè·³è¿‡")
                    continue
                
                # åªåŠ è½½å¯¹åº”çš„åˆ†æç»“æœæ–‡ä»¶
                df_analyzed_single = pd.read_excel(corresponding_analyzed)
                analyzed_source = self._identify_source(os.path.basename(corresponding_analyzed))
                df_analyzed_single['Source'] = analyzed_source
                print(f"   åˆ†æç»“æœåŠ è½½æˆåŠŸ: {len(df_analyzed_single)} æ¡è®°å½•")
                
                # ç”Ÿæˆå½“å‰æ–‡ä»¶çš„æ ·æœ¬ï¼ˆè¿”å›æŠ½æ ·æ•°æ®ï¼Œä¸ç›´æ¥å†™æ–‡ä»¶ï¼‰
                pos_samples = self._generate_positive_samples(df_analyzed_single, df_input_single)
                neg_samples = self._generate_negative_samples(df_analyzed_single, df_input_single)
                
                positive_samples.extend(pos_samples)
                negative_samples.extend(neg_samples)
                print(f"   æœ¬æ–‡ä»¶è´¡çŒ®: æ­£å‘æ ·æœ¬{len(pos_samples)}ä¸ª, åå‘æ ·æœ¬{len(neg_samples)}ä¸ª")
                
            except Exception as e:
                print(f"   å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
                continue
        
        # ã€å†…å­˜ä¼˜åŒ–ã€‘ç¬¬ä¸‰æ­¥ï¼šæœ€åç»Ÿä¸€ä¿å­˜æŠ½æ ·ç»“æœ
        print(f"\næ­£åœ¨ä¿å­˜æœ€ç»ˆçš„ä¿¡åº¦æ£€éªŒæ–‡ä»¶...")
        
        try:
            if positive_samples:
                self._save_positive_test_file(positive_samples)
                print(f"æ­£å‘æ£€éªŒæ–‡ä»¶å·²ç”Ÿæˆï¼Œå…± {len(positive_samples)} ä¸ªæ ·æœ¬")
            else:
                print("æ²¡æœ‰æ­£å‘æ£€éªŒæ ·æœ¬")
        except Exception as e:
            print(f"æ­£å‘æ£€éªŒæ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
        
        try:
            if negative_samples:
                self._save_negative_test_file(negative_samples)
                print(f"åå‘æ£€éªŒæ–‡ä»¶å·²ç”Ÿæˆï¼Œå…± {len(negative_samples)} ä¸ªæ ·æœ¬")
            else:
                print("æ²¡æœ‰åå‘æ£€éªŒæ ·æœ¬")
        except Exception as e:
            print(f"åå‘æ£€éªŒæ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
        
        print("\n" + "=" * 60)
        print("æµå¼ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
    
    def _validate_input_columns(self, df_input: pd.DataFrame) -> bool:
        if df_input is None or df_input.empty:
            return False

        columns = set(df_input.columns)
        required_pairs = [
            (self.id_column, self.text_column),      # é»˜è®¤é…ç½®
            ('åºå·', 'text'),                        # åª’ä½“æ–‡æœ¬åŸå§‹åˆ—
            ('comment_id', 'comment_text'),          # VK åŸå§‹åˆ—
            ('id', 'å›ç­”å†…å®¹'),                       # çŸ¥ä¹åŸå§‹åˆ—
            ('åºå·', 'å›ç­”å†…å®¹')                      # çŸ¥ä¹ Excel å¦ä¸€ç§å¸¸è§å‘½å
        ]

        for id_col, text_col in required_pairs:
            if id_col in columns and text_col in columns:
                return True

        print(f"   è¾“å…¥åˆ—ä¸æ»¡è¶³è¦æ±‚ï¼Œç°æœ‰åˆ—: {list(columns)}")
        return False

    def _files_match(self, input_file: str, analyzed_file: str) -> bool:
        """åˆ¤æ–­è¾“å…¥æ–‡ä»¶å’Œåˆ†æç»“æœæ–‡ä»¶æ˜¯å¦åŒ¹é…"""
        input_basename = os.path.basename(input_file).replace('.xlsx', '')
        analyzed_basename = os.path.basename(analyzed_file).replace('(ä¸èƒ½åˆ )analyzed_', '').replace('.xlsx', '')
        return input_basename == analyzed_basename
    
    def _generate_positive_samples(self, df_analyzed: pd.DataFrame, df_input: pd.DataFrame) -> list:
        """ç”Ÿæˆæ­£å‘æ£€éªŒæ ·æœ¬ï¼ˆåŸºäºåŸgenerate_positive_test_fileé€»è¾‘ï¼‰"""
        samples = []
        source = df_analyzed.get('Source', '').iloc[0] if not df_analyzed.empty else 'æœªçŸ¥æ¥æº'
        
        if source not in self.sampling_config:
            return samples
            
        precision_count = self.sampling_config[source].get('precision', 0)
        if precision_count <= 0:
            return samples
        
        # ç­›é€‰æˆåŠŸè®°å½•
        source_results = df_analyzed[
            (df_analyzed.get('Source', '') == source) & 
            (df_analyzed.get('processing_status', '') == ProcessingStatus.SUCCESS)
        ]
        
        if len(source_results) == 0:
            return samples
            
        # æŠ½æ ·
        sample_size = min(precision_count, len(source_results))
        sampled = source_results.sample(n=sample_size, random_state=self.random_seed)
        
        # ç”Ÿæˆæ ·æœ¬è®°å½•
        for _, row in sampled.iterrows():
            unit_text = str(row.get('Unit_Text', ''))
            article_id = str(row.get(self.id_column, ''))
            
            # è·å–å®Œæ•´åŸæ–‡
            full_text = ""
            matching_input = df_input[df_input[self.id_column].astype(str) == article_id]
            if not matching_input.empty:
                full_text = str(matching_input.iloc[0][self.text_column])
            
            # æ–‡æœ¬é«˜äº®å®šä½
            location = self.locate_unit(unit_text, full_text)
            if location and full_text:
                start, end = location
                highlighted_text = (
                    full_text[:start] + 
                    "ã€" + full_text[start:end] + "ã€‘" + 
                    full_text[end:]
                )
            else:
                highlighted_text = f"ã€å®šä½å¤±è´¥ã€‘{unit_text}"
            
            # æ„å»ºæ ·æœ¬è®°å½•ï¼ˆåŒ…å«AIåˆ†æç»“æœå’Œæ£€éªŒå‘˜ç©ºç™½åˆ—ï¼‰
            sample_record = self._build_positive_sample_record(row, article_id, source, unit_text, highlighted_text)
            samples.append(sample_record)
        
        return samples
    
    def _generate_negative_samples(self, df_analyzed: pd.DataFrame, df_input: pd.DataFrame) -> list:
        """ç”Ÿæˆåå‘æ£€éªŒæ ·æœ¬ï¼ˆåŸºäºåŸgenerate_negative_test_fileé€»è¾‘ï¼‰"""
        samples = []
        source = df_analyzed.get('Source', '').iloc[0] if not df_analyzed.empty else 'æœªçŸ¥æ¥æº'
        
        if source not in self.sampling_config:
            return samples
            
        recall_count = self.sampling_config[source].get('recall', 0)
        if recall_count <= 0:
            return samples
        
        # ä»è¾“å…¥æ–‡ä»¶ä¸­æŠ½æ ·æ–‡ç« 
        source_input = df_input[df_input.get('Source', '') == source]
        if len(source_input) == 0:
            return samples
            
        sample_size = min(recall_count, len(source_input))
        sampled_articles = source_input.sample(n=sample_size, random_state=self.random_seed)
        
        # å¤„ç†æ¯ç¯‡è¢«æŠ½æ ·çš„æ–‡ç« 
        for _, article_row in sampled_articles.iterrows():
            article_id_str = str(article_row[self.id_column])
            full_text = str(article_row[self.text_column])
            
            # è·å–è¯¥æ–‡ç« çš„æ‰€æœ‰è®®é¢˜å•å…ƒ
            article_units = df_analyzed[df_analyzed[self.id_column].astype(str) == article_id_str]
            unit_texts = article_units['Unit_Text'].dropna().tolist() if not article_units.empty else []
            
            # æ‰§è¡ŒæŒ–é™¤æ“ä½œ
            positions = []
            for unit_text in unit_texts:
                location = self.locate_unit(str(unit_text), full_text)
                if location:
                    positions.append(location)
            
            # æŒ‰start_indexé™åºæ’åºå¹¶æŒ–é™¤
            positions.sort(key=lambda x: x[0], reverse=True)
            modified_text = full_text
            for start, end in positions:
                modified_text = modified_text[:start] + "ã€å·²æå–ã€‘" + modified_text[end:]
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å‰©ä½™æœ‰æ•ˆæ–‡æœ¬
            check_text = modified_text.replace("ã€å·²æå–ã€‘", "").strip()
            if check_text:
                sample_record = {
                    'Article_ID': article_id_str,
                    'Source': article_row.get('Source', ''),
                    'Extracted_Units_Count': len(positions),
                    'Remaining_Text': modified_text,
                    'Inspector_Has_Missed_Content': '',
                    'Inspector_Missed_Content_Type': '',
                    'Inspector_Comments': ''
                }
                samples.append(sample_record)
        
        return samples

    def _identify_source(self, filename: str) -> str:
        """æ ¹æ®æ–‡ä»¶åè¯†åˆ«æ¥æºï¼Œä¸ä¸»è„šæœ¬é€»è¾‘ä¿æŒä¸€è‡´"""
        source_map = {
            'ä¿„æ€»ç»Ÿ': ['ä¿„æ€»ç»Ÿ', 'æ€»ç»Ÿ', 'Putin', 'president'],
            'ä¿„è¯­åª’ä½“': ['ä¿„è¯­åª’ä½“', 'ä¿„è¯­', 'russian', 'ru_media', 'ä¿„åª’'],
            'ä¸­æ–‡åª’ä½“': ['ä¸­æ–‡åª’ä½“', 'ä¸­æ–‡', 'chinese', 'cn_media', 'ä¸­åª’', 'æ–°åç¤¾'],
            'è‹±è¯­åª’ä½“': ['è‹±è¯­åª’ä½“', 'è‹±è¯­', 'english', 'en_media', 'è‹±åª’'],
            'vk': ['vk'],
            'çŸ¥ä¹': ['çŸ¥ä¹', 'zhihu']
        }

        filename_lower = filename.lower()
        for source, keywords in source_map.items():
            if any(kw.lower() in filename_lower for kw in keywords):
                return source

        return 'æœªçŸ¥æ¥æº'
    
    def _build_positive_sample_record(self, row, article_id: str, source: str, unit_text: str, highlighted_text: str) -> dict:
        """æ„å»ºæ­£å‘æ£€éªŒæ ·æœ¬è®°å½•"""
        # å…ˆå¤åˆ¶AIåˆ†æç»“æœçš„æ‰€æœ‰ç›¸å…³åˆ—
        analysis_columns = [
            "Incident", "Frame_SolutionRecommendation", "Frame_ResponsibilityAttribution",
            "Frame_CausalExplanation", "Frame_MoralEvaluation", "Frame_ProblemDefinition",
            "Frame_ActionStatement", "Valence", "Evidence_Type", "Attribution_Level",
            "Temporal_Focus", "Primary_Actor_Type", "Geographic_Scope",
            "Relationship_Model_Definition", "Discourse_Type"
        ]

        result_record = {}
        for col in analysis_columns:
            if col in row:
                result_record[col] = row[col]

        # æ·»åŠ æ£€éªŒæ–‡ä»¶ç‰¹æœ‰çš„åˆ—
        result_record.update({
            'Unit_ID': row.get('Unit_ID', ''),
            'Article_ID': article_id,
            'Source': source,
            'Unit_Text': unit_text,
            'Highlighted_Full_Text': highlighted_text,
            'Inspector_Is_Relevant': '',
            'Inspector_Boundary_Quality': '',
            'Inspector_Comments': ''
        })

        # ä¸ºæ¯ä¸ªAIåˆ†æåˆ—æ·»åŠ å¯¹åº”çš„Inspectoræ£€éªŒåˆ—
        analysis_columns_to_inspect = [
            "Incident", "Valence", "Evidence_Type", "Attribution_Level",
            "Temporal_Focus", "Primary_Actor_Type", "Geographic_Scope",
            "Relationship_Model_Definition", "Discourse_Type",
            "Frame_SolutionRecommendation", "Frame_ResponsibilityAttribution",
            "Frame_CausalExplanation", "Frame_MoralEvaluation", "Frame_ProblemDefinition",
            "Frame_ActionStatement"
        ]

        for col in analysis_columns_to_inspect:
            result_record[f"Inspector_{col}"] = ''

        return result_record
    
    def _save_positive_test_file(self, samples: list) -> None:
        """ä¿å­˜æ­£å‘æ£€éªŒæ–‡ä»¶"""
        if not samples:
            return
        
        df_output = pd.DataFrame(samples)

        # å®šä¹‰æœ€ç»ˆçš„åˆ—é¡ºåºï¼ˆAIåˆ†æåˆ—å’Œæ£€éªŒåˆ—äº¤é”™ï¼‰
        final_column_order = [
            'Unit_ID', 'Article_ID', 'Source', 'Unit_Text', 'Highlighted_Full_Text'
        ]

        analysis_columns_to_inspect = [
            "Incident", "Valence", "Evidence_Type", "Attribution_Level",
            "Temporal_Focus", "Primary_Actor_Type", "Geographic_Scope",
            "Relationship_Model_Definition", "Discourse_Type",
            "Frame_SolutionRecommendation", "Frame_ResponsibilityAttribution",
            "Frame_CausalExplanation", "Frame_MoralEvaluation", "Frame_ProblemDefinition",
            "Frame_ActionStatement"
        ]

        # åŠ¨æ€æ„å»º"AIåˆ†æåˆ—"å’Œ"æ£€éªŒåˆ—"å¹¶æ’çš„é¡ºåº
        for col in analysis_columns_to_inspect:
            if col in df_output.columns:
                final_column_order.append(col)
                final_column_order.append(f"Inspector_{col}")

        # æ·»åŠ é—ç•™çš„æ£€éªŒå‘˜è¯„è®ºåˆ—
        final_column_order.extend(['Inspector_Is_Relevant', 'Inspector_Boundary_Quality', 'Inspector_Comments'])

        # è¿‡æ»¤æ‰ä¸å­˜åœ¨äºDataFrameä¸­çš„åˆ—åï¼Œå¹¶é‡æ’
        existing_columns_ordered = [col for col in final_column_order if col in df_output.columns]
        df_output = df_output.reindex(columns=existing_columns_ordered)

        zh_positive_path = os.path.join(self.output_path, 'æ­£å‘æ£€éªŒåŠæ¡†æ¶ç»´åº¦æ£€éªŒ_é«˜äº®ç‰ˆ.xlsx')
        ru_positive_path = os.path.join(self.output_path, 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°_Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ_Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°(Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ_Ğ¸_Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹).xlsx')
        self._save_bilingual(df_output, zh_positive_path, ru_positive_path)
    
    def _save_negative_test_file(self, samples: list) -> None:
        """ä¿å­˜åå‘æ£€éªŒæ–‡ä»¶"""
        if not samples:
            return
        
        df_output = pd.DataFrame(samples)
        zh_negative_path = os.path.join(self.output_path, 'åå‘æ£€éªŒ_æŒ–é™¤ç‰ˆ.xlsx')
        ru_negative_path = os.path.join(self.output_path, 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°_Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹_Ñ‚ĞµĞºÑÑ‚(Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ°).xlsx')
        self._save_bilingual(df_output, zh_negative_path, ru_negative_path)
    
    def _load_locale_mapping(self, lang: str) -> dict:
        """ä»JSONæ–‡ä»¶åŠ è½½æœ¬åœ°åŒ–æ˜ å°„"""
        if lang in self._locale_cache:
            return self._locale_cache[lang]

        locales_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'locales')
        file_path = os.path.join(locales_dir, f'{lang}.json')

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                mapping = json.load(f)
                self._locale_cache[lang] = mapping
                return mapping
        except Exception as e:
            print(f"åŠ è½½æœ¬åœ°åŒ–æ–‡ä»¶ {lang}.json å¤±è´¥: {e}")
            return {}

    def _decorate_headers(self, df: pd.DataFrame, lang: str) -> pd.DataFrame:
        """è£…é¥°åˆ—åä¸ºè¯­è¨€æ ‡ç­¾æ ¼å¼"""
        mapping = self._load_locale_mapping(lang)
        new_cols = []

        for c in list(df.columns):
            if c in mapping:
                new_cols.append(f"{mapping[c]}({c})")
            else:
                new_cols.append(c)

        df_out = df.copy()
        df_out.columns = new_cols
        return df_out

    def _save_bilingual(self, df: pd.DataFrame, zh_path: str, ru_path: str):
        """ä¿å­˜åŒè¯­ç‰ˆæœ¬æ–‡ä»¶"""
        try:
            zh_dir = os.path.dirname(zh_path)
            ru_dir = os.path.dirname(ru_path)

            if zh_dir:
                os.makedirs(zh_dir, exist_ok=True)

            if ru_dir and ru_dir != zh_dir:
                os.makedirs(ru_dir, exist_ok=True)
        except Exception as e:
            print(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {e}")

        try:
            df_zh = self._decorate_headers(df, 'zh')
            df_zh.to_excel(zh_path, index=False)
        except Exception as e:
            print(f"ä¸­æ–‡ç‰ˆæœ¬å¯¼å‡ºå¤±è´¥: {e}")

        try:
            df_ru = self._decorate_headers(df, 'ru')
            df_ru.to_excel(ru_path, index=False)
        except Exception as e:
            print(f"ä¿„è¯­ç‰ˆæœ¬å¯¼å‡ºå¤±è´¥: {e}")

def create_reliability_test_module(input_path: str, output_path: str, sampling_config: dict,
                                 id_column: str = "åºå·", text_column: str = "text", 
                                 random_seed: int = 42) -> ReliabilityTestModule:
    return ReliabilityTestModule(input_path, output_path, sampling_config, id_column, text_column, random_seed)