# -*- coding: utf-8 -*-
"""
Processor classes for handling different data sources and analysis strategies.
"""
import os
import asyncio
import hashlib
import pandas as pd
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from tqdm.asyncio import tqdm as aio_tqdm, gather as aio_gather
from .utils import (
    UX, safe_str_convert, normalize_text, count_tokens, identify_source,
    get_processing_state, clean_failed_records, create_unified_record, detect_language_and_get_config,
    ProcessingStatus, ANALYSIS_COLUMNS
)
from .api_service import APIService

# ä»é…ç½®ä¸­è·å–é˜ˆå€¼å¸¸é‡çš„è¾…åŠ©å‡½æ•°
def _get_thresholds_from_config(config):
    """ä»é…ç½®ä¸­æå–é˜ˆå€¼å¸¸é‡"""
    social_config = config.get('social_media', {})
    thresholds = social_config.get('text_length_thresholds', {})
    return {
        'ZHIHU_SHORT_TOKEN_THRESHOLD': thresholds.get('zhihu_short_text', 100),
        'ZHIHU_LONG_TOKEN_THRESHOLD': thresholds.get('zhihu_long_text', 1300),
        'VK_LONG_TEXT_THRESHOLD': thresholds.get('vk_long_text', 1500)
    }

class BaseProcessor:
    def __init__(self, api_service, config=None, prompts=None):
        self.api_service = api_service
        self.config = config or {}
        self.prompts = prompts
        self.Units_collector = []
        
        # ä»é…ç½®ä¸­è·å–é˜ˆå€¼å¸¸é‡
        self.thresholds = _get_thresholds_from_config(self.config)
    
    def _save_progress_generic(self, df_to_process, output_path, id_column):
        """é€šç”¨ä¿å­˜è¿›åº¦æ–¹æ³•"""
        if df_to_process.empty:
            return
        
        # åªä¿å­˜æˆåŠŸå’Œæ— ç›¸å…³çš„è®°å½•
        if 'processing_status' in df_to_process.columns:
            save_mask = df_to_process['processing_status'].isin([
                ProcessingStatus.SUCCESS, 
                ProcessingStatus.NO_RELEVANT
            ])
            df_to_save = df_to_process[save_mask].copy()
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šæ’é™¤API_CALL_FAILED
            if 'speaker' in df_to_process.columns:
                save_mask = ~df_to_process['speaker'].astype(str).str.contains('API_CALL_FAILED', na=False)
                df_to_save = df_to_process[save_mask].copy()
            else:
                df_to_save = df_to_process.copy()
        
        if df_to_save.empty:
            return
        
        # ç²¾ç¡®åˆå¹¶æˆ–åˆ›å»ºæ–‡ä»¶
        if os.path.exists(output_path):
            try:
                df_existing = pd.read_excel(output_path)
                if not df_existing.empty and id_column in df_existing.columns:
                    # è·å–å·²æˆåŠŸå¤„ç†çš„IDï¼ˆä¸åŒ…æ‹¬å¤±è´¥çš„ï¼‰
                    success_existing_ids, _ = get_processing_state(df_existing, id_column)
                    
                    # åªæ·»åŠ æ–°çš„æˆåŠŸè®°å½•ï¼Œä¸è¦†ç›–å·²æˆåŠŸçš„
                    new_mask = ~df_to_save[id_column].astype(str).isin(success_existing_ids)
                    df_new = df_to_save[new_mask]
                    
                    if not df_new.empty:
                        df_final = pd.concat([df_existing, df_new], ignore_index=True)
                        UX.info(f"æ·»åŠ äº† {len(df_new)} æ¡æ–°è®°å½•åˆ°ç°æœ‰ {len(df_existing)} æ¡")
                    else:
                        df_final = df_existing
                        UX.info(f"æ— æ–°è®°å½•éœ€è¦æ·»åŠ ï¼Œä¿æŒç°æœ‰ {len(df_existing)} æ¡")
                else:
                    df_final = df_to_save
            except Exception as e:
                UX.warn(f"è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨æ–°æ•°æ®")
                df_final = df_to_save
        else:
            df_final = df_to_save
        
        df_final.to_excel(output_path, index=False)
        
        # æ˜¾ç¤ºç»Ÿè®¡
        if 'processing_status' in df_final.columns:
            success_count = (df_final['processing_status'] == ProcessingStatus.SUCCESS).sum()
            no_relevant_count = (df_final['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
            failed_count = (df_final['processing_status'] == ProcessingStatus.API_FAILED).sum()
            UX.info(f"è¿›åº¦ä¿å­˜: æˆåŠŸ{success_count}æ¡, æ— ç›¸å…³{no_relevant_count}æ¡, å¤±è´¥{failed_count}æ¡")

class VKProcessor(BaseProcessor):
    """VKè¯„è®ºå¤„ç†å™¨ - ä¿®å¤ç‰ˆ"""
    
    def process(self, df, output_path, source='vk'):
        """å¤„ç†VKæ–‡ä»¶ - ä¿®å¤ç‰ˆ"""
        UX.info("å¤„ç†VKè¯„è®º...")
        
        # è·å–VKåˆ—æ˜ å°„é…ç½®ï¼ˆç»Ÿä¸€è·¯å¾„ï¼‰
        social_config = self.config.get('social_media', {})
        column_mapping = social_config.get('column_mapping', {})
        mapping = column_mapping.get('vk', {})
        
        # è¾“å…¥æ•°æ®æ¨¡å¼æ ¡éªŒ
        required_columns = set(mapping.values())
        actual_columns = set(df.columns)
        
        if not required_columns.issubset(actual_columns):
            missing = required_columns - actual_columns
            UX.err(f"VKæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ä»¥ä¸‹å¿…éœ€åˆ—: {list(missing)}ã€‚å·²è·³è¿‡æ­¤æ–‡ä»¶ã€‚")
            return
        
        # æ£€æŸ¥å·²å¤„ç†çš„è®°å½•ï¼ˆåŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥ï¼‰
        processed_ids = set()
        failed_ids = set()
        if os.path.exists(output_path):
            try:
                df_existing_check = pd.read_excel(output_path)
                if not df_existing_check.empty and mapping['comment_id'] in df_existing_check.columns:
                    processed_ids, failed_ids = get_processing_state(df_existing_check, mapping['comment_id'])
                    
                    if 'processing_status' in df_existing_check.columns:
                        success_count = (df_existing_check['processing_status'] == ProcessingStatus.SUCCESS).sum()
                        no_relevant_count = (df_existing_check['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                        failed_count = (df_existing_check['processing_status'] == ProcessingStatus.API_FAILED).sum()
                        UX.info(f"VKå·²å¤„ç†: {len(processed_ids)} (æˆåŠŸ: {success_count}, æ— ç›¸å…³: {no_relevant_count})")
                        if failed_ids:
                            UX.info(f"å‘ç°VKå¤±è´¥è®°å½•: {len(failed_ids)} ä¸ªï¼Œå°†é‡æ–°åˆ†æ")
                    else:
                        UX.info(f"VKå·²å¤„ç†: {len(processed_ids)}")
                        
            except Exception as e:
                UX.warn(f"è¯»å–VKå·²å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
        
        # æ¸…ç†å¤±è´¥è®°å½•ï¼Œä¸ºé‡æ–°åˆ†æåšå‡†å¤‡
        if failed_ids:
            clean_failed_records(output_path, mapping['comment_id'])
            UX.info(f"æ¸…ç†äº† {len(failed_ids)} ä¸ªVKå¤±è´¥è®°å½•ï¼Œå‡†å¤‡é‡æ–°åˆ†æ")

        # ç­›é€‰å¾…å¤„ç†æ•°æ®ï¼šåªå¤„ç†å®Œå…¨æœªå¤„ç†çš„è®°å½• + å¤±è´¥çš„è®°å½•
        df[mapping['comment_id']] = df[mapping['comment_id']].astype(str)
        unprocessed_ids = set(df[mapping['comment_id']].astype(str)) - (processed_ids - failed_ids)
        df_to_process = df[df[mapping['comment_id']].astype(str).isin(unprocessed_ids)]
        
        if df_to_process.empty:
            UX.ok("æ‰€æœ‰VKè¯„è®ºå·²å¤„ç†")
            return
        
        UX.info(f"å¾…å¤„ç†: {len(df_to_process)} æ¡")
        
        # è·å–VKå¤„ç†é…ç½®
        vk_config = self.config.get('social_media', {}).get('vk_processing', {})
        
        # ã€å…³é”®ä¿®å¤ã€‘ï¼šåˆå§‹åŒ–ä¸ºPENDINGè€Œä¸æ˜¯SUCCESS
        df_to_process['processing_status'] = ProcessingStatus.PENDING  # å¾…å¤„ç†çŠ¶æ€
        
        # åˆå§‹åŒ–åˆ†æåˆ—
        analysis_columns = ANALYSIS_COLUMNS + ['Unit_Hash']  # åªæ·»åŠ Unit_Hashå­—æ®µ
        
        df_to_process['Source'] = source
        for col in analysis_columns:
            if col not in df_to_process.columns:
                df_to_process[col] = pd.NA
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œå°†Unit_Hashæ”¾åœ¨Post_IDå’ŒSourceä¹‹é—´
        if 'Unit_Hash' in df_to_process.columns:
            cols = list(df_to_process.columns)
            if 'Post_ID' in cols and 'Source' in cols:
                # ç§»é™¤Unit_Hash
                cols.remove('Unit_Hash')
                # æ‰¾åˆ°Post_IDçš„ä½ç½®ï¼Œåœ¨å…¶åæ’å…¥Unit_Hash
                post_idx = cols.index('Post_ID')
                cols.insert(post_idx + 1, 'Unit_Hash')
                df_to_process = df_to_process[cols]
        
        # ä½¿ç”¨ç»Ÿä¸€çš„APIé…ç½®
        api_strategy = self.config.get('api', {}).get('strategy', {})
        max_concurrent = api_strategy.get('max_concurrent_requests', 2)
        
        # åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        batch_tasks = []
        batch_to_comments = {}  # è®°å½•æ‰¹æ¬¡å¯¹åº”çš„è¯„è®ºID
        
        for post_id, group in df_to_process.groupby(mapping['post_id'], dropna=False):
            if pd.isna(post_id):
                continue
                
            post_text = safe_str_convert(group[mapping['post_text']].iloc[0])
            
            comments_list = []
            comment_ids_in_batch = []
            
            for _, row in group.iterrows():
                comment_id = row[mapping['comment_id']]
                comment_text = row[mapping['comment_text']]
                
                if pd.isna(comment_id) or pd.isna(comment_text) or not str(comment_text).strip():
                    # ç©ºè¯„è®ºç›´æ¥æ ‡è®°ä¸ºNO_RELEVANT
                    mask = df_to_process[mapping['comment_id']].astype(str) == str(comment_id)
                    df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.NO_RELEVANT
                    df_to_process.loc[mask, 'relevance'] = 'ç©ºè¯„è®º'
                    continue
                    
                comments_list.append({
                    "comment_id": str(comment_id),
                    "comment_text": safe_str_convert(comment_text)
                })
                comment_ids_in_batch.append(str(comment_id))
                
                # æ”¶é›†åŸºç¡€å•å…ƒä¿¡æ¯
                # è·å–channel_nameï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
                channel_name = safe_str_convert(row.get(mapping.get('channel_name', 'channel_name'), 'æœªçŸ¥é¢‘é“')) if mapping.get('channel_name', 'channel_name') in df_to_process.columns else 'æœªçŸ¥é¢‘é“'
                
                self.Units_collector.append({
                    'Unit_ID': f"VK-{comment_id}",
                    'Source': source,
                    'Post_ID': str(post_id),
                    'Post_Text': post_text,
                    'Comment_Text': safe_str_convert(comment_text),
                    'channel_name': channel_name,
                    'AI_Is_Relevant': None
                })
            
            # åˆ›å»ºæ‰¹æ¬¡
            batch_size_limit = vk_config.get('batch_size_limit', 20)
            for i in range(0, len(comments_list), batch_size_limit):
                chunk = comments_list[i:i + batch_size_limit]
                chunk_ids = comment_ids_in_batch[i:i + batch_size_limit]
                if chunk:
                    batch_idx = len(batch_tasks)
                    batch_tasks.append({
                        "post_id": str(post_id),
                        "post_text": post_text,
                        "comments": chunk
                    })
                    batch_to_comments[batch_idx] = chunk_ids
        
        if not batch_tasks:
            UX.warn("æ²¡æœ‰å¯å¤„ç†çš„æ‰¹æ¬¡ä»»åŠ¡")
            return
            
        UX.info(f"åˆ›å»ºäº† {len(batch_tasks)} ä¸ªæ‰¹å¤„ç†ä»»åŠ¡")
        
        # å¤„ç†æ‰¹æ¬¡
        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            future_to_batch = {
                executor.submit(self._process_batch, batch_tasks[i]): i 
                for i in range(len(batch_tasks))
            }
            progress_bar = tqdm(as_completed(future_to_batch), total=len(batch_tasks), desc="æ‰¹å¤„ç†è¿›åº¦")
            
            for future in progress_bar:
                batch_idx = future_to_batch[future]
                expected_comment_ids = batch_to_comments[batch_idx]
                
                try:
                    batch_results = future.result()
                    
                    if batch_results is None:
                        # æ•´ä¸ªæ‰¹æ¬¡å¤±è´¥
                        UX.warn(f"æ‰¹æ¬¡ {batch_idx} è¿”å›Noneï¼Œæ ‡è®°æ‰€æœ‰è¯„è®ºä¸ºå¤±è´¥")
                        for comment_id in expected_comment_ids:
                            mask = df_to_process[mapping['comment_id']].astype(str) == comment_id
                            df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.API_FAILED
                            df_to_process.loc[mask, 'relevance'] = 'APIè°ƒç”¨å¤±è´¥'
                            for col in analysis_columns:
                                if col != 'Source':
                                    df_to_process.loc[mask, col] = 'API_FAILED'
                        continue
                    
                    if not isinstance(batch_results, list):
                        UX.warn(f"æ‰¹æ¬¡ {batch_idx} è¿”å›éåˆ—è¡¨ç»“æœ")
                        for comment_id in expected_comment_ids:
                            mask = df_to_process[mapping['comment_id']].astype(str) == comment_id
                            df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.API_FAILED
                            df_to_process.loc[mask, 'relevance'] = 'è¿”å›æ ¼å¼é”™è¯¯'
                        continue
                    
                    # å¤„ç†è¿”å›çš„ç»“æœ
                    processed_comment_ids = set()
                    
                    for result in batch_results:
                        if not isinstance(result, dict):
                            continue
                            
                        comment_id = str(result.get('comment_id', ''))
                        if not comment_id:
                            continue
                        
                        processed_comment_ids.add(comment_id)
                        mask = df_to_process[mapping['comment_id']].astype(str) == comment_id
                        
                        # ä»DataFrameä¸­è·å–åŸå§‹comment_textå¹¶æ·»åŠ åˆ°resultä¸­ç”¨äºå“ˆå¸Œè®¡ç®—
                        if mask.any():
                            original_comment_text = df_to_process.loc[mask, mapping['comment_text']].iloc[0]
                            result['comment_text'] = original_comment_text
                        
                        # ã€å…³é”®ã€‘ï¼šæ­£ç¡®è®¾ç½®processing_status
                        if 'processing_status' in result:
                            df_to_process.loc[mask, 'processing_status'] = result['processing_status']
                        else:
                            # æ ¹æ®relevanceåˆ¤æ–­
                            if result.get('relevance') == 'ä¸ç›¸å…³':
                                df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.NO_RELEVANT
                            elif result.get('relevance') in ['API_FAILED', 'INVALID_RESPONSE', 'EXCEPTION']:
                                df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.API_FAILED
                            else:
                                df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.SUCCESS
                        
                        # æ›´æ–°Units_collectorä¸­çš„ç›¸å…³æ€§ä¿¡æ¯
                        for Unit in self.Units_collector:
                            if Unit['Unit_ID'] == f"VK-{comment_id}":
                                Unit['AI_Is_Relevant'] = (result.get('relevance') not in ['ä¸ç›¸å…³', 'API_FAILED', 'INVALID_RESPONSE', 'EXCEPTION'])
                                break
                        
                        # æ›´æ–°å…¶ä»–å­—æ®µï¼ˆåŒ…æ‹¬Unit_Hashï¼‰
                        for key, value in result.items():
                            if (key in df_to_process.columns or key == 'Unit_Hash') and key != mapping['comment_id']:
                                if isinstance(value, list):
                                    df_to_process.loc[mask, key] = json.dumps(value, ensure_ascii=False)
                                else:
                                    df_to_process.loc[mask, key] = value
                    
                    # æ ‡è®°æœªè¿”å›çš„è¯„è®ºä¸ºå¤±è´¥
                    missing_ids = set(expected_comment_ids) - processed_comment_ids
                    if missing_ids:
                        UX.warn(f"æ‰¹æ¬¡ {batch_idx} ä¸­æœ‰ {len(missing_ids)} ä¸ªè¯„è®ºæœªè¿”å›ç»“æœï¼Œæ ‡è®°ä¸ºå¤±è´¥")
                        for comment_id in missing_ids:
                            mask = df_to_process[mapping['comment_id']].astype(str) == comment_id
                            df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.API_FAILED
                            df_to_process.loc[mask, 'relevance'] = 'æœªè¿”å›ç»“æœ'
                                            
                    # å®šæœŸä¿å­˜è¿›åº¦
                    save_interval = vk_config.get('save_interval', 5)
                    if (batch_idx + 1) % save_interval == 0:
                        self._save_progress_generic(df_to_process, output_path, mapping['comment_id'])
                        # æ˜¾ç¤ºå½“å‰ç»Ÿè®¡
                        success_count = (df_to_process['processing_status'] == ProcessingStatus.SUCCESS).sum()
                        failed_count = (df_to_process['processing_status'] == ProcessingStatus.API_FAILED).sum()
                        no_relevant_count = (df_to_process['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                        pending_count = (df_to_process['processing_status'] == ProcessingStatus.PENDING).sum()
                        total_processed = success_count + failed_count + no_relevant_count
                        progress_rate = (total_processed / len(df_to_process)) * 100
                        UX.info(f"ğŸ“Š VKå¤„ç†è¿›åº¦ ({progress_rate:.1f}%): æˆåŠŸ{success_count}, å¤±è´¥{failed_count}, æ— ç›¸å…³{no_relevant_count}, å¾…å¤„ç†{pending_count}")
                        
                except Exception as e:
                    UX.err(f"å¤„ç†æ‰¹æ¬¡ {batch_idx} å¼‚å¸¸: {str(e)[:100]}")
                    # æ‰¹æ¬¡å¼‚å¸¸ï¼Œæ ‡è®°æ‰€æœ‰è¯„è®ºä¸ºå¤±è´¥
                    for comment_id in expected_comment_ids:
                        mask = df_to_process[mapping['comment_id']].astype(str) == comment_id
                        df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.API_FAILED
                        df_to_process.loc[mask, 'relevance'] = f'æ‰¹æ¬¡å¼‚å¸¸: {str(e)[:50]}'
                    continue
        
        # æœ€ç»ˆæ£€æŸ¥ï¼šå°†æ‰€æœ‰ä»ä¸ºPENDINGçš„è®°å½•æ ‡è®°ä¸ºå¤±è´¥
        pending_mask = df_to_process['processing_status'] == ProcessingStatus.PENDING
        if pending_mask.any():
            UX.warn(f"å‘ç° {pending_mask.sum()} æ¡æœªå¤„ç†è®°å½•ï¼Œæ ‡è®°ä¸ºå¤±è´¥")
            df_to_process.loc[pending_mask, 'processing_status'] = ProcessingStatus.API_FAILED
            df_to_process.loc[pending_mask, 'relevance'] = 'æœªè¢«å¤„ç†'
        
        # æœ€ç»ˆä¿å­˜
        self._save_progress_generic(df_to_process, output_path, mapping['comment_id'])
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        success_count = (df_to_process['processing_status'] == ProcessingStatus.SUCCESS).sum()
        failed_count = (df_to_process['processing_status'] == ProcessingStatus.API_FAILED).sum()
        no_relevant_count = (df_to_process['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
        
        completion_rate = ((success_count + no_relevant_count) / len(df_to_process)) * 100
        UX.ok(f"ğŸ“‹ VKå¤„ç†å®Œæˆæ€»ç»“: å®Œæˆåº¦{completion_rate:.1f}% (æˆåŠŸ{success_count} + æ— ç›¸å…³{no_relevant_count})")
        UX.info(f"   ğŸ“Š è¯¦ç»†ç»Ÿè®¡: æˆåŠŸ{success_count}, å¤±è´¥{failed_count}, æ— ç›¸å…³{no_relevant_count}")
        
        if failed_count > 0:
            UX.warn(f"   âš ï¸  ä»æœ‰{failed_count}æ¡è®°å½•å¤„ç†å¤±è´¥ï¼Œå¯å†æ¬¡è¿è¡Œè¿›è¡Œæ™ºèƒ½é‡è¯•")
    
    def _process_batch(self, batch_task):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        try:
            post_text = batch_task['post_text']
            comments_json = json.dumps(batch_task['comments'], ensure_ascii=False)
            
            prompt = self.prompts.VK_BATCH_ANALYSIS.format(
                post_text=str(post_text),
                comments_json=comments_json
            )
            
            # æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼šæ ¹æ®æ–‡æœ¬é•¿åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹
            combined_text = str(post_text) + " " + comments_json
            text_tokens = count_tokens(combined_text)
            
            # ä½¿ç”¨å·²åˆå§‹åŒ–çš„é˜ˆå€¼
            VK_LONG_TEXT_THRESHOLD = self.thresholds['VK_LONG_TEXT_THRESHOLD']
            
            if text_tokens > VK_LONG_TEXT_THRESHOLD:
                stage_key = 'VK_BATCH_LONG'
                UX.info(f"ğŸ”§ VKæ‰¹æ¬¡æ¨¡å‹é€‰æ‹©: ({text_tokens} tokens > {VK_LONG_TEXT_THRESHOLD}) â†’ é•¿æ–‡æœ¬æ¨¡å‹")
            else:
                stage_key = 'VK_BATCH'
                UX.info(f"ğŸ”§ VKæ‰¹æ¬¡æ¨¡å‹é€‰æ‹©: ({text_tokens} tokens) â†’ æ ‡å‡†æ¨¡å‹")
            
            # è°ƒç”¨API
            result = self.api_service.call_api_sync(prompt, language='ru', stage_key=stage_key)
            
            # APIè°ƒç”¨å¤±è´¥
            if result is None:
                UX.warn(f"APIè°ƒç”¨è¿”å›Noneï¼Œæ‰¹æ¬¡åŒ…å« {len(batch_task['comments'])} æ¡è¯„è®º")
                post_id = batch_task.get('post_id')
                return [self._create_failed_record(c['comment_id'], 'APIè¿”å›None', post_id) 
                        for c in batch_task['comments']]
            
            # å¼ºåŒ–APIç»“æœç±»å‹æ£€æŸ¥
            if not isinstance(result, list):
                UX.warn(f"æ‰¹æ¬¡APIè¿”å›æ ¼å¼é”™è¯¯ï¼ˆéåˆ—è¡¨ï¼‰ï¼Œå°†æ•´ä¸ªæ‰¹æ¬¡æ ‡è®°ä¸ºå¤±è´¥ã€‚è¿”å›å†…å®¹: {str(result)[:100]}")
                post_id = batch_task.get('post_id')
                return [self._create_failed_record(c['comment_id'], 'APIå“åº”æ ¼å¼æ— æ•ˆ', post_id) 
                        for c in batch_task['comments']]
            
            # å°è¯•æå–ç»“æœ
            processed_results = []
            
            if isinstance(result, list):
                for item in result:
                    if isinstance(item, dict):
                        # ç¡®ä¿æœ‰comment_id
                        if 'comment_id' not in item:
                            continue
                        
                        comment_id = str(item.get('comment_id', ''))
                        
                        # ä»batch_taskä¸­æ‰¾åˆ°å¯¹åº”çš„comment_text
                        for comment in batch_task['comments']:
                            if str(comment['comment_id']) == comment_id:
                                item['comment_text'] = comment['comment_text']
                                item['Unit_Text'] = comment['comment_text']  # æ·»åŠ Unit_Textå­—æ®µ
                                break
                        
                        # æ·»åŠ processing_status
                        if 'processing_status' not in item:
                            if item.get('relevance') == 'ä¸ç›¸å…³':
                                item['processing_status'] = ProcessingStatus.NO_RELEVANT
                            else:
                                item['processing_status'] = ProcessingStatus.SUCCESS
                        
                        self._add_hash_to_record(item, 'comment_text')
                        processed_results.append(item)
                        
            elif isinstance(result, dict):
                # å°è¯•å¤šä¸ªå¯èƒ½çš„é”®
                for key in ['analysis', 'results', 'processed_results', 'data', 'comments']:
                    if key in result and isinstance(result[key], list):
                        for item in result[key]:
                            if isinstance(item, dict) and 'comment_id' in item:
                                comment_id = str(item.get('comment_id', ''))
                                
                                if comment_id:
                                    # ä»batch_taskä¸­æ‰¾åˆ°å¯¹åº”çš„comment_text
                                    for comment in batch_task['comments']:
                                        if str(comment['comment_id']) == comment_id:
                                            item['comment_text'] = comment['comment_text']
                                            item['Unit_Text'] = comment['comment_text']  # æ·»åŠ Unit_Textå­—æ®µ
                                            break
                                
                                if 'processing_status' not in item:
                                    if item.get('relevance') == 'ä¸ç›¸å…³':
                                        item['processing_status'] = ProcessingStatus.NO_RELEVANT
                                    else:
                                        item['processing_status'] = ProcessingStatus.SUCCESS
                                
                                self._add_hash_to_record(item, 'comment_text')
                                processed_results.append(item)
                        break
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°æœ‰æ•ˆç»“æœ
            if not processed_results:
                UX.warn(f"æ— æ³•ä»APIå“åº”ä¸­æå–æœ‰æ•ˆç»“æœï¼Œæ‰¹æ¬¡åŒ…å« {len(batch_task['comments'])} æ¡è¯„è®º")
                post_id = batch_task.get('post_id')
                return [self._create_failed_record(c['comment_id'], 'APIå“åº”æ ¼å¼æ— æ•ˆ', post_id) 
                        for c in batch_task['comments']]
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¯„è®ºéƒ½æœ‰ç»“æœ
            result_ids = {str(r.get('comment_id')) for r in processed_results if r.get('comment_id')}
            expected_ids = {c['comment_id'] for c in batch_task['comments']}
            missing_ids = expected_ids - result_ids
            extra_ids = result_ids - expected_ids
            
            if missing_ids:
                UX.warn(f"APIå“åº”ç¼ºå°‘ {len(missing_ids)} æ¡è¯„è®ºçš„ç»“æœ")
                post_id = batch_task.get('post_id')
                for comment_id in missing_ids:
                    processed_results.append(self._create_failed_record(comment_id, 'APIå“åº”ä¸­ç¼ºå¤±', post_id))
            
            if extra_ids:
                UX.warn(f"APIå“åº”åŒ…å« {len(extra_ids)} æ¡é¢å¤–è¯„è®ºï¼Œå°†è¢«å¿½ç•¥")
                processed_results = [r for r in processed_results if str(r.get('comment_id', '')) in expected_ids]
                        
            return processed_results
            
        except Exception as e:
            UX.err(f"æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {str(e)}")
            post_id = batch_task.get('post_id')
            return [self._create_failed_record(c['comment_id'], f'å¼‚å¸¸: {str(e)[:50]}', post_id) 
                    for c in batch_task['comments']]
    
    def _create_failed_record(self, comment_id, reason, post_id=None):
        """åˆ›å»ºå¤±è´¥è®°å½•"""
        record = create_unified_record(ProcessingStatus.API_FAILED, comment_id, 'vk', '', reason)
        record['comment_id'] = comment_id
        record['relevance'] = 'API_FAILED'
        record['speaker'] = 'API_CALL_FAILED'
        record['Incident'] = reason
        
        if post_id:
            record['Batch_ID'] = f"{post_id}-BATCH_FAILED"
        
        return record
    
    def _add_hash_to_record(self, record, text_field):
        """ä¸ºè®°å½•æ·»åŠ å“ˆå¸Œå€¼"""
        text = safe_str_convert(record.get(text_field, ''))
        normalized_text = normalize_text(text)
        record['Unit_Hash'] = hashlib.sha256(normalized_text.encode('utf-8')).hexdigest()
        return record

class ZhihuProcessor(BaseProcessor):
    
    """çŸ¥ä¹å›ç­”å¤„ç†å™¨ï¼ˆä¸¤æ­¥å¼ï¼Œä¸åª’ä½“æ–‡æœ¬å¯¹é½ï¼‰"""
    
    def _get_author_name(self, original_row, mapping):
        """æ™ºèƒ½è·å–ä½œè€…åç§°ï¼šæœ‰å›ç­”ç”¨æˆ·ååˆ—åˆ™ä½¿ç”¨ï¼Œæ— åˆ™è¿”å›æœªçŸ¥ä½œè€…"""
        author_column = mapping.get("author", "å›ç­”ç”¨æˆ·å")
        if author_column in original_row.index:
            author_value = safe_str_convert(original_row[author_column])
            return author_value if author_value.strip() else 'æœªçŸ¥ä½œè€…'
        else:
            return 'æœªçŸ¥ä½œè€…'
    
    def _finalize_record(self, result_data, original_row, mapping, answer_id, Unit_index=1):
        """
        å°†APIè¿”å›çš„åˆ†æç»“æœå°è£…æˆä¸€ä¸ªå®Œæ•´çš„è®°å½•å­—å…¸ã€‚
        """
        if not result_data or not isinstance(result_data, dict):
            return None

        author = self._get_author_name(original_row, mapping)
        
        result_data['speaker'] = author
        result_data['Source'] = 'çŸ¥ä¹'
        result_data['processing_status'] = ProcessingStatus.SUCCESS
        result_data['Unit_ID'] = f"ZH-{answer_id}-{Unit_index}"
        result_data['Answer_ID'] = f"ZH-{answer_id}"
        result_data['id'] = answer_id  # å…¼å®¹æ—§åˆ—å
        result_data['åºå·'] = answer_id # å…¼å®¹æ—§åˆ—å

        # V2æ ¼å¼ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€è½¬æ¢

        self._add_hash_to_record(result_data, 'Unit_Text')
        
        return result_data
    
    async def process(self, df, output_path, source='çŸ¥ä¹'):
        """å¤„ç†çŸ¥ä¹æ–‡ä»¶"""
        UX.info("å¤„ç†çŸ¥ä¹å›ç­”...")
        
        # è·å–ç¤¾äº¤åª’ä½“ä¸“ç”¨çš„åˆ—æ˜ å°„é…ç½®
        # è·å–çŸ¥ä¹åˆ—æ˜ å°„é…ç½®ï¼ˆç»Ÿä¸€è·¯å¾„ï¼‰
        social_config = self.config.get('social_media', {})
        column_mapping = social_config.get('column_mapping', {})
        mapping = column_mapping.get('zhihu', {})
        failed_ids_list = []  # åˆå§‹åŒ–å¤±è´¥IDåˆ—è¡¨
        
        # è¾“å…¥æ•°æ®æ¨¡å¼æ ¡éªŒï¼ˆå›ç­”ç”¨æˆ·ååˆ—ä¸ºå¯é€‰ï¼‰
        required_columns = set(mapping.values())
        # ç§»é™¤å¯é€‰çš„å›ç­”ç”¨æˆ·ååˆ—
        optional_columns = {mapping.get("author", "å›ç­”ç”¨æˆ·å")}
        required_columns = required_columns - optional_columns
        actual_columns = set(df.columns)
        
        if not required_columns.issubset(actual_columns):
            missing = required_columns - actual_columns
            UX.err(f"çŸ¥ä¹æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ä»¥ä¸‹å¿…éœ€åˆ—: {list(missing)}ã€‚å·²è·³è¿‡æ­¤æ–‡ä»¶ã€‚")
            return failed_ids_list
        
        # æ£€æŸ¥å¯é€‰åˆ—æ˜¯å¦å­˜åœ¨
        author_column = mapping.get("author", "å›ç­”ç”¨æˆ·å")
        if author_column in actual_columns:
            UX.info(f"æ£€æµ‹åˆ°ä½œè€…åˆ—: {author_column}")
        else:
            UX.info(f"æœªæ£€æµ‹åˆ°ä½œè€…åˆ—: {author_column}ï¼Œå°†ä½¿ç”¨'æœªçŸ¥ä½œè€…'")
        
        # æ£€æŸ¥å·²å¤„ç†çš„è®°å½•ï¼ˆåŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥ï¼‰
        processed_ids = set()
        failed_ids = set()
        if os.path.exists(output_path):
            try:
                df_existing_check = pd.read_excel(output_path)
                if not df_existing_check.empty and mapping["id"] in df_existing_check.columns:
                    processed_ids, failed_ids = get_processing_state(df_existing_check, mapping["id"])
                    
                    if 'processing_status' in df_existing_check.columns:
                        success_count = (df_existing_check['processing_status'] == ProcessingStatus.SUCCESS).sum()
                        no_relevant_count = (df_existing_check['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                        failed_count = (df_existing_check['processing_status'] == ProcessingStatus.API_FAILED).sum()
                        UX.info(f"çŸ¥ä¹å·²å¤„ç†: {len(processed_ids)} (æˆåŠŸ: {success_count}, æ— ç›¸å…³: {no_relevant_count})")
                        if failed_ids:
                            UX.info(f"å‘ç°çŸ¥ä¹å¤±è´¥è®°å½•: {len(failed_ids)} ä¸ªï¼Œå°†é‡æ–°åˆ†æ")
                    else:
                        UX.info(f"çŸ¥ä¹å·²å¤„ç†: {len(processed_ids)}")
                        
            except Exception as e:
                UX.warn(f"è¯»å–çŸ¥ä¹å·²å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
        
        # æ¸…ç†å¤±è´¥è®°å½•ï¼Œä¸ºé‡æ–°åˆ†æåšå‡†å¤‡
        if failed_ids:
            clean_failed_records(output_path, mapping["id"])
            UX.info(f"æ¸…ç†äº† {len(failed_ids)} ä¸ªçŸ¥ä¹å¤±è´¥è®°å½•ï¼Œå‡†å¤‡é‡æ–°åˆ†æ")

        # ç­›é€‰å¾…å¤„ç†æ•°æ®ï¼šåªå¤„ç†å®Œå…¨æœªå¤„ç†çš„è®°å½• + å¤±è´¥çš„è®°å½•
        df[mapping["id"]] = df[mapping["id"]].astype(str)
        unprocessed_ids = set(df[mapping["id"]].astype(str)) - (processed_ids - failed_ids)
        df_to_process = df[df[mapping["id"]].astype(str).isin(unprocessed_ids)]
        
        if df_to_process.empty:
            UX.ok("æ‰€æœ‰çŸ¥ä¹å›ç­”å·²å¤„ç†")
            return failed_ids_list
        
        UX.info(f"å¾…å¤„ç†: {len(df_to_process)} æ¡")
        
        # å¹¶å‘å¤„ç†
        api_strategy = self.config.get('api', {}).get('strategy', {})
        max_concurrent = api_strategy.get('max_concurrent_requests', 2)
        semaphore = asyncio.Semaphore(max_concurrent)
        tasks = []
        
        for idx, row in df_to_process.iterrows():
            answer_id = str(row[mapping["id"]])
            tasks.append(self._process_answer(row, mapping, answer_id, semaphore))
        
        all_results = await aio_gather(*tasks)
        
        # å±•å¹³ç»“æœå¹¶æ”¶é›†æ‰€æœ‰è®°å½•ï¼ˆåŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥ï¼‰
        final_results = []
        for result_item in all_results:
            if isinstance(result_item, tuple) and result_item[0] == 'FAILED':
                # å…¼å®¹æ—§ç‰ˆæœ¬çš„å¤±è´¥å¤„ç†ï¼ˆä¸åº”è¯¥å†å‡ºç°ï¼‰
                failed_ids_list.append(f"ID: {result_item[1]} - Reason: {result_item[2]}")
            elif result_item:  # è¿™æ˜¯ä¸€ä¸ªæœ‰å†…å®¹çš„ä»»åŠ¡ï¼ˆæˆåŠŸæˆ–å¤±è´¥ï¼‰
                final_results.extend(result_item)
        
        # ä¿å­˜ç»“æœ
        if final_results:
            df_results = pd.DataFrame(final_results)
            
            if os.path.exists(output_path):
                df_existing = pd.read_excel(output_path)
                df_final = pd.concat([df_existing, df_results], ignore_index=True)
            else:
                df_final = df_results
            
            df_final.to_excel(output_path, index=False)
            UX.ok(f"ä¿å­˜ {len(final_results)} æ¡çŸ¥ä¹è®®é¢˜å•å…ƒåˆ†æç»“æœ")
        
        return failed_ids_list
    
    async def _process_answer(self, row, mapping, answer_id, semaphore):
        """å¤„ç†å•ä¸ªçŸ¥ä¹å›ç­”ï¼ˆæ ¹æ®é•¿åº¦æ™ºèƒ½é€‰æ‹©æ¨¡å¼ï¼‰"""
        async with semaphore:
            answer_text = safe_str_convert(row[mapping["answer_text"]])
            question = safe_str_convert(row[mapping["question"]])
            author = self._get_author_name(row, mapping)
            
            if not answer_text.strip():
                return []
            
            try:
                # æ ¹æ®æ–‡æœ¬é•¿åº¦é€‰æ‹©å¤„ç†æ¨¡å¼ï¼ˆç»Ÿä¸€ä½¿ç”¨tokenè®¡ç®—ï¼‰
                answer_tokens = count_tokens(answer_text)
                if answer_tokens < self.thresholds['ZHIHU_SHORT_TOKEN_THRESHOLD']:
                    # === çŸ­æ–‡æœ¬æ¨¡å¼ï¼šç›´æ¥åˆ†æï¼ˆç±»ä¼¼VKï¼‰ ===
                    UX.info(f"ğŸ”§ çŸ¥ä¹å›ç­” {answer_id} æ¨¡å¼é€‰æ‹©: ({answer_tokens} tokens) â†’ çŸ­æ–‡æœ¬ç›´æ¥åˆ†ææ¨¡å¼")
                    
                    # ä½¿ç”¨æ¨¡æ¿æ„é€ æç¤ºè¯
                    prompt = self.prompts.ZHIHU_SHORT_ANALYSIS.format(
                        question=question,
                        answer_text=answer_text
                    )
                    
                    # æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼šçŸ­æ–‡æœ¬æ¨¡å¼ç›´æ¥ä½¿ç”¨è½»é‡æ¨¡å‹
                    stage_key = 'ZHIHU_ANALYSIS_SHORT'
                    UX.info(f"ğŸ”§ çŸ¥ä¹æ¨¡å‹é€‰æ‹©: ({answer_tokens} tokens < {self.thresholds['ZHIHU_SHORT_TOKEN_THRESHOLD']}) â†’ è½»é‡æ¨¡å‹")
                    
                    result = await self.api_service.call_api_async(
                        prompt, 'zh', stage_key
                    )
                    
                    if result:
                        result['Unit_Text'] = answer_text # ç¡®ä¿Unit_Textå­˜åœ¨
                        final_record = self._finalize_record(result, row, mapping, answer_id)
                        
                        # æ·»åŠ åˆ°Units_collector
                        self.Units_collector.append({
                            'Unit_ID': f"ZH-{answer_id}-1",
                            'Source': 'çŸ¥ä¹',
                            'Question': question,
                            'Answer_Text': answer_text[:500],
                            'Author': author,
                            'AI_Is_Relevant': True
                        })
                        
                        return [final_record] if final_record else None
                    else:
                        # APIå¤±è´¥ï¼Œè¿”å›ç»Ÿä¸€å¤±è´¥è®°å½•
                        failed_record = create_unified_record(ProcessingStatus.API_FAILED, answer_id, 'çŸ¥ä¹', answer_text[:200], 'çŸ¥ä¹çŸ­æ–‡æœ¬åˆ†æå¤±è´¥')
                        failed_record['Unit_Text'] = f'[åˆ†æå¤±è´¥] {answer_text[:100]}...'
                        return [failed_record]
                        
                else:
                    # === é•¿æ–‡æœ¬æ¨¡å¼ï¼šä¸¤æ­¥å¼åˆ†æ ===
                    UX.info(f"ğŸ”§ çŸ¥ä¹å›ç­” {answer_id} æ¨¡å¼é€‰æ‹©: ({answer_tokens} tokens) â†’ ä¸¤æ­¥å¼åˆ†ææ¨¡å¼")
                    
                    # ç¬¬ä¸€æ­¥ï¼šè®®é¢˜å•å…ƒåˆ’åˆ†
                    prompt1 = self.prompts.ZHIHU_CHUNKING.format(full_text=answer_text)
                    result1 = await self.api_service.call_api_async(prompt1, 'zh', 'ZHIHU_CHUNKING')
                    
                    if not result1:
                        failed_record = create_unified_record(ProcessingStatus.API_FAILED, answer_id, 'çŸ¥ä¹', '', 'çŸ¥ä¹åˆ‡åˆ†å¤±è´¥')
                        failed_record['Unit_Text'] = f'[åˆ‡åˆ†å¤±è´¥] {answer_text[:100]}...'
                        return [failed_record]
                    
                    chapters = result1.get('argument_chapters', [])
                    if not chapters:
                        chapters = [{'Unit_Text': answer_text}]
                    
                    # æ·»åŠ åˆ°Units_collector
                    self.Units_collector.append({
                        'Unit_ID': f"ZH-{answer_id}",
                        'Source': 'çŸ¥ä¹',
                        'Question': question,
                        'Answer_Text': answer_text[:500],  # æˆªå–é¢„è§ˆ
                        'Author': author,
                        'AI_Is_Relevant': None  # åç»­æ›´æ–°
                    })
                    
                    # ç¬¬äºŒæ­¥ï¼šå¯¹æ¯ä¸ªè®®é¢˜å•å…ƒè¿›è¡Œåˆ†æ
                    results = []
                    for i, chapter in enumerate(chapters):
                        Unit_Text = chapter.get('Unit_Text', '')
                        if not Unit_Text.strip():
                            continue
                        
                        prompt2 = self.prompts.ZHIHU_ANALYSIS.format(
                            question=question,
                            Unit_Text=Unit_Text
                        )
                        
                        # æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼šæ ¹æ®è®®é¢˜å•å…ƒé•¿åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹
                        unit_tokens = count_tokens(Unit_Text)
                        if unit_tokens < self.thresholds['ZHIHU_SHORT_TOKEN_THRESHOLD']:
                            stage_key = 'ZHIHU_ANALYSIS_SHORT'
                            UX.info(f"ğŸ”§ çŸ¥ä¹è®®é¢˜å•å…ƒæ¨¡å‹é€‰æ‹©: ({unit_tokens} tokens < {self.thresholds['ZHIHU_SHORT_TOKEN_THRESHOLD']}) â†’ è½»é‡æ¨¡å‹")
                        elif unit_tokens > self.thresholds['ZHIHU_LONG_TOKEN_THRESHOLD']:
                            stage_key = 'ZHIHU_ANALYSIS_LONG'
                            UX.info(f"ğŸ”§ çŸ¥ä¹è®®é¢˜å•å…ƒæ¨¡å‹é€‰æ‹©: ({unit_tokens} tokens > {self.thresholds['ZHIHU_LONG_TOKEN_THRESHOLD']}) â†’ é«˜æ€§èƒ½æ¨¡å‹")
                        else:
                            stage_key = 'ZHIHU_ANALYSIS'
                            UX.info(f"ğŸ”§ çŸ¥ä¹è®®é¢˜å•å…ƒæ¨¡å‹é€‰æ‹©: ({unit_tokens} tokens) â†’ æ ‡å‡†æ¨¡å‹")
                        
                        result2 = await self.api_service.call_api_async(prompt2, 'zh', stage_key)
                        
                        if result2:
                            # æ„å»ºå®Œæ•´è®°å½•
                            Unit_record = {
                                "Unit_Text": Unit_Text,
                                "expansion_logic": f"ç¬¬{i+1}ä¸ªè®ºè¯ç« èŠ‚",  
                                **result2  # åŒ…å«æ‰€æœ‰åˆ†æç»´åº¦
                            }
                            
                            final_record = self._finalize_record(Unit_record, row, mapping, answer_id, Unit_index=i + 1)
                            if final_record:
                                results.append(final_record)
                    
                    # æ›´æ–°Units_collectorä¸­çš„ç›¸å…³æ€§
                    for Unit in self.Units_collector:
                        if Unit['Unit_ID'] == f"ZH-{answer_id}":
                            Unit['AI_Is_Relevant'] = bool(results)
                            break
                    
                    if results:
                        return results
                    else:
                        # é•¿æ–‡æœ¬åˆ†æå¤±è´¥ï¼Œè¿”å›ç»Ÿä¸€å¤±è´¥è®°å½•
                        failed_record = create_unified_record(ProcessingStatus.API_FAILED, answer_id, 'çŸ¥ä¹', answer_text[:200], 'çŸ¥ä¹é•¿æ–‡æœ¬åˆ†æå¤±è´¥')
                        failed_record['Unit_Text'] = f'[åˆ†æå¤±è´¥] {answer_text[:100]}...'
                        return [failed_record]
                    
            except Exception as e:
                UX.warn(f"å¤„ç†å›ç­” {answer_id} å¤±è´¥: {str(e)[:100]}")
                # è¿”å›ç»Ÿä¸€å¤±è´¥è®°å½•
                failed_record = create_unified_record(ProcessingStatus.API_FAILED, answer_id, 'çŸ¥ä¹', answer_text[:200], f'å¼‚å¸¸: {str(e)[:50]}')
                failed_record['Unit_Text'] = f'[å¼‚å¸¸] {str(e)[:100]}'
                return [failed_record]

class MediaTextProcessor(BaseProcessor):
    """åª’ä½“æ–‡æœ¬å¤„ç†å™¨ - ä¸¤é˜¶æ®µåˆ†æ"""
    
    async def process_row(self, row_data, output_file_path=None, failed_unit_ids=None):
        """å¤„ç†å•è¡Œæ•°æ®ï¼ˆä¸¤é˜¶æ®µæ¨¡å‹é‡æ„ç‰ˆï¼‰"""
        row = row_data[1]
        # è·å–åª’ä½“æ–‡æœ¬ä¸“ç”¨çš„åˆ—æ˜ å°„é…ç½®
        media_config = self.config.get('media_text', {})
        COLUMN_MAPPING = media_config.get('column_mapping', {})
        original_id = safe_str_convert(row.get(COLUMN_MAPPING.get("ID", "åºå·")))
        source = row_data[2]
        
        # è·å–å¤±è´¥çš„å•å…ƒIDï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
        failed_units = set()
        if failed_unit_ids and isinstance(failed_unit_ids, dict):
            failed_units = set(failed_unit_ids.get(original_id, set()))

        try:
            full_text = safe_str_convert(row.get(COLUMN_MAPPING.get("MEDIA_TEXT", "text"), ''))
            article_title = safe_str_convert(row.get(COLUMN_MAPPING.get('MEDIA_TITLE', "æ ‡é¢˜"), 'æ— æ ‡é¢˜'))
            
            if not full_text.strip():
                UX.warn(f"ID {original_id}: æ–‡ç« å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                no_relevant_record = create_unified_record(ProcessingStatus.NO_RELEVANT, original_id, source)
                return original_id, [no_relevant_record]

            # æ£€æŸ¥æ–‡æœ¬é•¿åº¦é™åˆ¶
            language, config = detect_language_and_get_config(full_text, self.config)
            text_tokens = count_tokens(full_text)
            if text_tokens > config['MAX_SINGLE_TEXT']:
                UX.warn(f"ID {original_id}: æ–‡æœ¬è¿‡é•¿({text_tokens} tokens)ï¼Œè·³è¿‡å¤„ç†")
                failed_record = create_unified_record(
                    ProcessingStatus.API_FAILED, original_id, source, 
                    full_text[:200], f"æ–‡æœ¬è¿‡é•¿({text_tokens} tokens)"
                )
                return original_id, [failed_record]
            
            UX.info(f"ğŸš€ ID {original_id}: å¼€å§‹ä¸¤é˜¶æ®µå¤„ç† ({text_tokens} tokens, {language})")

            # ç¬¬ä¸€é˜¶æ®µï¼šè®®é¢˜å•å…ƒæå–
            UX.info(f"ğŸ“‹ ID {original_id}: [1/2] è®®é¢˜å•å…ƒæå–é˜¶æ®µ - ä½¿ç”¨æ¨¡å‹: {self.get_stage_model('UNIT_EXTRACTION')}")

            extraction_prompt = self.prompts.UNIT_EXTRACTION.replace("{full_text}", full_text)
            
            extraction_result = await self.api_service.get_analysis(
                extraction_prompt, 'analyzed_Units', language,
                model_name=self.get_stage_model('UNIT_EXTRACTION'), 
                stage_key='UNIT_EXTRACTION',
                context_label=f"{original_id}:EXTRACTION"
            )
            
            if extraction_result is None:
                UX.warn(f"âŒ ID {original_id}: [1/2] ç¬¬ä¸€é˜¶æ®µè®®é¢˜å•å…ƒæå–å¤±è´¥")
                failed_record = create_unified_record(
                    ProcessingStatus.API_FAILED, original_id, source, 
                    full_text[:200], "ç¬¬ä¸€é˜¶æ®µè®®é¢˜å•å…ƒæå–å¤±è´¥"
                )
                return original_id, [failed_record]
            
            if not isinstance(extraction_result, list) or not extraction_result:
                UX.info(f"ğŸ“ ID {original_id}: [1/2] å®Œæˆï¼Œä½†æœªæå–åˆ°ä»»ä½•è®®é¢˜å•å…ƒ")
                no_relevant_record = create_unified_record(ProcessingStatus.NO_RELEVANT, original_id, source)
                return original_id, [no_relevant_record]
            
            UX.ok(f"âœ… ID {original_id}: [1/2] å®Œæˆï¼Œæå–åˆ° {len(extraction_result)} ä¸ªè®®é¢˜å•å…ƒ")

            # ç¬¬äºŒé˜¶æ®µï¼šå•å…ƒæ·±åº¦åˆ†æ
            UX.info(f"ğŸ” ID {original_id}: [2/2] å•å…ƒæ·±åº¦åˆ†æé˜¶æ®µ - å¤„ç† {len(extraction_result)} ä¸ªå•å…ƒ")
            final_data = []
            
            for i, unit_data in enumerate(extraction_result, 1):
                unit_id = f"{original_id}-Unit-{i}"
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡å·²æˆåŠŸå¤„ç†çš„å•å…ƒï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
                if failed_units and unit_id not in failed_units:
                    UX.info(f"â­ï¸  {unit_id}: è·³è¿‡å·²æˆåŠŸå¤„ç†çš„å•å…ƒ")
                    continue
                        
                unit_text = safe_str_convert(unit_data.get('Unit_Text', '')).strip()
                if not unit_text:
                    UX.warn(f"âš ï¸  {unit_id}: è®®é¢˜å•å…ƒæ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                                
                seed_sentence = safe_str_convert(unit_data.get('seed_sentence', ''))
                expansion_logic = safe_str_convert(unit_data.get('expansion_logic', ''))
                unit_speaker = safe_str_convert(unit_data.get('speaker', identify_source(source)))
                
                UX.info(f"ğŸ“ {unit_id}: å¼€å§‹åˆ†æ (å‘è¨€äºº: {unit_speaker[:20]}{'...' if len(unit_speaker) > 20 else ''})")
                
                analysis_prompt = self.prompts.UNIT_ANALYSIS.replace("{speaker}", unit_speaker).replace("{unit_text}", unit_text)
                
                analysis_result = await self.api_service.get_analysis(
                    analysis_prompt, None, language,
                    model_name=self.get_stage_model('UNIT_ANALYSIS'), 
                    stage_key='UNIT_ANALYSIS',
                    context_label=f"{unit_id}:ANALYSIS"
                )
                
                if analysis_result is None:
                    UX.warn(f"âŒ {unit_id}: ç¬¬äºŒé˜¶æ®µåˆ†æå¤±è´¥")
                    failed_unit = {
                        "processing_status": ProcessingStatus.API_FAILED,
                        "Source": source,
                        "Unit_ID": unit_id,
                        "speaker": "API_CALL_FAILED",
                        "Unit_Text": f"[API_FAILED] å•å…ƒ {unit_id} ç¬¬äºŒé˜¶æ®µåˆ†æå¤±è´¥: {unit_text[:200]}...",
                        "seed_sentence": seed_sentence,
                        "expansion_logic": expansion_logic,
                        "Unit_Hash": "",
                        "Incident": "API_CALL_FAILED",
                        "Frame_SolutionRecommendation": "[]",
                        "Frame_ResponsibilityAttribution": "[]",
                        "Frame_CausalExplanation": "[]",
                        "Frame_MoralEvaluation": "[]",
                        "Frame_ProblemDefinition": "[]",
                        "Frame_ActionStatement": "[]",
                        "Valence": "API_CALL_FAILED",
                        "Evidence_Type": "API_CALL_FAILED",
                        "Attribution_Level": "API_CALL_FAILED",
                        "Temporal_Focus": "API_CALL_FAILED",
                        "Primary_Actor_Type": "API_CALL_FAILED",
                        "Geographic_Scope": "API_CALL_FAILED",
                        "Relationship_Model_Definition": "API_CALL_FAILED",
                        "Discourse_Type": "API_CALL_FAILED"
                    }
                    final_data.append(failed_unit)
                    continue
                            
                # åˆå¹¶ç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µçš„ç»“æœ
                norm_text = normalize_text(unit_text)
                unit_hash = hashlib.sha256(norm_text.encode('utf-8')).hexdigest()
                
                complete_unit = {
                    "Unit_ID": unit_id,
                    "Source": source,
                    "speaker": unit_speaker,
                    "Unit_Text": unit_text,
                    "seed_sentence": seed_sentence,
                    "expansion_logic": expansion_logic,
                    "Unit_Hash": unit_hash,
                    "processing_status": ProcessingStatus.SUCCESS, 
                    "Incident": analysis_result.get("Incident", ""),
                    "Frame_SolutionRecommendation": analysis_result.get("Frame_SolutionRecommendation", []),
                    "Frame_ResponsibilityAttribution": analysis_result.get("Frame_ResponsibilityAttribution", []),
                    "Frame_CausalExplanation": analysis_result.get("Frame_CausalExplanation", []),
                    "Frame_MoralEvaluation": analysis_result.get("Frame_MoralEvaluation", []),
                    "Frame_ProblemDefinition": analysis_result.get("Frame_ProblemDefinition", []),
                    "Frame_ActionStatement": analysis_result.get("Frame_ActionStatement", []),
                    "Valence": analysis_result.get("Valence", ""),
                    "Evidence_Type": analysis_result.get("Evidence_Type", ""),
                    "Attribution_Level": analysis_result.get("Attribution_Level", ""),
                    "Temporal_Focus": analysis_result.get("Temporal_Focus", ""),
                    "Primary_Actor_Type": analysis_result.get("Primary_Actor_Type", ""),
                    "Geographic_Scope": analysis_result.get("Geographic_Scope", ""),
                    "Relationship_Model_Definition": analysis_result.get("Relationship_Model_Definition", ""),
                    "Discourse_Type": analysis_result.get("Discourse_Type", "")
                }
                
                final_data.append(complete_unit)
                UX.ok(f"âœ… {unit_id}: åˆ†æå®Œæˆ")

            if not final_data:
                UX.info(f"ğŸ“ ID {original_id}: [2/2] å®Œæˆï¼Œä½†æ²¡æœ‰æˆåŠŸå¤„ç†çš„è®®é¢˜å•å…ƒ")
                no_relevant_record = create_unified_record(ProcessingStatus.NO_RELEVANT, original_id, source)
                return original_id, [no_relevant_record]

            UX.ok(f"ğŸ‰ ID {original_id}: [2/2] å®Œæˆï¼å…±ç”Ÿæˆ {len(final_data)} ä¸ªè®®é¢˜å•å…ƒ")
            return original_id, final_data

        except Exception as e:
            UX.err(f"å¤„ç†ID {original_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            failed_record = create_unified_record(
                ProcessingStatus.API_FAILED, original_id, source, 
                "", f"é”™è¯¯: {str(e)[:100]}"
            )
            return original_id, [failed_record]
    
    
    def get_stage_model(self, stage_key):
        """è·å–é˜¶æ®µæ¨¡å‹"""
        try:
            # ä¼˜å…ˆå°è¯•ä»åª’ä½“æ–‡æœ¬ä¸“ç”¨æ¨¡å‹æ± è·å–
            media_config = self.config.get('media_text', {})
            model_pools = media_config.get('model_pools', {})
            if model_pools and 'primary_models' in model_pools:
                return model_pools['primary_models'].get(stage_key)
            
            # å¤‡ç”¨ï¼šå°è¯•ä»APIé€šç”¨æ¨¡å‹é…ç½®è·å–
            api_config = self.config.get('api', {})
            models = api_config.get('models', {})
            stage_mapping = {
                'UNIT_EXTRACTION': 'media_text_extraction',
                'UNIT_ANALYSIS': 'media_text_analysis'
            }
            model_key = stage_mapping.get(stage_key, stage_key.lower())
            if model_key in models:
                return models[model_key]
            
            # å‘åå…¼å®¹ï¼šå°è¯•æ—§çš„é…ç½®ç»“æ„
            MODEL_POOLS = self.config.get('model_pools', {})
            if MODEL_POOLS and 'primary_models' in MODEL_POOLS:
                return MODEL_POOLS['primary_models'][stage_key]
            # ä½¿ç”¨ç»Ÿä¸€çš„åª’ä½“æ–‡æœ¬é…ç½®
            media_config = self.config.get('media_text', {})
            model_pools = media_config.get('model_pools', {})
            return model_pools.get('primary_models', {})[stage_key]
        except Exception:
            raise ValueError(f"æœªé…ç½®é˜¶æ®µæ¨¡å‹: {stage_key}")