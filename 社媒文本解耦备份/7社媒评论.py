# -*- coding: utf-8 -*-
"""
==============================================================================
ç¤¾äº¤åª’ä½“è®®é¢˜å•å…ƒåˆ†æå™¨ - ä¸åª’ä½“æ–‡æœ¬åŠŸèƒ½å¯¹é½ç‰ˆ
==============================================================================
"""
import os
import re
import json
import time
import pandas as pd
import asyncio
import aiohttp
import hashlib
import yaml
import tiktoken
from datetime import datetime
from tqdm.asyncio import tqdm as aio_tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import contextlib

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# åŠ è½½å¤–éƒ¨é…ç½®æ–‡ä»¶
def load_config():
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    yaml_path = os.path.join(BASE_DIR, "config.yaml")
    
    if not os.path.exists(yaml_path):
        print(f"âŒ YAMLé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {yaml_path}")
        print("è¯·ç¡®ä¿config.yamlæ–‡ä»¶å­˜åœ¨å¹¶åŒ…å«æ‰€æœ‰å¿…éœ€çš„å‚æ•°ã€‚")
        raise FileNotFoundError(f"YAMLé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {yaml_path}")
    
    try:
        with open(yaml_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            print(f"âœ… æˆåŠŸåŠ è½½YAMLé…ç½®æ–‡ä»¶: {yaml_path}")
            return config
    except yaml.YAMLError as e:
        print(f"âŒ YAMLé…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥YAMLæ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚")
        raise ValueError(f"YAMLé…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ åŠ è½½YAMLé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        raise FileNotFoundError(f"åŠ è½½YAMLé…ç½®æ–‡ä»¶å¤±è´¥: {e}")

# å…¨å±€é…ç½®
CONFIG = load_config()

class UX:
    """ç»Ÿä¸€çš„ç”¨æˆ·äº¤äº’ç®¡ç†å™¨"""
    RUN_T0 = time.perf_counter()

    @staticmethod
    def start_run():
        UX.RUN_T0 = time.perf_counter()

    @staticmethod
    def _ts():
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    @staticmethod
    def _elapsed():
        return time.perf_counter() - UX.RUN_T0

    @staticmethod
    def _elapsed_str():
        total = int(UX._elapsed())
        hours, rem = divmod(total, 3600)
        minutes, seconds = divmod(rem, 60)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    @staticmethod
    def phase(title):
        print(f"\n=== [{UX._ts()}][{UX._elapsed_str()}] {title} ===")

    @staticmethod
    def info(msg):
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [i] {msg}")

    @staticmethod
    def ok(msg):
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [OK] {msg}")

    @staticmethod
    def warn(msg):
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [!] {msg}")

    @staticmethod
    def err(msg):
        print(f"[{UX._ts()}][{UX._elapsed_str()}] [!!!] {msg}")

# ==============================================================================
# === ğŸ›ï¸ æ ¸å¿ƒå‚æ•°é…ç½®åŒºï¼ˆå·²å¤–éƒ¨åŒ–ï¼‰
# ==============================================================================

# ä»å¤–éƒ¨é…ç½®æ–‡ä»¶åŠ è½½æ‰€æœ‰å‚æ•°
INPUT_PATH = CONFIG['INPUT_PATH']
OUTPUT_PATH = CONFIG['OUTPUT_PATH']
RELIABILITY_TEST_MODE = CONFIG['RELIABILITY_TEST_MODE']
RELIABILITY_SAMPLING_CONFIG = CONFIG['RELIABILITY_SAMPLING_CONFIG']
API_CONFIG = CONFIG['API_CONFIG']
MAX_CONCURRENT_REQUESTS = CONFIG['MAX_CONCURRENT_REQUESTS']
API_RETRY_ATTEMPTS = CONFIG['API_RETRY_ATTEMPTS']
RATE_LIMIT_BASE_DELAY = CONFIG['RATE_LIMIT_BASE_DELAY']
SKIP_FAILED_TEXTS = CONFIG['SKIP_FAILED_TEXTS']
VK_LONG_TEXT_THRESHOLD = CONFIG['VK_LONG_TEXT_THRESHOLD']
ZHIHU_SHORT_TOKEN_THRESHOLD = CONFIG['ZHIHU_SHORT_TOKEN_THRESHOLD']
ZHIHU_LONG_TOKEN_THRESHOLD = CONFIG['ZHIHU_LONG_TOKEN_THRESHOLD']
LANGUAGE_CONFIGS = CONFIG['LANGUAGE_CONFIGS']
COLUMN_MAPPING = CONFIG['COLUMN_MAPPING']

# å¤„ç†çŠ¶æ€å®šä¹‰
class ProcessingStatus:
    SUCCESS = "SUCCESS"
    NO_RELEVANT = "NO_RELEVANT"
    API_FAILED = "API_FAILED"

# åˆ†æåˆ—å®šä¹‰ï¼ˆV2æ ¼å¼ - ç›´æ¥ä½¿ç”¨æ–°çš„æ¡†æ¶å­—æ®µï¼‰
ANALYSIS_COLUMNS = ['Source', 'Incident', 'relevance', 'speaker',
                   'Frame_ProblemDefinition', 
                   'Frame_ResponsibilityAttribution',
                   'Frame_MoralEvaluation',
                   'Frame_SolutionRecommendation',
                   'Frame_ActionStatement',
                   'Frame_CausalExplanation',
                   'Valence', 'Evidence_Type', 'Attribution_Level',
                   'Temporal_Focus', 'Primary_Actor_Type', 'Geographic_Scope',
                   'Relationship_Model_Definition', 'Discourse_Type']

# ==============================================================================
# === ğŸ”§ æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
# ==============================================================================

# Tokenè®¡ç®—å™¨åˆå§‹åŒ–
_tokenizer = None

def get_tokenizer():
    """è·å–tokenè®¡ç®—å™¨ï¼Œä½¿ç”¨å•ä¾‹æ¨¡å¼"""
    global _tokenizer
    if _tokenizer is None:
        try:
            _tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            UX.err(f"åˆå§‹åŒ–tokenizerå¤±è´¥: {e}")
            raise
    return _tokenizer

def count_tokens(text: str) -> int:
    """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
    if not isinstance(text, str) or not text.strip():
        return 0
    try:
        tokenizer = get_tokenizer()
        return len(tokenizer.encode(text))
    except Exception as e:
        UX.warn(f"è®¡ç®—tokenæ•°å¤±è´¥: {e}")
        # é™çº§ç­–ç•¥ï¼šä½¿ç”¨å­—ç¬¦æ•°çš„ç²—ç•¥ä¼°ç®—
        return len(text) // 2  # ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 2 å­—ç¬¦

class Utils:
    """ç»Ÿä¸€å·¥å…·å‡½æ•°ç±»"""
    
    @staticmethod
    def safe_str_convert(value):
        """å®‰å…¨å­—ç¬¦ä¸²è½¬æ¢"""
        if pd.isna(value) or value is None:
            return ''
        return str(value)

    @staticmethod
    def normalize_text(text):
        """æ–‡æœ¬è§„èŒƒåŒ–"""
        if not isinstance(text, str):
            text = Utils.safe_str_convert(text)
        return re.sub(r"\s+", " ", text).strip()

    @staticmethod
    def detect_language(text):
        """æ£€æµ‹è¯­è¨€"""
        if re.search(r'[\u4e00-\u9fa5]', text):
            return 'zh'
        elif re.search(r'[\u0400-\u04FF]', text):
            return 'ru'
        return 'en'

    @staticmethod
    def get_language_config(language):
        """è·å–è¯­è¨€å¯¹åº”çš„å¤„ç†é…ç½®"""
        return LANGUAGE_CONFIGS.get(language, LANGUAGE_CONFIGS['en'])

    @staticmethod
    def detect_file_type(df):
        """æ£€æµ‹æ–‡ä»¶ç±»å‹ï¼ˆVKæˆ–çŸ¥ä¹ï¼‰"""
        vk_columns = set(COLUMN_MAPPING['vk'].values())
        zhihu_required = {'åºå·', 'çŸ¥ä¹é—®é¢˜æ ‡é¢˜åŠæè¿°', 'å›ç­”å†…å®¹'}
        df_columns = set(df.columns)
        
        if vk_columns.issubset(df_columns):
            return 'vk'
        elif zhihu_required.issubset(df_columns):
            return 'zhihu'
        return None

    @staticmethod
    def identify_source(filename):
        """è¯†åˆ«ä¿¡æº"""
        filename_lower = filename.lower()
        if 'vk' in filename_lower:
            return 'vk'
        elif 'çŸ¥ä¹' in filename or 'zhihu' in filename_lower:
            return 'çŸ¥ä¹'
        return 'æœªçŸ¥æ¥æº'
    
    @staticmethod
    def safe_json_parse(json_str, default=None):
        """å®‰å…¨è§£æJSONå­—ç¬¦ä¸²"""
        if default is None:
            default = []
        if pd.isna(json_str) or json_str is None or json_str == '':
            return default
        if isinstance(json_str, list):
            return json_str
        if isinstance(json_str, str):
            try:
                return json.loads(json_str)
            except (json.JSONDecodeError, TypeError):
                return default
        return default
    
    @staticmethod
    def safe_json_dumps(obj, ensure_ascii=True):
        """å®‰å…¨åºåˆ—åŒ–JSONå¯¹è±¡"""
        if obj is None or (isinstance(obj, list) and len(obj) == 0):
            return '[]'
        try:
            return json.dumps(obj, ensure_ascii=ensure_ascii)
        except (TypeError, ValueError):
            return '[]'
    

# ä¿æŒå‘åå…¼å®¹æ€§çš„åˆ«å
safe_str_convert = Utils.safe_str_convert
normalize_text = Utils.normalize_text
detect_language = Utils.detect_language
get_language_config = Utils.get_language_config
detect_file_type = Utils.detect_file_type
identify_source = Utils.identify_source

def create_unified_record(record_type, id_value, source="æœªçŸ¥æ¥æº", text_snippet="", failure_reason=""):
    """åˆ›å»ºç»Ÿä¸€æ ¼å¼è®°å½•ï¼ˆä¸åª’ä½“æ–‡æœ¬ä¸€è‡´ï¼‰"""
    base = {
        "processing_status": record_type,
        "Source": source,
        "Unit_ID": f"{id_value}-{record_type}"
    }
    
    # æ¡†æ¶å’Œç»´åº¦å­—æ®µï¼ˆä¸Promptsç±»ä¿æŒä¸€è‡´ï¼‰
    frames = ["ProblemDefinition", "ResponsibilityAttribution", "MoralEvaluation",
              "SolutionRecommendation", "ActionStatement", "CausalExplanation"]
    dims = ["Valence", "Evidence_Type", "Attribution_Level", "Temporal_Focus",
            "Primary_Actor_Type", "Geographic_Scope", "Relationship_Model_Definition", 
            "Discourse_Type"]
    
    if record_type == ProcessingStatus.API_FAILED:
        base.update({
            "Unit_Text": f"[API_FAILED] {failure_reason}: {text_snippet[:200]}...",
            "speaker": "API_CALL_FAILED",
            "Incident": "API_CALL_FAILED",
            "relevance": "API_FAILED",  # VKç‰¹æœ‰å­—æ®µ
            **{f"Frame_{f}": [] for f in frames},
            **{d: "API_CALL_FAILED" for d in dims}
        })
    elif record_type == ProcessingStatus.NO_RELEVANT:
        base.update({
            "Unit_Text": "[æ— ç›¸å…³å†…å®¹]",
            "speaker": "NO_RELEVANT_CONTENT",
            "Incident": "NO_RELEVANT_CONTENT",
            "relevance": "ä¸ç›¸å…³",  # VKç‰¹æœ‰å­—æ®µ
            **{f"Frame_{f}": [] for f in frames},
            **{d: "NO_RELEVANT_CONTENT" for d in dims}
        })
    
    return base

def clean_failed_records(output_path, id_column):
    """æ¸…ç†å¤±è´¥è®°å½•ï¼ˆä¸åª’ä½“æ–‡æœ¬å¯¹é½ï¼‰"""
    if not os.path.exists(output_path):
        return set()
    
    try:
        df = pd.read_excel(output_path)
        if df.empty:
            return set()
        
        if 'processing_status' in df.columns:
            # åªæ¸…ç†API_FAILEDçš„è®°å½•ï¼Œä¿ç•™NO_RELEVANTè®°å½•
            failed_mask = df['processing_status'] == ProcessingStatus.API_FAILED
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬
            failed_mask = df['speaker'].astype(str).str.contains('API_CALL_FAILED', na=False) if 'speaker' in df.columns else pd.Series([False] * len(df))
        
        failed_ids = set(df[failed_mask][id_column].astype(str).unique())
        if failed_ids:
            df_clean = df[~failed_mask]
            df_clean.to_excel(output_path, index=False)
            
            if 'processing_status' in df_clean.columns:
                success_count = (df_clean['processing_status'] == ProcessingStatus.SUCCESS).sum()
                no_relevant_count = (df_clean['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                UX.info(f"ä¿ç•™äº† {success_count} æ¡æˆåŠŸè®°å½•, {no_relevant_count} æ¡æ— ç›¸å…³è®°å½•")
            
            UX.info(f"æ¸…ç†äº† {len(failed_ids)} ä¸ªAPIå¤±è´¥è®°å½•ï¼Œå‰©ä½™ {len(df_clean)} æ¡")
        
        return failed_ids
    except Exception as e:
        UX.warn(f"æ¸…ç†å¤±è´¥è®°å½•æ—¶å‡ºé”™: {e}")
        return set()

def get_processing_state(df, id_col):
    """ç»Ÿä¸€çš„çŠ¶æ€æ£€æŸ¥ï¼šè¿”å›(æˆåŠŸIDé›†åˆ, å¤±è´¥IDé›†åˆ)ï¼ˆä¸åª’ä½“æ–‡æœ¬ä¸€è‡´ï¼‰"""
    if df is None or df.empty or id_col not in df.columns:
        return set(), set()
    
    status_col = 'processing_status'
    try:
        if status_col in df.columns:
            # SUCCESSå’ŒNO_RELEVANTéƒ½ç®—æˆåŠŸï¼Œåªæœ‰API_FAILEDéœ€è¦é‡æ–°å¤„ç†
            success = df[df[status_col].isin([ProcessingStatus.SUCCESS, ProcessingStatus.NO_RELEVANT])][id_col]
            failed = df[df[status_col] == ProcessingStatus.API_FAILED][id_col]
        else:
            # å…¼å®¹æ—§ç‰ˆ
            speaker_col = 'speaker'
            if speaker_col in df.columns:
                success = df[~df[speaker_col].astype(str).str.contains('API_CALL_FAILED', na=False)][id_col]
                failed = df[df[speaker_col].astype(str).str.contains('API_CALL_FAILED', na=False)][id_col]
            else:
                return set(), set()
        
        return set(success.astype(str)), set(failed.astype(str))
    except Exception:
        return set(), set()

def get_failed_batch_ids(post_id, output_file_path):
    """è·å–æŒ‡å®šå¸–å­ä¸­å¤±è´¥çš„æ‰¹å¤„ç†IDï¼ˆç±»ä¼¼åª’ä½“æ–‡æœ¬çš„get_failed_macro_chunk_idsï¼‰"""
    if not os.path.exists(output_file_path):
        return set()
    
    try:
        df_output = pd.read_excel(output_file_path)
        if df_output.empty:
            return set()
        
        # ç­›é€‰æŒ‡å®šå¸–å­çš„è®°å½•
        if 'Post_ID' in df_output.columns:
            post_records = df_output[df_output['Post_ID'].astype(str) == str(post_id)]
        else:
            return set()
        
        if post_records.empty:
            return set()
        
        # è·å–å¤±è´¥çš„è®°å½•
        failed_mask = post_records['processing_status'] == ProcessingStatus.API_FAILED
        failed_records = post_records[failed_mask]
        
        if failed_records.empty:
            return set()
        
        # è·å–æ‰¹å¤„ç†IDï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        failed_batch_ids = set()
        if 'Batch_ID' in failed_records.columns:
            failed_batch_ids = set(failed_records['Batch_ID'].dropna().astype(str))
        
        return failed_batch_ids
    except Exception as e:
        UX.warn(f"è·å–å¤±è´¥æ‰¹å¤„ç†IDå¤±è´¥: {e}")
        return set()

# ==============================================================================
# === ğŸ¤– APIæœåŠ¡
# ==============================================================================

class APIService:
    """ç»Ÿä¸€çš„APIæœåŠ¡"""
    
    def __init__(self, session=None):
        self.session = session
        self.call_count = 0
        self.success_count = 0
        self.failure_count = 0
    
    def _create_payload(self, prompt, stage_key):
        """åˆ›å»ºAPIè¯·æ±‚è´Ÿè½½"""
        return {
            "model": API_CONFIG["STAGE_MODELS"].get(stage_key, "[å®˜è‡ª-0.7]gemini-2-5-flash"),
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0,
            "response_format": {"type": "json_object"}
        }
    
    def _extract_json_response(self, content):
        """ä»å“åº”ä¸­æå–JSONï¼ˆæ”¯æŒå¯¹è±¡å’Œæ•°ç»„ï¼‰"""
        # å°è¯•æ‰¾åˆ°JSONæ•°ç»„
        first_bracket = content.find('[')
        last_bracket = content.rfind(']')
        
        # å°è¯•æ‰¾åˆ°JSONå¯¹è±¡
        first_brace = content.find('{')
        last_brace = content.rfind('}')
        
        # åˆ¤æ–­å“ªä¸ªåœ¨å‰ï¼ˆä¼˜å…ˆå¤„ç†æœ€å¤–å±‚çš„ç»“æ„ï¼‰
        if first_bracket >= 0 and (first_brace < 0 or first_bracket < first_brace):
            # æ˜¯æ•°ç»„
            if last_bracket > first_bracket:
                try:
                    return json.loads(content[first_bracket:last_bracket+1])
                except json.JSONDecodeError:
                    pass
        
        if first_brace >= 0 and last_brace > first_brace:
            # æ˜¯å¯¹è±¡
            try:
                return json.loads(content[first_brace:last_brace+1])
            except json.JSONDecodeError:
                pass
        
        # æœ€åå°è¯•ç›´æ¥è§£ææ•´ä¸ªå†…å®¹
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            raise ValueError(f"æ— æ³•è§£æJSON: {content[:200]}...")
    
    def print_statistics(self):
        """æ‰“å°APIè°ƒç”¨ç»Ÿè®¡"""
        if self.call_count > 0:
            success_rate = (self.success_count / self.call_count) * 100
            UX.info(f"APIç»Ÿè®¡ - æ€»è°ƒç”¨: {self.call_count}, æˆåŠŸ: {self.success_count}, "
                    f"å¤±è´¥: {self.failure_count}, æˆåŠŸç‡: {success_rate:.1f}%")
    
    async def call_api_async(self, prompt, language='zh', stage_key=None):
        """å¼‚æ­¥APIè°ƒç”¨"""
        self.call_count += 1
        config = LANGUAGE_CONFIGS.get(language, LANGUAGE_CONFIGS['zh'])
        
        for attempt in range(API_RETRY_ATTEMPTS):
            try:
                url = f"{API_CONFIG['BASE_URL']}/chat/completions"
                headers = {"Content-Type": "application/json", "Authorization": f"Bearer {API_CONFIG['API_KEYS'][0]}"}
                payload = self._create_payload(prompt, stage_key)
                
                
                async with self.session.post(url, headers=headers, json=payload,
                                            timeout=aiohttp.ClientTimeout(total=config['TIMEOUT'])) as response:
                    response.raise_for_status()
                    result = json.loads(await response.text())
                    content = result['choices'][0]['message']['content']
                    
                    result = self._extract_json_response(content)
                    self.success_count += 1
                    return result
                    
            except Exception as e:
                if attempt < API_RETRY_ATTEMPTS - 1:
                    await asyncio.sleep(RATE_LIMIT_BASE_DELAY * (2 ** attempt))
                else:
                    self.failure_count += 1
                    if SKIP_FAILED_TEXTS:
                        UX.warn(f"APIå¤±è´¥: {str(e)[:100]}")
                        return None
                    raise
        return None

    def call_api_sync(self, prompt, language='zh', stage_key=None):
        """åŒæ­¥APIè°ƒç”¨"""
        import requests
        self.call_count += 1
        config = LANGUAGE_CONFIGS.get(language, LANGUAGE_CONFIGS['zh'])
        
        for attempt in range(API_RETRY_ATTEMPTS):
            try:
                url = f"{API_CONFIG['BASE_URL']}/chat/completions"
                headers = {"Content-Type": "application/json", "Authorization": f"Bearer {API_CONFIG['API_KEYS'][0]}"}
                payload = self._create_payload(prompt, stage_key)
                
                
                response = requests.post(url, headers=headers, json=payload, timeout=config['TIMEOUT'])
                response.raise_for_status()
                content = response.json()['choices'][0]['message']['content']
                
                result = self._extract_json_response(content)
                self.success_count += 1
                return result
                
            except Exception as e:
                if attempt < API_RETRY_ATTEMPTS - 1:
                    time.sleep(RATE_LIMIT_BASE_DELAY * (2 ** attempt))
                else:
                    self.failure_count += 1
                    if SKIP_FAILED_TEXTS:
                        UX.warn(f"APIå¤±è´¥: {str(e)[:100]}")
                        return None
                    raise
        return None

# ==============================================================================
# === ğŸ¤– æç¤ºè¯æ¨¡æ¿
# ==============================================================================

class Prompts:
    """æç¤ºè¯ç®¡ç†ç±» - ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½"""
    
    def __init__(self):
        self.prompts_dir = BASE_DIR
        self._load_prompts()
    
    def _load_prompts(self):
        """ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½æ‰€æœ‰æç¤ºè¯"""
        prompt_files = {
            'VK_BATCH_ANALYSIS': 'VK_BATCH_ANALYSIS.txt',
            'ZHIHU_CHUNKING': 'ZHIHU_CHUNKING.txt', 
            'ZHIHU_ANALYSIS': 'ZHIHU_ANALYSIS.txt',
            'ZHIHU_SHORT_ANALYSIS': 'ZHIHU_SHORT_ANALYSIS.txt'
        }
        
        for attr_name, filename in prompt_files.items():
            file_path = os.path.join(self.prompts_dir, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                setattr(self, attr_name, content)
            except FileNotFoundError:
                UX.warn(f"æç¤ºè¯æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å†…å®¹")
                setattr(self, attr_name, f"# æç¤ºè¯æ–‡ä»¶ {filename} æœªæ‰¾åˆ°")
            except Exception as e:
                UX.warn(f"åŠ è½½æç¤ºè¯æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                setattr(self, attr_name, f"# æç¤ºè¯æ–‡ä»¶ {filename} åŠ è½½å¤±è´¥: {e}")

# åˆ›å»ºå…¨å±€æç¤ºè¯å®ä¾‹
prompts = Prompts()
# ==============================================================================
# === âš™ï¸ å¤„ç†å™¨
# ==============================================================================

class BaseProcessor:
    """åŸºç¡€å¤„ç†å™¨ç±»"""
    
    def __init__(self, api_service):
        self.api_service = api_service
        self.Units_collector = []
    
    def _add_hash_to_record(self, record, text_field='Unit_Text'):
        """ä¸ºè®°å½•æ·»åŠ å“ˆå¸Œå€¼"""
        if text_field in record:
            norm = normalize_text(record[text_field])
            if norm:
                record['Unit_Hash'] = hashlib.sha256(norm.encode('utf-8')).hexdigest()
        return record
    
    def _save_progress_generic(self, df_to_process, output_path, id_column):
        """é€šç”¨ä¿å­˜è¿›åº¦æ–¹æ³•ï¼ˆä¸åª’ä½“æ–‡æœ¬å¯¹é½ï¼‰"""
        if df_to_process.empty:
            return
        
        # åªä¿å­˜æˆåŠŸå’Œæ— ç›¸å…³çš„è®°å½•
        if 'processing_status' in df_to_process.columns:
            save_mask = df_to_process['processing_status'].isin([
                ProcessingStatus.SUCCESS, 
                ProcessingStatus.NO_RELEVANT
            ])
            df_to_save = df_to_process[save_mask].copy()
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šæ’é™¤API_CALL_FAILED
            if 'speaker' in df_to_process.columns:
                save_mask = ~df_to_process['speaker'].astype(str).str.contains('API_CALL_FAILED', na=False)
                df_to_save = df_to_process[save_mask].copy()
            else:
                df_to_save = df_to_process.copy()
        
        if df_to_save.empty:
            return
        
        # ç²¾ç¡®åˆå¹¶æˆ–åˆ›å»ºæ–‡ä»¶
        if os.path.exists(output_path):
            try:
                df_existing = pd.read_excel(output_path)
                if not df_existing.empty and id_column in df_existing.columns:
                    # è·å–å·²æˆåŠŸå¤„ç†çš„IDï¼ˆä¸åŒ…æ‹¬å¤±è´¥çš„ï¼‰
                    success_existing_ids, _ = get_processing_state(df_existing, id_column)
                    
                    # åªæ·»åŠ æ–°çš„æˆåŠŸè®°å½•ï¼Œä¸è¦†ç›–å·²æˆåŠŸçš„
                    new_mask = ~df_to_save[id_column].astype(str).isin(success_existing_ids)
                    df_new = df_to_save[new_mask]
                    
                    if not df_new.empty:
                        df_final = pd.concat([df_existing, df_new], ignore_index=True)
                        UX.info(f"æ·»åŠ äº† {len(df_new)} æ¡æ–°è®°å½•åˆ°ç°æœ‰ {len(df_existing)} æ¡")
                    else:
                        df_final = df_existing
                        UX.info(f"æ— æ–°è®°å½•éœ€è¦æ·»åŠ ï¼Œä¿æŒç°æœ‰ {len(df_existing)} æ¡")
                else:
                    df_final = df_to_save
            except Exception as e:
                UX.warn(f"è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨æ–°æ•°æ®")
                df_final = df_to_save
        else:
            df_final = df_to_save
        
        df_final.to_excel(output_path, index=False)
        
        # æ˜¾ç¤ºç»Ÿè®¡
        if 'processing_status' in df_final.columns:
            success_count = (df_final['processing_status'] == ProcessingStatus.SUCCESS).sum()
            no_relevant_count = (df_final['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
            failed_count = (df_final['processing_status'] == ProcessingStatus.API_FAILED).sum()
            UX.info(f"è¿›åº¦ä¿å­˜: æˆåŠŸ{success_count}æ¡, æ— ç›¸å…³{no_relevant_count}æ¡, å¤±è´¥{failed_count}æ¡")

class VKProcessor(BaseProcessor):
    """VKè¯„è®ºå¤„ç†å™¨ - ä¿®å¤ç‰ˆ"""
    
    def process(self, df, output_path, source='vk'):
        """å¤„ç†VKæ–‡ä»¶ - ä¿®å¤ç‰ˆ"""
        UX.info("å¤„ç†VKè¯„è®º...")
        
        mapping = COLUMN_MAPPING['vk']
        
        # è¾“å…¥æ•°æ®æ¨¡å¼æ ¡éªŒ
        required_columns = set(mapping.values())
        actual_columns = set(df.columns)
        
        if not required_columns.issubset(actual_columns):
            missing = required_columns - actual_columns
            UX.err(f"VKæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ä»¥ä¸‹å¿…éœ€åˆ—: {list(missing)}ã€‚å·²è·³è¿‡æ­¤æ–‡ä»¶ã€‚")
            return
        
        # æ£€æŸ¥å·²å¤„ç†çš„è®°å½•ï¼ˆåŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥ï¼‰
        processed_ids = set()
        failed_ids = set()
        if os.path.exists(output_path):
            try:
                df_existing_check = pd.read_excel(output_path)
                if not df_existing_check.empty and mapping['comment_id'] in df_existing_check.columns:
                    processed_ids, failed_ids = get_processing_state(df_existing_check, mapping['comment_id'])
                    
                    if 'processing_status' in df_existing_check.columns:
                        success_count = (df_existing_check['processing_status'] == ProcessingStatus.SUCCESS).sum()
                        no_relevant_count = (df_existing_check['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                        failed_count = (df_existing_check['processing_status'] == ProcessingStatus.API_FAILED).sum()
                        UX.info(f"VKå·²å¤„ç†: {len(processed_ids)} (æˆåŠŸ: {success_count}, æ— ç›¸å…³: {no_relevant_count})")
                        if failed_ids:
                            UX.info(f"å‘ç°VKå¤±è´¥è®°å½•: {len(failed_ids)} ä¸ªï¼Œå°†é‡æ–°åˆ†æ")
                    else:
                        UX.info(f"VKå·²å¤„ç†: {len(processed_ids)}")
                        
            except Exception as e:
                UX.warn(f"è¯»å–VKå·²å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
        
        # æ¸…ç†å¤±è´¥è®°å½•ï¼Œä¸ºé‡æ–°åˆ†æåšå‡†å¤‡
        if failed_ids:
            clean_failed_records(output_path, mapping['comment_id'])
            UX.info(f"æ¸…ç†äº† {len(failed_ids)} ä¸ªVKå¤±è´¥è®°å½•ï¼Œå‡†å¤‡é‡æ–°åˆ†æ")

        # ç­›é€‰å¾…å¤„ç†æ•°æ®ï¼šåªå¤„ç†å®Œå…¨æœªå¤„ç†çš„è®°å½• + å¤±è´¥çš„è®°å½•
        df[mapping['comment_id']] = df[mapping['comment_id']].astype(str)
        unprocessed_ids = set(df[mapping['comment_id']].astype(str)) - (processed_ids - failed_ids)
        df_to_process = df[df[mapping['comment_id']].astype(str).isin(unprocessed_ids)]
        
        if df_to_process.empty:
            UX.ok("æ‰€æœ‰VKè¯„è®ºå·²å¤„ç†")
            return
        
        UX.info(f"å¾…å¤„ç†: {len(df_to_process)} æ¡")
        
        # ã€å…³é”®ä¿®å¤ã€‘ï¼šåˆå§‹åŒ–ä¸ºPENDINGè€Œä¸æ˜¯SUCCESS
        df_to_process['processing_status'] = 'PENDING'  # å¾…å¤„ç†çŠ¶æ€
        
        # åˆå§‹åŒ–åˆ†æåˆ—
        analysis_columns = ANALYSIS_COLUMNS + ['Unit_Hash']  # åªæ·»åŠ Unit_Hashå­—æ®µ
        
        df_to_process['Source'] = source
        for col in analysis_columns:
            if col not in df_to_process.columns:
                df_to_process[col] = pd.NA
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œå°†Unit_Hashæ”¾åœ¨Post_IDå’ŒSourceä¹‹é—´
        if 'Unit_Hash' in df_to_process.columns:
            cols = list(df_to_process.columns)
            if 'Post_ID' in cols and 'Source' in cols:
                # ç§»é™¤Unit_Hash
                cols.remove('Unit_Hash')
                # æ‰¾åˆ°Post_IDçš„ä½ç½®ï¼Œåœ¨å…¶åæ’å…¥Unit_Hash
                post_idx = cols.index('Post_ID')
                cols.insert(post_idx + 1, 'Unit_Hash')
                df_to_process = df_to_process[cols]
        
        config = get_language_config('ru')
        
        # åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        batch_tasks = []
        batch_to_comments = {}  # è®°å½•æ‰¹æ¬¡å¯¹åº”çš„è¯„è®ºID
        
        for post_id, group in df_to_process.groupby(mapping['post_id'], dropna=False):
            if pd.isna(post_id):
                continue
                
            post_text = safe_str_convert(group[mapping['post_text']].iloc[0])
            
            
            comments_list = []
            comment_ids_in_batch = []
            
            for _, row in group.iterrows():
                comment_id = row[mapping['comment_id']]
                comment_text = row[mapping['comment_text']]
                
                if pd.isna(comment_id) or pd.isna(comment_text) or not str(comment_text).strip():
                    # ç©ºè¯„è®ºç›´æ¥æ ‡è®°ä¸ºNO_RELEVANT
                    mask = df_to_process[mapping['comment_id']].astype(str) == str(comment_id)
                    df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.NO_RELEVANT
                    df_to_process.loc[mask, 'relevance'] = 'ç©ºè¯„è®º'
                    continue
                    
                comments_list.append({
                    "comment_id": str(comment_id),
                    "comment_text": safe_str_convert(comment_text)
                })
                comment_ids_in_batch.append(str(comment_id))
                
                # æ”¶é›†åŸºç¡€å•å…ƒä¿¡æ¯
                # è·å–channel_nameï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
                channel_name = safe_str_convert(row.get(mapping.get('channel_name', 'channel_name'), 'æœªçŸ¥é¢‘é“')) if mapping.get('channel_name', 'channel_name') in df_to_process.columns else 'æœªçŸ¥é¢‘é“'
                
                self.Units_collector.append({
                    'Unit_ID': f"VK-{comment_id}",
                    'Source': source,
                    'Post_ID': str(post_id),
                    'Post_Text': post_text,
                    'Comment_Text': safe_str_convert(comment_text),
                    'channel_name': channel_name,
                    'AI_Is_Relevant': None
                })
            
            # åˆ›å»ºæ‰¹æ¬¡
            for i in range(0, len(comments_list), config['BATCH_SIZE_LIMIT']):
                chunk = comments_list[i:i + config['BATCH_SIZE_LIMIT']]
                chunk_ids = comment_ids_in_batch[i:i + config['BATCH_SIZE_LIMIT']]
                if chunk:
                    batch_idx = len(batch_tasks)
                    batch_tasks.append({
                        "post_id": str(post_id),
                        "post_text": post_text,
                        "comments": chunk
                    })
                    batch_to_comments[batch_idx] = chunk_ids
        
        if not batch_tasks:
            UX.warn("æ²¡æœ‰å¯å¤„ç†çš„æ‰¹æ¬¡ä»»åŠ¡")
            return
            
        UX.info(f"åˆ›å»ºäº† {len(batch_tasks)} ä¸ªæ‰¹å¤„ç†ä»»åŠ¡")
        
        # å¤„ç†æ‰¹æ¬¡
        with ThreadPoolExecutor(max_workers=config['MAX_CONCURRENT']) as executor:
            future_to_batch = {
                executor.submit(self._process_batch, batch_tasks[i]): i 
                for i in range(len(batch_tasks))
            }
            progress_bar = tqdm(as_completed(future_to_batch), total=len(batch_tasks), desc="æ‰¹å¤„ç†è¿›åº¦")
            
            for future in progress_bar:
                batch_idx = future_to_batch[future]
                expected_comment_ids = batch_to_comments[batch_idx]
                
                try:
                    batch_results = future.result()
                    
                    if batch_results is None:
                        # æ•´ä¸ªæ‰¹æ¬¡å¤±è´¥
                        UX.warn(f"æ‰¹æ¬¡ {batch_idx} è¿”å›Noneï¼Œæ ‡è®°æ‰€æœ‰è¯„è®ºä¸ºå¤±è´¥")
                        for comment_id in expected_comment_ids:
                            mask = df_to_process[mapping['comment_id']].astype(str) == comment_id
                            df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.API_FAILED
                            df_to_process.loc[mask, 'relevance'] = 'APIè°ƒç”¨å¤±è´¥'
                            for col in analysis_columns:
                                if col != 'Source':
                                    df_to_process.loc[mask, col] = 'API_FAILED'
                        continue
                    
                    if not isinstance(batch_results, list):
                        UX.warn(f"æ‰¹æ¬¡ {batch_idx} è¿”å›éåˆ—è¡¨ç»“æœ")
                        for comment_id in expected_comment_ids:
                            mask = df_to_process[mapping['comment_id']].astype(str) == comment_id
                            df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.API_FAILED
                            df_to_process.loc[mask, 'relevance'] = 'è¿”å›æ ¼å¼é”™è¯¯'
                        continue
                    
                    # å¤„ç†è¿”å›çš„ç»“æœ
                    processed_comment_ids = set()
                    
                    for result in batch_results:
                        if not isinstance(result, dict):
                            continue
                            
                        comment_id = str(result.get('comment_id', ''))
                        if not comment_id:
                            continue
                        
                        processed_comment_ids.add(comment_id)
                        mask = df_to_process[mapping['comment_id']].astype(str) == comment_id
                        
                        # ä»DataFrameä¸­è·å–åŸå§‹comment_textå¹¶æ·»åŠ åˆ°resultä¸­ç”¨äºå“ˆå¸Œè®¡ç®—
                        if mask.any():
                            original_comment_text = df_to_process.loc[mask, mapping['comment_text']].iloc[0]
                            result['comment_text'] = original_comment_text
                        
                        # ã€å…³é”®ã€‘ï¼šæ­£ç¡®è®¾ç½®processing_status
                        if 'processing_status' in result:
                            df_to_process.loc[mask, 'processing_status'] = result['processing_status']
                        else:
                            # æ ¹æ®relevanceåˆ¤æ–­
                            if result.get('relevance') == 'ä¸ç›¸å…³':
                                df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.NO_RELEVANT
                            elif result.get('relevance') in ['API_FAILED', 'INVALID_RESPONSE', 'EXCEPTION']:
                                df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.API_FAILED
                            else:
                                df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.SUCCESS
                        
                        # æ›´æ–°Units_collectorä¸­çš„ç›¸å…³æ€§ä¿¡æ¯
                        for Unit in self.Units_collector:
                            if Unit['Unit_ID'] == f"VK-{comment_id}":
                                Unit['AI_Is_Relevant'] = (result.get('relevance') not in ['ä¸ç›¸å…³', 'API_FAILED', 'INVALID_RESPONSE', 'EXCEPTION'])
                                break
                        
                        # æ›´æ–°å…¶ä»–å­—æ®µï¼ˆåŒ…æ‹¬Unit_Hashï¼‰
                        for key, value in result.items():
                            if (key in df_to_process.columns or key == 'Unit_Hash') and key != mapping['comment_id']:
                                if isinstance(value, list):
                                    df_to_process.loc[mask, key] = Utils.safe_json_dumps(value, ensure_ascii=False)
                                else:
                                    df_to_process.loc[mask, key] = value
                    
                    # æ ‡è®°æœªè¿”å›çš„è¯„è®ºä¸ºå¤±è´¥
                    missing_ids = set(expected_comment_ids) - processed_comment_ids
                    if missing_ids:
                        UX.warn(f"æ‰¹æ¬¡ {batch_idx} ä¸­æœ‰ {len(missing_ids)} ä¸ªè¯„è®ºæœªè¿”å›ç»“æœï¼Œæ ‡è®°ä¸ºå¤±è´¥")
                        for comment_id in missing_ids:
                            mask = df_to_process[mapping['comment_id']].astype(str) == comment_id
                            df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.API_FAILED
                            df_to_process.loc[mask, 'relevance'] = 'æœªè¿”å›ç»“æœ'
                                            
                    # å®šæœŸä¿å­˜è¿›åº¦
                    if (batch_idx + 1) % config['SAVE_INTERVAL'] == 0:
                        self._save_progress_generic(df_to_process, output_path, mapping['comment_id'])
                        # æ˜¾ç¤ºå½“å‰ç»Ÿè®¡
                        success_count = (df_to_process['processing_status'] == ProcessingStatus.SUCCESS).sum()
                        failed_count = (df_to_process['processing_status'] == ProcessingStatus.API_FAILED).sum()
                        no_relevant_count = (df_to_process['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                        pending_count = (df_to_process['processing_status'] == 'PENDING').sum()
                        total_processed = success_count + failed_count + no_relevant_count
                        progress_rate = (total_processed / len(df_to_process)) * 100
                        UX.info(f"ğŸ“Š VKå¤„ç†è¿›åº¦ ({progress_rate:.1f}%): æˆåŠŸ{success_count}, å¤±è´¥{failed_count}, æ— ç›¸å…³{no_relevant_count}, å¾…å¤„ç†{pending_count}")
                        
                except Exception as e:
                    UX.err(f"å¤„ç†æ‰¹æ¬¡ {batch_idx} å¼‚å¸¸: {str(e)[:100]}")
                    # æ‰¹æ¬¡å¼‚å¸¸ï¼Œæ ‡è®°æ‰€æœ‰è¯„è®ºä¸ºå¤±è´¥
                    for comment_id in expected_comment_ids:
                        mask = df_to_process[mapping['comment_id']].astype(str) == comment_id
                        df_to_process.loc[mask, 'processing_status'] = ProcessingStatus.API_FAILED
                        df_to_process.loc[mask, 'relevance'] = f'æ‰¹æ¬¡å¼‚å¸¸: {str(e)[:50]}'
                    continue
        
        # æœ€ç»ˆæ£€æŸ¥ï¼šå°†æ‰€æœ‰ä»ä¸ºPENDINGçš„è®°å½•æ ‡è®°ä¸ºå¤±è´¥
        pending_mask = df_to_process['processing_status'] == 'PENDING'
        if pending_mask.any():
            UX.warn(f"å‘ç° {pending_mask.sum()} æ¡æœªå¤„ç†è®°å½•ï¼Œæ ‡è®°ä¸ºå¤±è´¥")
            df_to_process.loc[pending_mask, 'processing_status'] = ProcessingStatus.API_FAILED
            df_to_process.loc[pending_mask, 'relevance'] = 'æœªè¢«å¤„ç†'
        
        # æœ€ç»ˆä¿å­˜
        self._save_progress_generic(df_to_process, output_path, mapping['comment_id'])
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        success_count = (df_to_process['processing_status'] == ProcessingStatus.SUCCESS).sum()
        failed_count = (df_to_process['processing_status'] == ProcessingStatus.API_FAILED).sum()
        no_relevant_count = (df_to_process['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
        
        completion_rate = ((success_count + no_relevant_count) / len(df_to_process)) * 100
        UX.ok(f"ğŸ“‹ VKå¤„ç†å®Œæˆæ€»ç»“: å®Œæˆåº¦{completion_rate:.1f}% (æˆåŠŸ{success_count} + æ— ç›¸å…³{no_relevant_count})")
        UX.info(f"   ğŸ“Š è¯¦ç»†ç»Ÿè®¡: æˆåŠŸ{success_count}, å¤±è´¥{failed_count}, æ— ç›¸å…³{no_relevant_count}")
        
        if failed_count > 0:
            UX.warn(f"   âš ï¸  ä»æœ‰{failed_count}æ¡è®°å½•å¤„ç†å¤±è´¥ï¼Œå¯å†æ¬¡è¿è¡Œè¿›è¡Œæ™ºèƒ½é‡è¯•")
    
    def _process_batch(self, batch_task):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡ - ä¿®å¤ç‰ˆ"""
        try:
            post_text = batch_task['post_text']
            comments_json = json.dumps(batch_task['comments'], ensure_ascii=False)
            
            prompt = prompts.VK_BATCH_ANALYSIS.format(
                post_text=str(post_text),
                comments_json=comments_json
            )
            
            # æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼šæ ¹æ®æ–‡æœ¬é•¿åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹
            combined_text = str(post_text) + " " + comments_json
            text_tokens = count_tokens(combined_text)
            
            if text_tokens > VK_LONG_TEXT_THRESHOLD:
                stage_key = 'VK_BATCH_LONG'
                UX.info(f"ğŸ”§ VKæ‰¹æ¬¡æ¨¡å‹é€‰æ‹©: ({text_tokens} tokens > {VK_LONG_TEXT_THRESHOLD}) â†’ é•¿æ–‡æœ¬æ¨¡å‹")
            else:
                stage_key = 'VK_BATCH'
                UX.info(f"ğŸ”§ VKæ‰¹æ¬¡æ¨¡å‹é€‰æ‹©: ({text_tokens} tokens) â†’ æ ‡å‡†æ¨¡å‹")
            
            # è°ƒç”¨API
            result = self.api_service.call_api_sync(prompt, language='ru', stage_key=stage_key)
            
            # APIè°ƒç”¨å¤±è´¥
            if result is None:
                UX.warn(f"APIè°ƒç”¨è¿”å›Noneï¼Œæ‰¹æ¬¡åŒ…å« {len(batch_task['comments'])} æ¡è¯„è®º")
                # è¿”å›å¤±è´¥è®°å½•åˆ—è¡¨ï¼Œæ·»åŠ æ‰¹å¤„ç†æ ‡è®°
                post_id = batch_task.get('post_id')
                return [self._create_failed_record(c['comment_id'], 'APIè¿”å›None', post_id) 
                        for c in batch_task['comments']]
            
            # å¼ºåŒ–APIç»“æœç±»å‹æ£€æŸ¥
            if not isinstance(result, list):
                UX.warn(f"æ‰¹æ¬¡APIè¿”å›æ ¼å¼é”™è¯¯ï¼ˆéåˆ—è¡¨ï¼‰ï¼Œå°†æ•´ä¸ªæ‰¹æ¬¡æ ‡è®°ä¸ºå¤±è´¥ã€‚è¿”å›å†…å®¹: {str(result)[:100]}")
                post_id = batch_task.get('post_id')
                return [self._create_failed_record(c['comment_id'], 'APIå“åº”æ ¼å¼æ— æ•ˆ', post_id) 
                        for c in batch_task['comments']]
            
            # å°è¯•æå–ç»“æœ
            processed_results = []
            
            if isinstance(result, list):
                for item in result:
                    if isinstance(item, dict):
                        # ç¡®ä¿æœ‰comment_id
                        if 'comment_id' not in item:
                            continue
                        
                        comment_id = str(item.get('comment_id', ''))
                        
                        # ä»batch_taskä¸­æ‰¾åˆ°å¯¹åº”çš„comment_text
                        for comment in batch_task['comments']:
                            if str(comment['comment_id']) == comment_id:
                                item['comment_text'] = comment['comment_text']
                                item['Unit_Text'] = comment['comment_text']  # æ·»åŠ Unit_Textå­—æ®µ
                                break
                        
                        # æ·»åŠ processing_status
                        if 'processing_status' not in item:
                            if item.get('relevance') == 'ä¸ç›¸å…³':
                                item['processing_status'] = ProcessingStatus.NO_RELEVANT
                            else:
                                item['processing_status'] = ProcessingStatus.SUCCESS
                        
                        # V2æ ¼å¼ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€è½¬æ¢
                        
                        self._add_hash_to_record(item, 'comment_text')  # ç›´æ¥åŸºäºcomment_textç”Ÿæˆå“ˆå¸Œ
                        processed_results.append(item)
                        
            elif isinstance(result, dict):
                # å°è¯•å¤šä¸ªå¯èƒ½çš„é”®
                for key in ['analysis', 'results', 'processed_results', 'data', 'comments']:
                    if key in result and isinstance(result[key], list):
                        for item in result[key]:
                            if isinstance(item, dict) and 'comment_id' in item:
                                comment_id = str(item.get('comment_id', ''))
                                
                                if comment_id:
                                    # ä»batch_taskä¸­æ‰¾åˆ°å¯¹åº”çš„comment_text
                                    for comment in batch_task['comments']:
                                        if str(comment['comment_id']) == comment_id:
                                            item['comment_text'] = comment['comment_text']
                                            item['Unit_Text'] = comment['comment_text']  # æ·»åŠ Unit_Textå­—æ®µ
                                            break
                                
                                if 'processing_status' not in item:
                                    if item.get('relevance') == 'ä¸ç›¸å…³':
                                        item['processing_status'] = ProcessingStatus.NO_RELEVANT
                                    else:
                                        item['processing_status'] = ProcessingStatus.SUCCESS
                                
                                # V2æ ¼å¼ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€è½¬æ¢
                                
                                self._add_hash_to_record(item, 'comment_text')  # ç›´æ¥åŸºäºcomment_textç”Ÿæˆå“ˆå¸Œ
                                processed_results.append(item)
                        break
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°æœ‰æ•ˆç»“æœ
            if not processed_results:
                UX.warn(f"æ— æ³•ä»APIå“åº”ä¸­æå–æœ‰æ•ˆç»“æœï¼Œæ‰¹æ¬¡åŒ…å« {len(batch_task['comments'])} æ¡è¯„è®º")
                post_id = batch_task.get('post_id')
                return [self._create_failed_record(c['comment_id'], 'APIå“åº”æ ¼å¼æ— æ•ˆ', post_id) 
                        for c in batch_task['comments']]
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¯„è®ºéƒ½æœ‰ç»“æœ
            result_ids = {str(r.get('comment_id')) for r in processed_results if r.get('comment_id')}
            expected_ids = {c['comment_id'] for c in batch_task['comments']}
            missing_ids = expected_ids - result_ids
            extra_ids = result_ids - expected_ids
            
            if missing_ids:
                UX.warn(f"APIå“åº”ç¼ºå°‘ {len(missing_ids)} æ¡è¯„è®ºçš„ç»“æœ")
                # ä¸ºç¼ºå¤±çš„è¯„è®ºæ·»åŠ å¤±è´¥è®°å½•
                post_id = batch_task.get('post_id')
                for comment_id in missing_ids:
                    processed_results.append(self._create_failed_record(comment_id, 'APIå“åº”ä¸­ç¼ºå¤±', post_id))
            
            if extra_ids:
                UX.warn(f"APIå“åº”åŒ…å« {len(extra_ids)} æ¡é¢å¤–è¯„è®ºï¼Œå°†è¢«å¿½ç•¥")
                # è¿‡æ»¤æ‰é¢å¤–çš„ç»“æœ
                processed_results = [r for r in processed_results if str(r.get('comment_id', '')) in expected_ids]
                        
            return processed_results
            
        except Exception as e:
            UX.err(f"æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {str(e)}")
            # è¿”å›å¼‚å¸¸è®°å½•ï¼Œæ·»åŠ æ‰¹å¤„ç†æ ‡è®°
            post_id = batch_task.get('post_id')
            return [self._create_failed_record(c['comment_id'], f'å¼‚å¸¸: {str(e)[:50]}', post_id) 
                    for c in batch_task['comments']]
    
    def _create_failed_record(self, comment_id, reason, post_id=None):
        """åˆ›å»ºå¤±è´¥è®°å½•ï¼ˆå¤ç”¨ç»Ÿä¸€å‡½æ•°ï¼‰"""
        record = create_unified_record(ProcessingStatus.API_FAILED, comment_id, 'vk', '', reason)
        # æ·»åŠ VKç‰¹æœ‰çš„å­—æ®µ
        record['comment_id'] = comment_id
        record['relevance'] = 'API_FAILED'
        record['speaker'] = 'API_CALL_FAILED'  # ä¿æŒä¸€è‡´æ€§
        record['Incident'] = reason
        
        # æ·»åŠ æ‰¹å¤„ç†ç‰¹æ®Šæ ‡è®°ï¼ˆç±»ä¼¼åª’ä½“æ–‡æœ¬çš„Macro_Chunk_IDï¼‰
        if post_id:
            record['Batch_ID'] = f"{post_id}-BATCH_FAILED"  # æ‰¹å¤„ç†å¤±è´¥æ ‡è®°
        
        return record
    
class ZhihuProcessor(BaseProcessor):
    """çŸ¥ä¹å›ç­”å¤„ç†å™¨ï¼ˆä¸¤æ­¥å¼ï¼Œä¸åª’ä½“æ–‡æœ¬å¯¹é½ï¼‰"""
    
    def _get_author_name(self, original_row, mapping):
        """æ™ºèƒ½è·å–ä½œè€…åç§°ï¼šæœ‰å›ç­”ç”¨æˆ·ååˆ—åˆ™ä½¿ç”¨ï¼Œæ— åˆ™è¿”å›æœªçŸ¥ä½œè€…"""
        author_column = mapping.get("author", "å›ç­”ç”¨æˆ·å")
        if author_column in original_row.index:
            author_value = safe_str_convert(original_row[author_column])
            return author_value if author_value.strip() else 'æœªçŸ¥ä½œè€…'
        else:
            return 'æœªçŸ¥ä½œè€…'
    
    def _finalize_record(self, result_data, original_row, mapping, answer_id, Unit_index=1):
        """
        å°†APIè¿”å›çš„åˆ†æç»“æœå°è£…æˆä¸€ä¸ªå®Œæ•´çš„è®°å½•å­—å…¸ã€‚
        """
        if not result_data or not isinstance(result_data, dict):
            return None

        author = self._get_author_name(original_row, mapping)
        
        result_data['speaker'] = author
        result_data['Source'] = 'çŸ¥ä¹'
        result_data['processing_status'] = ProcessingStatus.SUCCESS
        result_data['Unit_ID'] = f"ZH-{answer_id}-{Unit_index}"
        result_data['Answer_ID'] = f"ZH-{answer_id}"
        result_data['id'] = answer_id  # å…¼å®¹æ—§åˆ—å
        result_data['åºå·'] = answer_id # å…¼å®¹æ—§åˆ—å

        # V2æ ¼å¼ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€è½¬æ¢

        self._add_hash_to_record(result_data, 'Unit_Text')
        
        return result_data
    
    async def process(self, df, output_path, source='çŸ¥ä¹'):
        """å¤„ç†çŸ¥ä¹æ–‡ä»¶"""
        UX.info("å¤„ç†çŸ¥ä¹å›ç­”...")
        
        mapping = COLUMN_MAPPING['zhihu']
        failed_ids_list = []  # åˆå§‹åŒ–å¤±è´¥IDåˆ—è¡¨
        
        # è¾“å…¥æ•°æ®æ¨¡å¼æ ¡éªŒï¼ˆå›ç­”ç”¨æˆ·ååˆ—ä¸ºå¯é€‰ï¼‰
        required_columns = set(mapping.values())
        # ç§»é™¤å¯é€‰çš„å›ç­”ç”¨æˆ·ååˆ—
        optional_columns = {mapping.get("author", "å›ç­”ç”¨æˆ·å")}
        required_columns = required_columns - optional_columns
        actual_columns = set(df.columns)
        
        if not required_columns.issubset(actual_columns):
            missing = required_columns - actual_columns
            UX.err(f"çŸ¥ä¹æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ä»¥ä¸‹å¿…éœ€åˆ—: {list(missing)}ã€‚å·²è·³è¿‡æ­¤æ–‡ä»¶ã€‚")
            return failed_ids_list
        
        # æ£€æŸ¥å¯é€‰åˆ—æ˜¯å¦å­˜åœ¨
        author_column = mapping.get("author", "å›ç­”ç”¨æˆ·å")
        if author_column in actual_columns:
            UX.info(f"æ£€æµ‹åˆ°ä½œè€…åˆ—: {author_column}")
        else:
            UX.info(f"æœªæ£€æµ‹åˆ°ä½œè€…åˆ—: {author_column}ï¼Œå°†ä½¿ç”¨'æœªçŸ¥ä½œè€…'")
        
        # æ£€æŸ¥å·²å¤„ç†çš„è®°å½•ï¼ˆåŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥ï¼‰
        processed_ids = set()
        failed_ids = set()
        if os.path.exists(output_path):
            try:
                df_existing_check = pd.read_excel(output_path)
                if not df_existing_check.empty and mapping["id"] in df_existing_check.columns:
                    processed_ids, failed_ids = get_processing_state(df_existing_check, mapping["id"])
                    
                    if 'processing_status' in df_existing_check.columns:
                        success_count = (df_existing_check['processing_status'] == ProcessingStatus.SUCCESS).sum()
                        no_relevant_count = (df_existing_check['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                        failed_count = (df_existing_check['processing_status'] == ProcessingStatus.API_FAILED).sum()
                        UX.info(f"çŸ¥ä¹å·²å¤„ç†: {len(processed_ids)} (æˆåŠŸ: {success_count}, æ— ç›¸å…³: {no_relevant_count})")
                        if failed_ids:
                            UX.info(f"å‘ç°çŸ¥ä¹å¤±è´¥è®°å½•: {len(failed_ids)} ä¸ªï¼Œå°†é‡æ–°åˆ†æ")
                    else:
                        UX.info(f"çŸ¥ä¹å·²å¤„ç†: {len(processed_ids)}")
                        
            except Exception as e:
                UX.warn(f"è¯»å–çŸ¥ä¹å·²å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
        
        # æ¸…ç†å¤±è´¥è®°å½•ï¼Œä¸ºé‡æ–°åˆ†æåšå‡†å¤‡
        if failed_ids:
            clean_failed_records(output_path, mapping["id"])
            UX.info(f"æ¸…ç†äº† {len(failed_ids)} ä¸ªçŸ¥ä¹å¤±è´¥è®°å½•ï¼Œå‡†å¤‡é‡æ–°åˆ†æ")

        # ç­›é€‰å¾…å¤„ç†æ•°æ®ï¼šåªå¤„ç†å®Œå…¨æœªå¤„ç†çš„è®°å½• + å¤±è´¥çš„è®°å½•
        df[mapping["id"]] = df[mapping["id"]].astype(str)
        unprocessed_ids = set(df[mapping["id"]].astype(str)) - (processed_ids - failed_ids)
        df_to_process = df[df[mapping["id"]].astype(str).isin(unprocessed_ids)]
        
        if df_to_process.empty:
            UX.ok("æ‰€æœ‰çŸ¥ä¹å›ç­”å·²å¤„ç†")
            return failed_ids_list
        
        UX.info(f"å¾…å¤„ç†: {len(df_to_process)} æ¡")
        
        # å¹¶å‘å¤„ç†
        semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
        tasks = []
        
        for idx, row in df_to_process.iterrows():
            answer_id = str(row[mapping["id"]])
            tasks.append(self._process_answer(row, mapping, answer_id, semaphore))
        
        all_results = await aio_tqdm.gather(*tasks)
        
        # å±•å¹³ç»“æœå¹¶æ”¶é›†æ‰€æœ‰è®°å½•ï¼ˆåŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥ï¼‰
        final_results = []
        for result_item in all_results:
            if isinstance(result_item, tuple) and result_item[0] == 'FAILED':
                # å…¼å®¹æ—§ç‰ˆæœ¬çš„å¤±è´¥å¤„ç†ï¼ˆä¸åº”è¯¥å†å‡ºç°ï¼‰
                failed_ids_list.append(f"ID: {result_item[1]} - Reason: {result_item[2]}")
            elif result_item:  # è¿™æ˜¯ä¸€ä¸ªæœ‰å†…å®¹çš„ä»»åŠ¡ï¼ˆæˆåŠŸæˆ–å¤±è´¥ï¼‰
                final_results.extend(result_item)
        
        # ä¿å­˜ç»“æœ
        if final_results:
            df_results = pd.DataFrame(final_results)
            
            if os.path.exists(output_path):
                df_existing = pd.read_excel(output_path)
                df_final = pd.concat([df_existing, df_results], ignore_index=True)
            else:
                df_final = df_results
            
            df_final.to_excel(output_path, index=False)
            UX.ok(f"ä¿å­˜ {len(final_results)} æ¡çŸ¥ä¹è®®é¢˜å•å…ƒåˆ†æç»“æœ")
        
        return failed_ids_list
    
    async def _process_answer(self, row, mapping, answer_id, semaphore):
        """å¤„ç†å•ä¸ªçŸ¥ä¹å›ç­”ï¼ˆæ ¹æ®é•¿åº¦æ™ºèƒ½é€‰æ‹©æ¨¡å¼ï¼‰"""
        async with semaphore:
            answer_text = safe_str_convert(row[mapping["answer_text"]])
            question = safe_str_convert(row[mapping["question"]])
            author = self._get_author_name(row, mapping)
            
            if not answer_text.strip():
                return []
            
            try:
                # æ ¹æ®æ–‡æœ¬é•¿åº¦é€‰æ‹©å¤„ç†æ¨¡å¼ï¼ˆç»Ÿä¸€ä½¿ç”¨tokenè®¡ç®—ï¼‰
                answer_tokens = count_tokens(answer_text)
                if answer_tokens < ZHIHU_SHORT_TOKEN_THRESHOLD:
                    # === çŸ­æ–‡æœ¬æ¨¡å¼ï¼šç›´æ¥åˆ†æï¼ˆç±»ä¼¼VKï¼‰ ===
                    UX.info(f"ğŸ”§ çŸ¥ä¹å›ç­” {answer_id} æ¨¡å¼é€‰æ‹©: ({answer_tokens} tokens) â†’ çŸ­æ–‡æœ¬ç›´æ¥åˆ†ææ¨¡å¼")
                    
                    # ä½¿ç”¨æ¨¡æ¿æ„é€ æç¤ºè¯
                    prompt = prompts.ZHIHU_SHORT_ANALYSIS.format(
                        question=question,
                        answer_text=answer_text
                    )
                    
                    # æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼šçŸ­æ–‡æœ¬æ¨¡å¼ç›´æ¥ä½¿ç”¨è½»é‡æ¨¡å‹
                    stage_key = 'ZHIHU_ANALYSIS_SHORT'
                    UX.info(f"ğŸ”§ çŸ¥ä¹æ¨¡å‹é€‰æ‹©: ({answer_tokens} tokens < {ZHIHU_SHORT_TOKEN_THRESHOLD}) â†’ è½»é‡æ¨¡å‹")
                    
                    result = await self.api_service.call_api_async(
                        prompt, 'zh', stage_key
                    )
                    
                    if result:
                        result['Unit_Text'] = answer_text # ç¡®ä¿Unit_Textå­˜åœ¨
                        final_record = self._finalize_record(result, row, mapping, answer_id)
                        
                        # æ·»åŠ åˆ°Units_collector
                        self.Units_collector.append({
                            'Unit_ID': f"ZH-{answer_id}-1",
                            'Source': 'çŸ¥ä¹',
                            'Question': question,
                            'Answer_Text': answer_text[:500],
                            'Author': author,
                            'AI_Is_Relevant': True
                        })
                        
                        return [final_record] if final_record else None
                    else:
                        # APIå¤±è´¥ï¼Œè¿”å›ç»Ÿä¸€å¤±è´¥è®°å½•
                        failed_record = create_unified_record(ProcessingStatus.API_FAILED, answer_id, 'çŸ¥ä¹', answer_text[:200], 'çŸ¥ä¹çŸ­æ–‡æœ¬åˆ†æå¤±è´¥')
                        failed_record['Unit_Text'] = f'[åˆ†æå¤±è´¥] {answer_text[:100]}...'
                        return [failed_record]
                        
                else:
                    # === é•¿æ–‡æœ¬æ¨¡å¼ï¼šä¸¤æ­¥å¼åˆ†æ ===
                    UX.info(f"ğŸ”§ çŸ¥ä¹å›ç­” {answer_id} æ¨¡å¼é€‰æ‹©: ({answer_tokens} tokens) â†’ ä¸¤æ­¥å¼åˆ†ææ¨¡å¼")
                    
                    # ç¬¬ä¸€æ­¥ï¼šè®®é¢˜å•å…ƒåˆ’åˆ†
                    prompt1 = prompts.ZHIHU_CHUNKING.format(full_text=answer_text)
                    result1 = await self.api_service.call_api_async(prompt1, 'zh', 'ZHIHU_CHUNKING')
                    
                    if not result1:
                        failed_record = create_unified_record(ProcessingStatus.API_FAILED, answer_id, 'çŸ¥ä¹', '', 'çŸ¥ä¹åˆ‡åˆ†å¤±è´¥')
                        failed_record['Unit_Text'] = f'[åˆ‡åˆ†å¤±è´¥] {answer_text[:100]}...'
                        return [failed_record]
                    
                    chapters = result1.get('argument_chapters', [])
                    if not chapters:
                        chapters = [{'Unit_Text': answer_text}]
                    
                    # æ·»åŠ åˆ°Units_collector
                    self.Units_collector.append({
                        'Unit_ID': f"ZH-{answer_id}",
                        'Source': 'çŸ¥ä¹',
                        'Question': question,
                        'Answer_Text': answer_text[:500],  # æˆªå–é¢„è§ˆ
                        'Author': author,
                        'AI_Is_Relevant': None  # åç»­æ›´æ–°
                    })
                    
                    # ç¬¬äºŒæ­¥ï¼šå¯¹æ¯ä¸ªè®®é¢˜å•å…ƒè¿›è¡Œåˆ†æ
                    results = []
                    for i, chapter in enumerate(chapters):
                        Unit_Text = chapter.get('Unit_Text', '')
                        if not Unit_Text.strip():
                            continue
                        
                        prompt2 = prompts.ZHIHU_ANALYSIS.format(
                            question=question,
                            Unit_Text=Unit_Text
                        )
                        
                        # æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼šæ ¹æ®è®®é¢˜å•å…ƒé•¿åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹
                        unit_tokens = count_tokens(Unit_Text)
                        if unit_tokens < ZHIHU_SHORT_TOKEN_THRESHOLD:
                            stage_key = 'ZHIHU_ANALYSIS_SHORT'
                            UX.info(f"ğŸ”§ çŸ¥ä¹è®®é¢˜å•å…ƒæ¨¡å‹é€‰æ‹©: ({unit_tokens} tokens < {ZHIHU_SHORT_TOKEN_THRESHOLD}) â†’ è½»é‡æ¨¡å‹")
                        elif unit_tokens > ZHIHU_LONG_TOKEN_THRESHOLD:
                            stage_key = 'ZHIHU_ANALYSIS_LONG'
                            UX.info(f"ğŸ”§ çŸ¥ä¹è®®é¢˜å•å…ƒæ¨¡å‹é€‰æ‹©: ({unit_tokens} tokens > {ZHIHU_LONG_TOKEN_THRESHOLD}) â†’ é«˜æ€§èƒ½æ¨¡å‹")
                        else:
                            stage_key = 'ZHIHU_ANALYSIS'
                            UX.info(f"ğŸ”§ çŸ¥ä¹è®®é¢˜å•å…ƒæ¨¡å‹é€‰æ‹©: ({unit_tokens} tokens) â†’ æ ‡å‡†æ¨¡å‹")
                        
                        result2 = await self.api_service.call_api_async(prompt2, 'zh', stage_key)
                        
                        if result2:
                            # æ„å»ºå®Œæ•´è®°å½•
                            Unit_record = {
                                "Unit_Text": Unit_Text,
                                "expansion_logic": f"ç¬¬{i+1}ä¸ªè®ºè¯ç« èŠ‚",  
                                **result2  # åŒ…å«æ‰€æœ‰åˆ†æç»´åº¦
                            }
                            
                            final_record = self._finalize_record(Unit_record, row, mapping, answer_id, Unit_index=i + 1)
                            if final_record:
                                results.append(final_record)
                    
                    # æ›´æ–°Units_collectorä¸­çš„ç›¸å…³æ€§
                    for Unit in self.Units_collector:
                        if Unit['Unit_ID'] == f"ZH-{answer_id}":
                            Unit['AI_Is_Relevant'] = bool(results)
                            break
                    
                    if results:
                        return results
                    else:
                        # é•¿æ–‡æœ¬åˆ†æå¤±è´¥ï¼Œè¿”å›ç»Ÿä¸€å¤±è´¥è®°å½•
                        failed_record = create_unified_record(ProcessingStatus.API_FAILED, answer_id, 'çŸ¥ä¹', answer_text[:200], 'çŸ¥ä¹é•¿æ–‡æœ¬åˆ†æå¤±è´¥')
                        failed_record['Unit_Text'] = f'[åˆ†æå¤±è´¥] {answer_text[:100]}...'
                        return [failed_record]
                    
            except Exception as e:
                UX.warn(f"å¤„ç†å›ç­” {answer_id} å¤±è´¥: {str(e)[:100]}")
                # è¿”å›ç»Ÿä¸€å¤±è´¥è®°å½•
                failed_record = create_unified_record(ProcessingStatus.API_FAILED, answer_id, 'çŸ¥ä¹', answer_text[:200], f'å¼‚å¸¸: {str(e)[:50]}')
                failed_record['Unit_Text'] = f'[å¼‚å¸¸] {str(e)[:100]}'
                return [failed_record]

# ==============================================================================
# === ğŸ”¬ ä¿¡åº¦æ£€éªŒï¼ˆä¸åª’ä½“æ–‡æœ¬å¯¹é½ï¼‰
# ==============================================================================

def save_Units_database(Units_data, output_path):
    """ä¿å­˜åŸºç¡€å•å…ƒæ•°æ®åº“"""
    if not Units_data:
        return None
    
    database_path = os.path.join(output_path, 'ç¤¾äº¤åª’ä½“_åŸºç¡€å•å…ƒæ•°æ®åº“.xlsx')
    
    if os.path.exists(database_path):
        df_existing = pd.read_excel(database_path)
        existing_ids = set(df_existing['Unit_ID'].unique())
        new_data = [u for u in Units_data if u['Unit_ID'] not in existing_ids]
        
        if new_data:
            df_new = pd.DataFrame(new_data)
            df_final = pd.concat([df_existing, df_new], ignore_index=True)
        else:
            df_final = df_existing
    else:
        df_final = pd.DataFrame(Units_data)
    
    df_final.to_excel(database_path, index=False)
    UX.ok(f"åŸºç¡€å•å…ƒæ•°æ®åº“å·²ä¿å­˜: {len(df_final)} æ¡è®°å½•")
    
    return database_path

def save_parent_texts_database(parent_texts_data, output_path):
    """ä¿å­˜çˆ¶æ–‡æœ¬æ•°æ®åº“ï¼ˆå¸–å­/å›ç­”ï¼‰"""
    if not parent_texts_data:
        return None
    
    database_path = os.path.join(output_path, 'ç¤¾äº¤åª’ä½“_çˆ¶æ–‡æœ¬æ•°æ®åº“.xlsx')
    
    if os.path.exists(database_path):
        df_existing = pd.read_excel(database_path)
        # æ ¹æ®æ•°æ®æºç¡®å®šIDå­—æ®µ
        id_col = 'Post_ID' if 'Post_ID' in df_existing.columns else 'Answer_ID'
        existing_ids = set(df_existing[id_col].unique())
        new_data = [m for m in parent_texts_data if m[id_col] not in existing_ids]
        
        if new_data:
            df_new = pd.DataFrame(new_data)
            df_final = pd.concat([df_existing, df_new], ignore_index=True)
        else:
            df_final = df_existing
    else:
        df_final = pd.DataFrame(parent_texts_data)
    
    df_final.to_excel(database_path, index=False)
    UX.ok(f"çˆ¶æ–‡æœ¬æ•°æ®åº“å·²ä¿å­˜: {len(df_final)} æ¡è®°å½•")
    
    return database_path

# ==============================================================================
# === ğŸŒ åŒè¯­æ”¯æŒåŠŸèƒ½ï¼ˆä¸åª’ä½“æ–‡æœ¬å¯¹é½ï¼‰
# ==============================================================================

def _highlight_Unit_in_parent(parent_text: str, Unit_Text: str) -> tuple:
    """æ™ºèƒ½é«˜äº®ï¼šä½¿ç”¨ç¼–è¾‘è·ç¦»æ‰¾æœ€ä½³åŒ¹é…ä½ç½®"""
    import difflib
    try:
        if not parent_text or not Unit_Text:
            return parent_text, False
        
        def normalize_spaces(s):
            return ' '.join(s.split())
        
        parent_norm = normalize_spaces(safe_str_convert(parent_text))
        Unit_norm = normalize_spaces(safe_str_convert(Unit_Text))
        
        # 1) ç›´æ¥æŸ¥æ‰¾
        if Unit_norm in parent_norm:
            idx = parent_norm.find(Unit_norm)
            return parent_norm[:idx] + "ã€" + Unit_norm + "ã€‘" + parent_norm[idx+len(Unit_norm):], True
        
        # 2) ä½¿ç”¨åºåˆ—åŒ¹é…å™¨çª—å£æœç´¢
        Unit_len = len(Unit_norm)
        if Unit_len == 0:
            return parent_text, False
        matcher = difflib.SequenceMatcher(None, '', Unit_norm)
        best_ratio = 0.0
        best_start = 0
        window_size = max(1, int(Unit_len * 1.2))
        min_start = 0
        max_start = max(0, len(parent_norm) - max(1, int(Unit_len * 0.8)) + 1)
        for i in range(min_start, max_start):
            end = min(i + window_size, len(parent_norm))
            window = parent_norm[i:end]
            matcher.set_seq1(window)
            ratio = matcher.ratio()
            if ratio > best_ratio:
                best_ratio = ratio
                best_start = i
        
        if best_ratio > 0.85:
            end = min(best_start + window_size, len(parent_norm))
            return parent_norm[:best_start] + "ã€" + parent_norm[best_start:end] + "ã€‘" + parent_norm[end:], True
        
        return parent_text, False
    except Exception as e:
        UX.warn(f"é«˜äº®å¤„ç†å¤±è´¥: {e}")
        return safe_str_convert(parent_text), False

class BilingualSupport:
    """åŒè¯­æ”¯æŒåŠŸèƒ½ - ç®€åŒ–ç‰ˆ"""
    
    # åŸºç¡€æ˜ å°„æ¨¡æ¿
    _BASE_MAPPINGS = {
        'zh': {
            # åŸºç¡€å­—æ®µ
            'Unit_ID': 'è®®é¢˜å•å…ƒæ ‡è¯†ç¬¦', 'Source': 'æ•°æ®æ¥æº', 'Post_ID': 'å¸–å­æ ‡è¯†ç¬¦', 'Comment_ID': 'è¯„è®ºæ ‡è¯†ç¬¦',
            'Answer_ID': 'å›ç­”æ ‡è¯†ç¬¦', 'Unit_Text': 'è®®é¢˜å•å…ƒæ–‡æœ¬', 'Unit_Hash': 'æ–‡æœ¬å“ˆå¸Œå€¼',
            'speaker': 'å‘è¨€äºº', 'Incident': 'æ ¸å¿ƒäº‹ä»¶æ¦‚æ‹¬', 'expansion_logic': 'åˆ†å—é€»è¾‘è¯´æ˜',
            'processing_status': 'å¤„ç†çŠ¶æ€', 'comment_text': 'è¯„è®ºæ–‡æœ¬', 'Post_Text': 'å¸–å­æ–‡æœ¬', 'Comment_Text': 'è¯„è®ºæ–‡æœ¬',
            'Question': 'çŸ¥ä¹é—®é¢˜', 'Answer_Text': 'çŸ¥ä¹å›ç­”æ–‡æœ¬', 'Author': 'ä½œè€…',
            'Original_Post_ID': 'åŸå§‹å¸–å­ID', 'Original_Comment_ID': 'åŸå§‹è¯„è®ºID',
            # åˆ†æç»´åº¦
            'Valence': 'æƒ…æ„Ÿå€¾å‘', 'Evidence_Type': 'è¯æ®ç±»å‹', 'Attribution_Level': 'å½’å› å±‚æ¬¡',
            'Temporal_Focus': 'æ—¶é—´èšç„¦', 'Primary_Actor_Type': 'ä¸»è¦è¡ŒåŠ¨è€…ç±»å‹',
            'Geographic_Scope': 'åœ°ç†èŒƒå›´', 'Relationship_Model_Definition': 'å…³ç³»æ¨¡å¼ç•Œå®š',
            'Discourse_Type': 'è¯è¯­ç±»å‹',
            # æ£€éªŒå­—æ®µ
            'Inspector_Is_CN_RU_Related': 'äººå·¥æ£€éªŒï¼šæ˜¯å¦ä¸­ä¿„ç›¸å…³',
            'Inspector_Boundary': 'äººå·¥æ£€éªŒï¼šè¾¹ç•Œåˆ’åˆ†',
            # VKç‰¹æœ‰
            'post_id': 'å¸–å­ID', 'comment_id': 'è¯„è®ºID', 'relevance': 'ç›¸å…³æ€§åˆ¤æ–­', 'channel_name': 'é¢‘é“åç§°'
        },
        'ru': {
            # åŸºç¡€å­—æ®µ
            'Unit_ID': 'Ğ˜Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹', 'Source': 'Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 
            'Post_ID': 'Ğ˜Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ¾ÑÑ‚Ğ°', 'Comment_ID': 'Ğ˜Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ñ',
            'Answer_ID': 'Ğ˜Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°', 'Unit_Text': 'Ğ¢ĞµĞºÑÑ‚ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹', 'Unit_Hash': 'Ğ¥ĞµÑˆ Ñ‚ĞµĞºÑÑ‚Ğ°',
            'speaker': 'Ğ“Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¹', 'Incident': 'ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğµ', 'expansion_logic': 'Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²ĞºĞ¸',
            'processing_status': 'Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸', 'comment_text': 'Ğ¢ĞµĞºÑÑ‚ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ñ',
            'Post_Text': 'Ğ¢ĞµĞºÑÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ°', 'Comment_Text': 'Ğ¢ĞµĞºÑÑ‚ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ñ', 'Question': 'Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ—Ğ½Ğ°Ñ…Ñƒ', 'Answer_Text': 'Ğ¢ĞµĞºÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ—Ğ½Ğ°Ñ…Ñƒ', 'Author': 'ĞĞ²Ñ‚Ğ¾Ñ€',
            'Original_Post_ID': 'ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ID Ğ¿Ğ¾ÑÑ‚Ğ°', 'Original_Comment_ID': 'ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ID ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ñ',
            # åˆ†æç»´åº¦
            'Valence': 'Ğ­Ğ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾ĞºÑ€Ğ°ÑĞºĞ°', 'Evidence_Type': 'Ğ¢Ğ¸Ğ¿ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²',
            'Attribution_Level': 'Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸', 'Temporal_Focus': 'Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¾ĞºÑƒÑ',
            'Primary_Actor_Type': 'Ğ¢Ğ¸Ğ¿ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°', 'Geographic_Scope': 'Ğ“ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ñ…Ğ²Ğ°Ñ‚',
            'Relationship_Model_Definition': 'ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹', 'Discourse_Type': 'Ğ¢Ğ¸Ğ¿ Ğ´Ğ¸ÑĞºÑƒÑ€ÑĞ°',
            # æ£€éªŒå­—æ®µ
            'Inspector_Is_CN_RU_Related': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ¼: ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ Ñ ĞšĞĞ -Ğ Ğ¤',
            'Inspector_Boundary': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ¼: Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹',
            # VKç‰¹æœ‰
            'post_id': 'ID Ğ¿Ğ¾ÑÑ‚Ğ°', 'comment_id': 'ID ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ñ', 'relevance': 'ĞÑ†ĞµĞ½ĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸', 'channel_name': 'ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ½Ğ°Ğ»Ğ°'
        }
    }
    
    @classmethod
    def _generate_frame_mappings(cls, lang):
        """åŠ¨æ€ç”Ÿæˆæ¡†æ¶ç›¸å…³æ˜ å°„"""
        frames = ['ProblemDefinition', 'ResponsibilityAttribution', 'MoralEvaluation', 
                 'SolutionRecommendation', 'ActionStatement', 'CausalExplanation']
        mappings = {}
        
        if lang == 'zh':
            frame_names = {'ProblemDefinition': 'é—®é¢˜å»ºæ„', 'ResponsibilityAttribution': 'è´£ä»»å½’å› ',
                          'MoralEvaluation': 'é“å¾·è¯„ä»·', 'SolutionRecommendation': 'è§£å†³æ–¹æ¡ˆ',
                          'ActionStatement': 'è¡ŒåŠ¨å£°æ˜', 'CausalExplanation': 'å› æœè§£é‡Š'}
            for frame in frames:
                cn_name = frame_names[frame]
                mappings[f'AI_Frame_{frame}_Present'] = f'AIè¯†åˆ«ï¼š{cn_name}æ¡†æ¶'
                mappings[f'Inspector_Frame_{frame}_Present'] = f'äººå·¥æ£€éªŒï¼š{cn_name}æ¡†æ¶'
        else:  # ru
            frame_names = {'ProblemDefinition': 'Ğ¿Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹', 'ResponsibilityAttribution': 'Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸',
                          'MoralEvaluation': 'Ğ¼Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸', 'SolutionRecommendation': 'Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹',
                          'ActionStatement': 'Ğ·Ğ°ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…', 'CausalExplanation': 'Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ'}
            for frame in frames:
                ru_name = frame_names[frame]
                mappings[f'AI_Frame_{frame}_Present'] = f'Ğ˜Ğ˜ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ»: Ñ„Ñ€ĞµĞ¹Ğ¼ {ru_name}'
                mappings[f'Inspector_Frame_{frame}_Present'] = f'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ¼: Ñ„Ñ€ĞµĞ¹Ğ¼ {ru_name}'
        
        return mappings
    
    @classmethod
    def _generate_dimension_mappings(cls, lang):
        """åŠ¨æ€ç”Ÿæˆç»´åº¦æ£€éªŒæ˜ å°„"""
        dims = ['Valence', 'Evidence_Type', 'Attribution_Level', 'Temporal_Focus',
               'Primary_Actor_Type', 'Geographic_Scope', 'Relationship_Model_Definition', 'Discourse_Type']
        mappings = {}
        
        prefix = 'äººå·¥æ£€éªŒï¼š' if lang == 'zh' else 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ¼: Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ '
        suffix = 'æ­£ç¡®æ€§' if lang == 'zh' else ''
        
        for dim in dims:
            base_name = cls._BASE_MAPPINGS[lang].get(dim, dim)
            mappings[f'Inspector_{dim}_Correct'] = f'{prefix}{base_name}{suffix}'
        
        return mappings
    
    @classmethod
    def get_mappings(cls, lang):
        """è·å–å®Œæ•´çš„æ˜ å°„å­—å…¸"""
        mappings = cls._BASE_MAPPINGS[lang].copy()
        mappings.update(cls._generate_frame_mappings(lang))
        mappings.update(cls._generate_dimension_mappings(lang))
        return mappings
    
    # å…¼å®¹æ€§å±æ€§
    @property
    def LABEL_MAPPINGS(self):
        return {'zh': self.get_mappings('zh'), 'ru': self.get_mappings('ru')}
    
    @staticmethod
    def decorate_headers(df: pd.DataFrame, lang: str) -> pd.DataFrame:
        """è£…é¥°åˆ—åä¸ºè¯­è¨€æ ‡ç­¾æ ¼å¼ï¼šæœ¬åœ°è¯­è¨€å(è‹±æ–‡å)"""
        mapping = BilingualSupport.get_mappings(lang)
        df_out = df.copy()
        new_columns = []
        for c in df.columns:
            if c in mapping:
                # æ ¼å¼ï¼šæœ¬åœ°è¯­è¨€å(è‹±æ–‡å)
                new_columns.append(f"{mapping[c]}({c})")
            else:
                # æœªæ˜ å°„çš„åˆ—åä¿æŒåŸæ ·
                new_columns.append(c)
        df_out.columns = new_columns
        return df_out

# ä¿æŒå‘åå…¼å®¹æ€§çš„åˆ«å
_decorate_headers = BilingualSupport.decorate_headers
_decorate_headers_chinese = lambda df: BilingualSupport.decorate_headers(df, 'zh')
_decorate_headers_russian = lambda df: BilingualSupport.decorate_headers(df, 'ru')

def _clean_dataframe(df: pd.DataFrame, operation_name: str = "æ•°æ®å¤„ç†") -> pd.DataFrame:
    """é€šç”¨çš„DataFrameæ¸…ç†å‡½æ•°ï¼šä¿®å¤é‡å¤ç´¢å¼•å’Œåˆ—å"""
    # é‡ç½®ç´¢å¼•
    df = df.reset_index(drop=True)
    
    # æ£€æŸ¥å¹¶ä¿®å¤é‡å¤åˆ—å
    if df.columns.duplicated().any():
        UX.warn(f"{operation_name}å‘ç°é‡å¤åˆ—å: {df.columns[df.columns.duplicated()].tolist()}")
        df = df.loc[:, ~df.columns.duplicated()]
        UX.info(f"å·²å»é™¤é‡å¤åˆ—ï¼Œå½“å‰åˆ—æ•°: {len(df.columns)}")
    
    return df

def _is_frame_present(frame_value):
    """
    åˆ¤æ–­V2æ ¼å¼çš„æ¡†æ¶å†…å®¹æ˜¯å¦çœŸå®å­˜åœ¨ã€‚
    V2æ ¼å¼ï¼š[{"quote": "...", "reason": "...", "confidence": "..."}]
    """
    # 1. å¤„ç†Pandasçš„ç©ºå€¼ (None, nan, etc.)
    if pd.isna(frame_value):
        return 0
    
    # 2. å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
    if isinstance(frame_value, str):
        s_value = frame_value.strip()
        # æ£€æŸ¥ä»£è¡¨"ç©º"æˆ–"æ— å†…å®¹"çš„å¸¸è§å­—ç¬¦ä¸²
        if not s_value or s_value == '[]' or s_value == '[""]':
            return 0
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ˜ç¡®çš„å¤±è´¥æˆ–ä¸ç›¸å…³æ ‡è®°
        if 'API_FAILED' in s_value or 'NO_RELEVANT' in s_value:
            return 0
        try:
            frame_value = json.loads(s_value)
        except:
            return 0
    
    # 3. å¦‚æœæ˜¯åˆ—è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„å¯¹è±¡
    if isinstance(frame_value, list):
        if not frame_value:  # ç©ºåˆ—è¡¨
            return 0
        # æ£€æŸ¥æ˜¯å¦æœ‰åŒ…å«æœ‰æ•ˆquoteçš„å¯¹è±¡
        for item in frame_value:
            if isinstance(item, dict) and item.get('quote', '').strip():
                return 1
        return 0
    
    # 4. å…¶ä»–æƒ…å†µè®¤ä¸ºæ— å†…å®¹
    return 0

def _add_frame_boolean_columns(df, frames):
    """ä¸ºDataFrameæ·»åŠ æ¡†æ¶å¸ƒå°”å€¼åˆ—ï¼ˆV2æ ¼å¼ï¼‰"""
    for frame in frames:
        frame_col = f'Frame_{frame}'  # V2æ ¼å¼ç›´æ¥ä½¿ç”¨Frame_{frame}
        ai_col = f'AI_Frame_{frame}_Present'
        
        if frame_col in df.columns:
            df[ai_col] = df[frame_col].apply(_is_frame_present)
        else:
            df[ai_col] = 0
    
    return df

def _add_inspector_columns(df, frames, dims):
    """ä¸ºDataFrameæ·»åŠ äººå·¥æ£€éªŒåˆ—"""
    # æ·»åŠ æ¡†æ¶æ£€éªŒåˆ—
    for frame in frames:
        inspector_col = f'Inspector_Frame_{frame}_Present'
        if inspector_col not in df.columns:
            df[inspector_col] = ''
    
    # æ·»åŠ ç»´åº¦æ£€éªŒåˆ—
    for dim in dims:
        inspector_col = f'Inspector_{dim}_Correct'
        if inspector_col not in df.columns:
            df[inspector_col] = ''
    
    return df

def _organize_columns_order(df: pd.DataFrame, source: str) -> pd.DataFrame:
    """
    æ ¹æ®æ•°æ®æºï¼Œä½¿ç”¨é¢„å®šä¹‰çš„é¡ºåºå’ŒèŒƒå›´æ¥ç»„ç»‡DataFrameçš„åˆ—ã€‚
    """
    # ç†æƒ³çš„åˆ—é¡ºåº master list
    base_order = [
        'Unit_ID', 'Unit_Hash', 'Source', 'Post_ID', 'Answer_ID', 
        'Post_Text', 'Unit_Text', 'speaker', 'Incident', 'expansion_logic',
        # æ£€éªŒå­—æ®µ
        'Inspector_Boundary', 'Inspector_Is_CN_RU_Related'
    ]
    
    frames = ['ProblemDefinition', 'ResponsibilityAttribution', 'MoralEvaluation',
              'SolutionRecommendation', 'ActionStatement', 'CausalExplanation']
    dims = ['Valence', 'Evidence_Type', 'Attribution_Level', 'Temporal_Focus',
            'Primary_Actor_Type', 'Geographic_Scope', 'Relationship_Model_Definition',
            'Discourse_Type']

    for frame in frames:
        base_order.extend([f'Frame_{frame}', f'AI_Frame_{frame}_Present', f'Inspector_Frame_{frame}_Present'])
    
    for dim in dims:
        base_order.extend([dim, f'Inspector_{dim}_Correct'])

    # å®šä¹‰æ¯ä¸ªæ•°æ®æºå®é™…éœ€è¦çš„åˆ—
    if source == 'vk':
        required_cols = {
            'Unit_ID', 'Unit_Hash', 'Source', 'Post_ID', 'Post_Text', 'Unit_Text', 
            'speaker', 'Incident', 
        }
    elif source == 'zhihu':
        required_cols = {
            'Unit_ID', 'Unit_Hash', 'Source', 'Answer_ID', 'Unit_Text', 
            'speaker', 'Incident', 'expansion_logic'
        }
    else: # é»˜è®¤æƒ…å†µ
        required_cols = set(df.columns)

    # æ·»åŠ æ‰€æœ‰æ¡†æ¶å’Œç»´åº¦çš„åˆ—åˆ°å¿…éœ€é›†åˆä¸­
    for frame in frames:
        required_cols.update([f'Frame_{frame}', f'AI_Frame_{frame}_Present', f'Inspector_Frame_{frame}_Present'])
    for dim in dims:
        required_cols.update([dim, f'Inspector_{dim}_Correct'])
    
    # 1. ç­›é€‰å‡ºdfä¸­å®é™…å­˜åœ¨ä¸”å¿…éœ€çš„åˆ—
    existing_and_required = [col for col in df.columns if col in required_cols]
    
    # 2. æŒ‰ç…§ç†æƒ³é¡ºåºå¯¹è¿™äº›åˆ—è¿›è¡Œæ’åº
    final_ordered_cols = [col for col in base_order if col in existing_and_required]
    
    # 3. æ·»åŠ ä»»ä½•ä¸åœ¨ç†æƒ³é¡ºåºä¸­ä½†ç¡®å®å­˜åœ¨çš„å¿…éœ€åˆ—ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
    for col in existing_and_required:
        if col not in final_ordered_cols:
            final_ordered_cols.append(col)
            
    return df[final_ordered_cols]

def _save_bilingual(df: pd.DataFrame, zh_path: str, ru_path: str):
    """ä¿å­˜åŒè¯­ç‰ˆæœ¬æ–‡ä»¶ï¼ˆä¼˜åŒ–ç­–ç•¥ï¼šVKç”¨ä¿„è¯­ç‰ˆï¼ŒçŸ¥ä¹ç”¨ä¸­æ–‡ç‰ˆï¼‰"""
    try:
        zh_dir = os.path.dirname(zh_path)
        ru_dir = os.path.dirname(ru_path)
        if zh_dir:
            os.makedirs(zh_dir, exist_ok=True)
        if ru_dir and ru_dir != zh_dir:
            os.makedirs(ru_dir, exist_ok=True)
    except Exception as e:
        UX.warn(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {e}")
    
    # ä¸­æ–‡ç‰ˆï¼šä¿æŒåŸæ ·ï¼ˆé€‚åˆçŸ¥ä¹æ•°æ®ï¼‰
    try:
        df_zh = _decorate_headers(df, 'zh')
        df_zh.to_excel(zh_path, index=False)
    except Exception as e:
        UX.warn(f"ä¸­æ–‡ç‰ˆæœ¬å¯¼å‡ºå¤±è´¥: {e}")
    
    # ä¿„è¯­ç‰ˆï¼šä¿æŒåŸæ ·ï¼ˆé€‚åˆVKæ•°æ®ï¼‰
    try:
        df_ru = _decorate_headers(df, 'ru')
        df_ru.to_excel(ru_path, index=False)
    except Exception as e:
        UX.warn(f"ä¿„è¯­ç‰ˆæœ¬å¯¼å‡ºå¤±è´¥: {e}")

def _save_source_specific_bilingual(df: pd.DataFrame, output_path: str, file_prefix: str):
    """æŒ‰ä¿¡æºåˆ†åˆ«ä¿å­˜åŒè¯­ç‰ˆæœ¬ï¼ˆVKç”¨ä¿„è¯­ç‰ˆï¼ŒçŸ¥ä¹ç”¨ä¸­æ–‡ç‰ˆï¼‰"""
    # æ¸…ç†DataFrame
    df = _clean_dataframe(df, f"{file_prefix}ä¿å­˜")
    
    try:
        os.makedirs(output_path, exist_ok=True)
    except Exception as e:
        UX.warn(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {e}")
    
    # æŒ‰ä¿¡æºåˆ†ç»„
    if 'Source' in df.columns:
        vk_data = df[df['Source'] == 'vk']
        zhihu_data = df[df['Source'] == 'çŸ¥ä¹']
        
        # VKæ•°æ®ï¼šç”Ÿæˆä¿„è¯­ç‰ˆï¼ˆå†…å®¹æœ¬æ¥å°±æ˜¯ä¿„è¯­ï¼‰
        if not vk_data.empty:
            ru_path = os.path.join(output_path, f'{file_prefix}_VK_ä¿„è¯­ç‰ˆ.xlsx')
            try:
                df_ru = _organize_columns_order(vk_data, 'vk')
                df_ru = _decorate_headers(df_ru, 'ru')
                df_ru.to_excel(ru_path, index=False)
                UX.ok(f"VKä¿„è¯­ç‰ˆå·²ç”Ÿæˆ: {ru_path}")
            except Exception as e:
                UX.warn(f"VKä¿„è¯­ç‰ˆå¯¼å‡ºå¤±è´¥: {e}")
        
        # çŸ¥ä¹æ•°æ®ï¼šç”Ÿæˆä¸­æ–‡ç‰ˆï¼ˆå†…å®¹æœ¬æ¥å°±æ˜¯ä¸­æ–‡ï¼‰
        if not zhihu_data.empty:
            zh_path = os.path.join(output_path, f'{file_prefix}_çŸ¥ä¹_ä¸­æ–‡ç‰ˆ.xlsx')
            try:
                df_zh = _organize_columns_order(zhihu_data, 'zhihu')
                df_zh = _decorate_headers(df_zh, 'zh')
                df_zh.to_excel(zh_path, index=False)
                UX.ok(f"çŸ¥ä¹ä¸­æ–‡ç‰ˆå·²ç”Ÿæˆ: {zh_path}")
            except Exception as e:
                UX.warn(f"çŸ¥ä¹ä¸­æ–‡ç‰ˆå¯¼å‡ºå¤±è´¥: {e}")
        
    else:
        # æ²¡æœ‰Sourceåˆ—ï¼Œä½¿ç”¨é€šç”¨åŒè¯­ä¿å­˜
        zh_path = os.path.join(output_path, f'{file_prefix}_ä¸­æ–‡ç‰ˆ.xlsx')
        ru_path = os.path.join(output_path, f'{file_prefix}_ä¿„è¯­ç‰ˆ.xlsx')
        _save_bilingual(df, zh_path, ru_path)

def generate_reliability_files_from_input(input_path, final_results_path, output_path):
    """ç›´æ¥ä»åŸå§‹è¾“å…¥æ–‡ä»¶ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶"""
    UX.info("ä»åŸå§‹è¾“å…¥æ–‡ä»¶ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶...")
    
    try:
        df_results = pd.read_excel(final_results_path)
        UX.info(f"æœ€ç»ˆç»“æœæ•°æ®åº“åŠ è½½æˆåŠŸ: {len(df_results)}æ¡è®°å½•")
        UX.info(f"æœ€ç»ˆç»“æœæ•°æ®åº“åˆ—å: {list(df_results.columns)}")
    except Exception as e:
        UX.err(f"åŠ è½½æœ€ç»ˆç»“æœæ•°æ®åº“å¤±è´¥: {e}")
        return
    
    # è·å–è¾“å…¥æ–‡ä»¶
    files = [f for f in os.listdir(input_path) 
            if f.endswith('.xlsx') and not f.startswith('~$')]
    
    if not files:
        UX.warn("æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶")
        return
    
    # åˆ†ç±»æ–‡ä»¶
    vk_files = []
    zhihu_files = []
    
    for f in files:
        source = identify_source(f)
        if source == 'vk':
            vk_files.append(f)
        elif source == 'çŸ¥ä¹':
            zhihu_files.append(f)
        else:
            UX.warn(f"æ— æ³•è¯†åˆ«æ–‡ä»¶ç±»å‹: {f}")
    
    # æ„å»ºçˆ¶æ–‡æœ¬æ•°æ®
    parent_data = []
    
    # å¤„ç†VKæ–‡ä»¶
    for filename in vk_files:
        input_file = os.path.join(input_path, filename)
        try:
            df = pd.read_excel(input_file)
            mapping = COLUMN_MAPPING['vk']
            
            for post_id, group in df.groupby(mapping['post_id'], dropna=False):
                if pd.isna(post_id):
                    continue
                    
                post_text = safe_str_convert(group[mapping['post_text']].iloc[0])
                # è·å–channel_nameï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨æ–‡ä»¶å
                channel_name = safe_str_convert(group[mapping.get('channel_name', 'channel_name')].iloc[0]) if mapping.get('channel_name', 'channel_name') in group.columns else filename.replace('.xlsx', '')
                
                # ä¸ºæ¯ä¸ªè¯„è®ºåˆ›å»ºä¸€æ¡è®°å½•ï¼ŒåŒ…å«post_textå’Œcomment_text
                for _, comment_row in group.iterrows():
                    comment_id = comment_row[mapping['comment_id']]
                    comment_text = comment_row[mapping['comment_text']]
                    
                    if pd.isna(comment_id) or pd.isna(comment_text):
                        continue
                    
                    parent_data.append({
                        'Post_ID': f"VK-{post_id}",
                        'Comment_ID': f"VK-{comment_id}",
                        'Original_Post_ID': str(post_id),
                        'Original_Comment_ID': str(comment_id),
                        'Source': 'vk',
                        'Post_Text': post_text,
                        'Comment_Text': safe_str_convert(comment_text),
                        'channel_name': channel_name
                    })
                
        except Exception as e:
            UX.warn(f"å¤„ç†VKæ–‡ä»¶ {filename} å¤±è´¥: {e}")
    
    # å¤„ç†çŸ¥ä¹æ–‡ä»¶ - åŸºäºç»“æœæ•°æ®æ„å»ºçˆ¶æ–‡æœ¬ï¼ˆåŒ…å«è®®é¢˜å•å…ƒï¼‰
    for filename in zhihu_files:
        input_file = os.path.join(input_path, filename)
        try:
            df = pd.read_excel(input_file)
            mapping = COLUMN_MAPPING['zhihu']
            
            # ä»ç»“æœæ•°æ®ä¸­è·å–çŸ¥ä¹çš„è®®é¢˜å•å…ƒ
            zhihu_results = df_results[df_results['Source'] == 'çŸ¥ä¹']
            UX.info(f"çŸ¥ä¹æ–‡ä»¶ {filename}: ç»“æœæ•°æ®ä¸­æ‰¾åˆ° {len(zhihu_results)} æ¡çŸ¥ä¹è®®é¢˜å•å…ƒ")
            if not zhihu_results.empty:
                for _, unit_row in zhihu_results.iterrows():
                    unit_text = safe_str_convert(unit_row.get('Unit_Text', ''))
                    if not unit_text.strip():
                        continue
                    
                    # ä»Unit_IDä¸­æå–åŸå§‹answer_id
                    unit_id = str(unit_row.get('Unit_ID', ''))
                    if unit_id.startswith('ZH-') and '-' in unit_id:
                        original_answer_id = unit_id.split('-')[1]  # æå–answer_idéƒ¨åˆ†
                        
                        # ä»åŸå§‹è¾“å…¥æ–‡ä»¶ä¸­æ‰¾åˆ°å¯¹åº”çš„å›ç­”ä¿¡æ¯
                        original_row = df[df[mapping["id"]].astype(str) == original_answer_id]
                        if not original_row.empty:
                            original_row = original_row.iloc[0]
                            question = safe_str_convert(original_row[mapping["question"]])
                            # æ™ºèƒ½è·å–ä½œè€…åç§°
                            author_column = mapping.get("author", "å›ç­”ç”¨æˆ·å")
                            if author_column in original_row.index:
                                author_value = safe_str_convert(original_row[author_column])
                                author = author_value if author_value.strip() else 'æœªçŸ¥ä½œè€…'
                            else:
                                author = 'æœªçŸ¥ä½œè€…'
                            
                            parent_data.append({
                                'Answer_ID': f"ZH-{original_answer_id}",
                                'Original_ID': original_answer_id,
                                'Source': 'çŸ¥ä¹',
                                'Question': question,
                                'Unit_Text': unit_text,  # ä½¿ç”¨è®®é¢˜å•å…ƒæ–‡æœ¬è€Œä¸æ˜¯å®Œæ•´å›ç­”
                                'Author': author
                            })
            else:
                # å¦‚æœç»“æœæ•°æ®ä¸­æ²¡æœ‰çŸ¥ä¹æ•°æ®ï¼Œå›é€€åˆ°åŸå§‹æ–¹å¼
                UX.warn(f"ç»“æœæ•°æ®ä¸­æ²¡æœ‰çŸ¥ä¹æ•°æ®ï¼Œè·³è¿‡ {filename}")
                
        except Exception as e:
            UX.warn(f"å¤„ç†çŸ¥ä¹æ–‡ä»¶ {filename} å¤±è´¥: {e}")
    
    if not parent_data:
        UX.warn("æ²¡æœ‰æ”¶é›†åˆ°çˆ¶æ–‡æœ¬æ•°æ®")
        return
    
    # åˆ›å»ºçˆ¶æ–‡æœ¬DataFrame
    df_parent = pd.DataFrame(parent_data)
    UX.info(f"æ”¶é›†åˆ° {len(df_parent)} æ¡çˆ¶æ–‡æœ¬æ•°æ®")
    
    # æ ‡è®°ä½¿ç”¨æƒ…å†µ - ä½¿ç”¨å“ˆå¸Œå€¼åŒ¹é…
    UX.info("ä½¿ç”¨å“ˆå¸Œå€¼åŒ¹é…çˆ¶æ–‡æœ¬ä½¿ç”¨æƒ…å†µ...")
    
    # ä»ç»“æœæ•°æ®ä¸­è·å–å·²ä½¿ç”¨çš„å“ˆå¸Œå€¼
    if 'Unit_Hash' not in df_results.columns:
        UX.warn(f"æ— æ³•åœ¨ç»“æœæ•°æ®ä¸­æ‰¾åˆ°Unit_Hashåˆ—ï¼Œå¯ç”¨åˆ—: {list(df_results.columns)}")
        df_parent['Was_Used'] = False
        used_count = 0
        UX.info(f"çˆ¶æ–‡æœ¬ä½¿ç”¨æƒ…å†µ: {used_count}/{len(df_parent)} æ¡è¢«ä½¿ç”¨")
        return
    
    used_hashes = set(df_results['Unit_Hash'].dropna().astype(str).unique())
    UX.info(f"ä»ç»“æœæ•°æ®ä¸­æ‰¾åˆ° {len(used_hashes)} ä¸ªå·²ä½¿ç”¨çš„å“ˆå¸Œå€¼")
    
    # ä¸ºçˆ¶æ–‡æœ¬æ•°æ®æ·»åŠ å“ˆå¸Œå€¼å¹¶åŒ¹é…
    if 'Comment_Text' in df_parent.columns:
        # VKæ•°æ®ï¼šåŸºäºComment_Textç”Ÿæˆå“ˆå¸Œå€¼
        df_parent['Text_Hash'] = df_parent['Comment_Text'].apply(
            lambda x: hashlib.sha256(normalize_text(safe_str_convert(x)).encode('utf-8')).hexdigest() 
            if pd.notna(x) and str(x).strip() else None
        )
        UX.info(f"VKæ•°æ®ä½¿ç”¨è¯„è®ºæ–‡æœ¬å“ˆå¸Œå€¼åŒ¹é…")
    elif 'Unit_Text' in df_parent.columns:
        # çŸ¥ä¹æ•°æ®ï¼šåŸºäºUnit_Textç”Ÿæˆå“ˆå¸Œå€¼
        df_parent['Text_Hash'] = df_parent['Unit_Text'].apply(
            lambda x: hashlib.sha256(normalize_text(safe_str_convert(x)).encode('utf-8')).hexdigest() 
            if pd.notna(x) and str(x).strip() else None
        )
        UX.info(f"çŸ¥ä¹æ•°æ®ä½¿ç”¨è®®é¢˜å•å…ƒæ–‡æœ¬å“ˆå¸Œå€¼åŒ¹é…")
    else:
        UX.warn("æ— æ³•æ‰¾åˆ°æ–‡æœ¬åˆ—ç”¨äºå“ˆå¸Œå€¼åŒ¹é…")
        df_parent['Was_Used'] = False
        used_count = 0
        UX.info(f"çˆ¶æ–‡æœ¬ä½¿ç”¨æƒ…å†µ: {used_count}/{len(df_parent)} æ¡è¢«ä½¿ç”¨")
        return
    
    # ç»Ÿä¸€å“ˆå¸Œå€¼åŒ¹é…
    df_parent['Was_Used'] = df_parent['Text_Hash'].astype(str).isin(used_hashes)
    
    used_count = df_parent['Was_Used'].sum()
    UX.info(f"çˆ¶æ–‡æœ¬ä½¿ç”¨æƒ…å†µ: {used_count}/{len(df_parent)} æ¡è¢«ä½¿ç”¨")
    
    # åå‘æ£€éªŒï¼ˆå¬å›ç‡ï¼‰- åªé’ˆå¯¹VKæ•°æ®ï¼ŒçŸ¥ä¹é»˜è®¤å…¨ç›¸å…³
    negative_samples = []
    # åªå¤„ç†VKæ•°æ®
    if 'vk' in RELIABILITY_SAMPLING_CONFIG:
        cfg = RELIABILITY_SAMPLING_CONFIG['vk']
        unused = df_parent[(df_parent['Source'] == 'vk') & (df_parent['Was_Used'] == False)]
        UX.info(f"VKä¿¡æº: æœªä½¿ç”¨æ ·æœ¬ {len(unused)} æ¡ï¼Œéœ€è¦æŠ½æ · {cfg['recall']} æ¡")
        if len(unused) > 0:
            n = min(cfg['recall'], len(unused))
            if n > 0:
                sample = unused.sample(n=n, replace=False, random_state=2025)
                negative_samples.append(sample)
                UX.info(f"VKä¿¡æº: æˆåŠŸæŠ½æ · {len(sample)} æ¡åå‘æ£€éªŒæ ·æœ¬")
            else:
                UX.warn(f"VKä¿¡æº: éœ€è¦æŠ½æ ·æ•°é‡ä¸º0")
        else:
            UX.warn(f"VKä¿¡æº: æ²¡æœ‰æœªä½¿ç”¨çš„æ ·æœ¬å¯ä¾›æŠ½æ ·")
    
    if negative_samples:
        df_neg = pd.concat(negative_samples, ignore_index=True)
        
        # åå‘æ£€éªŒåªä¿ç•™VKç›¸å…³å­—æ®µï¼ˆå»æ‰Post_IDå’ŒComment_IDï¼‰
        negative_essential_columns = ['Original_Post_ID', 'Original_Comment_ID', 'Source', 'Post_Text', 'Comment_Text', 'channel_name']
        
        # è¿‡æ»¤åˆ—ï¼ˆåªä¿ç•™å­˜åœ¨çš„åˆ—ï¼‰
        available_neg_columns = [col for col in negative_essential_columns if col in df_neg.columns]
        df_neg_clean = df_neg[available_neg_columns].copy()
        
        # æ·»åŠ ç›¸å…³æ€§æ£€éªŒåˆ—
        if 'Inspector_Is_CN_RU_Related' not in df_neg_clean.columns:
            df_neg_clean['Inspector_Is_CN_RU_Related'] = ''
        
        # åªç”ŸæˆVKä¿„è¯­ç‰ˆï¼ˆå› ä¸ºåå‘æ£€éªŒåªæ¶‰åŠVKï¼‰
        try:
            os.makedirs(output_path, exist_ok=True)
            ru_path = os.path.join(output_path, 'åå‘æ£€éªŒ_å¬å›ç‡æ ·æœ¬_VK_ä¿„è¯­ç‰ˆ.xlsx')
            df_ru = _decorate_headers(df_neg_clean, 'ru')
            df_ru.to_excel(ru_path, index=False)
            UX.ok(f"VKåå‘æ£€éªŒæ ·æœ¬å·²ç”Ÿæˆ: {ru_path}")
        except Exception as e:
            UX.warn(f"VKåå‘æ£€éªŒæ ·æœ¬å¯¼å‡ºå¤±è´¥: {e}")
    else:
        UX.warn("æ²¡æœ‰VKåå‘æ£€éªŒæ ·æœ¬å¯ä¾›ç”Ÿæˆ")
    
    # æ¡†æ¶ç»´åº¦æ£€éªŒï¼ˆåŸºäºæ‰€æœ‰æˆåŠŸå¤„ç†çš„ç»“æœï¼‰
    UX.info("ç”Ÿæˆæ¡†æ¶ç»´åº¦æ£€éªŒæ–‡ä»¶...")
    
    # è·å–æ‰€æœ‰æˆåŠŸå¤„ç†çš„ç»“æœ
    all_success_results = df_results[df_results['processing_status'] == ProcessingStatus.SUCCESS]
    
    if not all_success_results.empty:
        # ç›´æ¥ä½¿ç”¨æ‰€æœ‰æˆåŠŸç»“æœï¼Œè®©åç»­çš„åˆ—å¤„ç†é€»è¾‘è‡ªåŠ¨è¿‡æ»¤
        df_combined = all_success_results.copy()
        
        # ç®€åŒ–ï¼šåªåœ¨æœ‰VKæ•°æ®æ—¶æ·»åŠ VKç‰¹æœ‰æ£€éªŒåˆ—
        if 'Source' in df_combined.columns and (df_combined['Source'] == 'vk').any():
            if 'Inspector_Boundary' not in df_combined.columns:
                df_combined['Inspector_Boundary'] = ''
            if 'Inspector_Is_CN_RU_Related' not in df_combined.columns:
                df_combined['Inspector_Is_CN_RU_Related'] = ''
            
            # ç¡®ä¿VKæ•°æ®æœ‰Post_Textåˆ—ï¼ˆä»åŸå§‹è¾“å…¥æ–‡ä»¶è·å–ï¼‰
            vk_mask = df_combined['Source'] == 'vk'
            if vk_mask.any():
                # æ£€æŸ¥VKæ•°æ®æ˜¯å¦å·²ç»æœ‰æœ‰æ•ˆçš„Post_Textåˆ—
                needs_post_text = True
                if 'Post_Text' in df_combined.columns:
                    # æ£€æŸ¥Post_Textåˆ—æ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
                    vk_post_text_sample = df_combined[vk_mask]['Post_Text'].dropna()
                    if not vk_post_text_sample.empty and not vk_post_text_sample.iloc[0] in ['', 'æœªæ‰¾åˆ°åŸå§‹å¸–å­æ–‡æœ¬']:
                        needs_post_text = False
                        UX.info("VKæ•°æ®å·²æœ‰æœ‰æ•ˆçš„Post_Textåˆ—ï¼Œè·³è¿‡é‡æ–°è·å–")
                
                if needs_post_text:
                    UX.info("å¼€å§‹ä¸ºVKæ•°æ®æ·»åŠ Post_Textåˆ—...")
                    
                    # ä»åŸå§‹è¾“å…¥æ–‡ä»¶è·å–Post_Textï¼Œä½¿ç”¨å“ˆå¸Œå€¼åŒ¹é…
                    comment_to_post_map = {}  # è¯„è®ºå“ˆå¸Œ -> å¸–å­æ–‡æœ¬
                    for filename in [f for f in os.listdir(input_path) if f.endswith('.xlsx') and not f.startswith('~$')]:
                        if 'vk' in filename.lower():
                            try:
                                input_file = os.path.join(input_path, filename)
                                df_input = pd.read_excel(input_file)
                                mapping = COLUMN_MAPPING['vk']
                                
                                for post_id, group in df_input.groupby(mapping['post_id'], dropna=False):
                                    if not pd.isna(post_id):
                                        post_text = safe_str_convert(group[mapping['post_text']].iloc[0])
                                        
                                        # ä¸ºæ¯ä¸ªè¯„è®ºè®¡ç®—å“ˆå¸Œå€¼å¹¶æ˜ å°„åˆ°å¸–å­æ–‡æœ¬
                                        for _, row in group.iterrows():
                                            comment_text = safe_str_convert(row[mapping['comment_text']])
                                            if comment_text.strip():
                                                comment_norm = normalize_text(comment_text)
                                                comment_hash = hashlib.sha256(comment_norm.encode('utf-8')).hexdigest()
                                                comment_to_post_map[comment_hash] = post_text
                                
                                UX.info(f"ä»æ–‡ä»¶ {filename} å»ºç«‹äº† {len([k for k in comment_to_post_map.keys()])} ä¸ªè¯„è®ºåˆ°å¸–å­çš„æ˜ å°„")
                            except Exception as e:
                                UX.warn(f"è¯»å–VKè¾“å…¥æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                
                    UX.info(f"æ€»å…±å»ºç«‹äº† {len(comment_to_post_map)} ä¸ªè¯„è®ºå“ˆå¸Œåˆ°å¸–å­æ–‡æœ¬çš„æ˜ å°„")
                    
                    # ä¸ºVKæ•°æ®æ·»åŠ Post_Textåˆ—
                    if 'Post_Text' not in df_combined.columns:
                        df_combined['Post_Text'] = ''
                    
                    vk_data_count = vk_mask.sum()
                    UX.info(f"éœ€è¦å¤„ç† {vk_data_count} æ¡VKæ•°æ®")
                    
                    # æ£€æŸ¥VKæ•°æ®çš„åˆ—å
                    if vk_mask.any():
                        vk_sample = df_combined[vk_mask].iloc[0]
                        UX.info(f"VKæ•°æ®åˆ—å: {list(vk_sample.index)}")
                    
                    success_count = 0
                    for idx, row in df_combined[vk_mask].iterrows():
                        # ä½¿ç”¨Unit_HashåŒ¹é…
                        if 'Unit_Hash' in row and not pd.isna(row['Unit_Hash']):
                            unit_hash = row['Unit_Hash']
                            if unit_hash in comment_to_post_map:
                                df_combined.loc[idx, 'Post_Text'] = comment_to_post_map[unit_hash]
                                success_count += 1
                            else:
                                df_combined.loc[idx, 'Post_Text'] = f'æœªæ‰¾åˆ°å“ˆå¸ŒåŒ¹é… (hash: {unit_hash[:8]}...)'
                        else:
                            df_combined.loc[idx, 'Post_Text'] = 'æœªæ‰¾åˆ°Unit_Hashåˆ—'
                    
                    UX.info(f"æˆåŠŸåŒ¹é… {success_count}/{vk_data_count} æ¡VKæ•°æ®çš„Post_Text")
        
        # å®šä¹‰æ¡†æ¶å’Œç»´åº¦
        frames = ['ProblemDefinition', 'ResponsibilityAttribution', 'MoralEvaluation',
                 'SolutionRecommendation', 'ActionStatement', 'CausalExplanation']
        dims = ['Valence', 'Evidence_Type', 'Attribution_Level', 'Temporal_Focus',
               'Primary_Actor_Type', 'Geographic_Scope', 'Relationship_Model_Definition',
               'Discourse_Type']
        
        # ä½¿ç”¨æ–°çš„å…¬å…±å‡½æ•°å¤„ç†
        df_combined = _add_frame_boolean_columns(df_combined, frames)
        df_combined = _add_inspector_columns(df_combined, frames, dims)
        # æ³¨æ„ï¼šè¿™é‡Œä¸è°ƒç”¨ _organize_columns_orderï¼Œå› ä¸ºæ•°æ®æ˜¯æ··åˆçš„
        # åˆ—çš„ç»„ç»‡å°†åœ¨ _save_source_specific_bilingual ä¸­æŒ‰ä¿¡æºåˆ†åˆ«å¤„ç†
        
        # æ¸…ç†å¹¶ä¿å­˜
        df_combined = _clean_dataframe(df_combined, "æ¡†æ¶ç»´åº¦æ£€éªŒæ ·æœ¬")
        _save_source_specific_bilingual(df_combined, output_path, 'æ¡†æ¶ç»´åº¦æ£€éªŒ_å•æ£€éªŒå‘˜')
        UX.ok("æ¡†æ¶ç»´åº¦æ£€éªŒæ–‡ä»¶å·²ç”Ÿæˆ(æŒ‰ä¿¡æºåˆ†è¯­è¨€)")
    else:
        UX.warn("æ²¡æœ‰æˆåŠŸå¤„ç†çš„ç»“æœå¯ä¾›ç”Ÿæˆæ¡†æ¶ç»´åº¦æ£€éªŒæ–‡ä»¶")
    
    UX.ok("ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆå®Œæˆ")

# ==============================================================================
# === ğŸ“Š ä¸»å‡½æ•°
# ==============================================================================

async def main():
    """ä¸»å‡½æ•°"""
    UX.start_run()
    UX.phase("ç¤¾äº¤åª’ä½“è®®é¢˜å•å…ƒåˆ†æå™¨å¯åŠ¨")
    UX.info(f"ä¿¡åº¦æ£€éªŒæ¨¡å¼: {'å¼€å¯' if RELIABILITY_TEST_MODE else 'å…³é—­'}")
    
    # æ£€æŸ¥é…ç½®
    required_models = ["VK_BATCH", "ZHIHU_CHUNKING", "ZHIHU_ANALYSIS"]
    missing = [m for m in required_models if m not in API_CONFIG.get("STAGE_MODELS", {})]
    if missing:
        UX.err(f"ç¼ºå°‘æ¨¡å‹é…ç½®: {missing}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_PATH, exist_ok=True)
    
    # è·å–è¾“å…¥æ–‡ä»¶
    files = [f for f in os.listdir(INPUT_PATH) 
            if f.endswith('.xlsx') and not f.startswith('~$')]
    
    if not files:
        UX.warn("æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶")
        return
    
    all_Units_data = []
    all_results_files = []
    
    # åˆ†ç±»æ–‡ä»¶
    vk_files = []
    zhihu_files = []
    
    for f in files:
        source = identify_source(f)
        if source == 'vk':
            vk_files.append(f)
        elif source == 'çŸ¥ä¹':
            zhihu_files.append(f)
        else:
            UX.warn(f"æ— æ³•è¯†åˆ«æ–‡ä»¶ç±»å‹: {f}")
    
    # åˆ›å»ºAPIæœåŠ¡å®ä¾‹ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
    api_service_sync = APIService()
    
    # å¤„ç†VKæ–‡ä»¶
    if vk_files:
        UX.phase("å¤„ç†VKæ–‡ä»¶")
        
        for filename in vk_files:
            input_file = os.path.join(INPUT_PATH, filename)
            output_file = os.path.join(OUTPUT_PATH, f"(ä¸èƒ½åˆ )analyzed_{filename}")
            
            try:
                df = pd.read_excel(input_file)
                processor = VKProcessor(api_service_sync)
                processor.process(df, output_file, 'vk')
                
                if processor.Units_collector:
                    all_Units_data.extend(processor.Units_collector)
                all_results_files.append(output_file)
                
            except Exception as e:
                UX.err(f"å¤„ç†VKæ–‡ä»¶ {filename} å¤±è´¥: {e}")
    
    # å¤„ç†çŸ¥ä¹æ–‡ä»¶
    if zhihu_files:
        UX.phase("å¤„ç†çŸ¥ä¹æ–‡ä»¶")
        failed_zhihu_ids = []  # åˆå§‹åŒ–å¤±è´¥ä»»åŠ¡åˆ—è¡¨
        
        async with aiohttp.ClientSession() as session:
            api_service_async = APIService(session)
            
            for filename in zhihu_files:
                input_file = os.path.join(INPUT_PATH, filename)
                output_file = os.path.join(OUTPUT_PATH, f"(ä¸èƒ½åˆ )analyzed_{filename}")
                
                try:
                    df = pd.read_excel(input_file)
                    processor = ZhihuProcessor(api_service_async)
                    # æ¥æ”¶è¿”å›çš„å¤±è´¥ID
                    failed_ids_in_file = await processor.process(df, output_file, 'çŸ¥ä¹')
                    if failed_ids_in_file:
                        failed_zhihu_ids.extend(failed_ids_in_file)
                    
                    if processor.Units_collector:
                        all_Units_data.extend(processor.Units_collector)
                    all_results_files.append(output_file)
                    
                except Exception as e:
                    UX.err(f"å¤„ç†çŸ¥ä¹æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                    # è®°å½•æ–‡ä»¶çº§åˆ«çš„å¤±è´¥
                    failed_zhihu_ids.append(f"FILE_FAILED: {filename} - {str(e)[:100]}")
            
            # æ‰“å°APIç»Ÿè®¡
            api_service_async.print_statistics()
        
        # æ£€æŸ¥å¹¶è®°å½•çŸ¥ä¹å¤±è´¥ä»»åŠ¡
        if failed_zhihu_ids:
            log_path = os.path.join(OUTPUT_PATH, 'zhihu_failed_ids_log.txt')
            try:
                with open(log_path, 'w', encoding='utf-8') as f:
                    for failed_id in failed_zhihu_ids:
                        f.write(f"{failed_id}\n")
                UX.warn(f"çŸ¥ä¹å¤±è´¥ä»»åŠ¡å·²è®°å½•åˆ°: {log_path}")
            except Exception as e:
                UX.warn(f"å†™å…¥çŸ¥ä¹å¤±è´¥æ—¥å¿—å¤±è´¥: {e}")
    
    # æ‰“å°VKå¤„ç†çš„APIç»Ÿè®¡
    api_service_sync.print_statistics()
    # ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶
    if RELIABILITY_TEST_MODE:
        UX.phase("ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶")
        
        # è°ƒè¯•ä¿¡æ¯
        UX.info(f"æ•°æ®æ”¶é›†çŠ¶æ€ - ç»“æœæ–‡ä»¶: {len(all_results_files)}ä¸ª")
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        final_path = None
        if all_results_files:
            all_results = []
            for file in all_results_files:
                if os.path.exists(file):
                    df = pd.read_excel(file)
                    all_results.append(df)
                    UX.info(f"åŠ è½½ç»“æœæ–‡ä»¶: {file} ({len(df)}æ¡è®°å½•)")
                else:
                    UX.warn(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {file}")
            
            if all_results:
                df_final = pd.concat(all_results, ignore_index=True)
                
                # --- BEGIN INSERTED BLOCK ---
                UX.info("æ­£åœ¨æ ‡å‡†åŒ–æ–‡æœ¬åˆ—å...")
                # æ£€æŸ¥'Unit_Text'åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºï¼Œé¿å…åç»­æ“ä½œå› åˆ—ä¸å­˜åœ¨è€Œå¤±è´¥
                if 'Unit_Text' not in df_final.columns:
                    df_final['Unit_Text'] = pd.NA

                # å®šä½åˆ°æ‰€æœ‰Sourceä¸º'vk'çš„è¡Œ
                vk_mask = df_final['Source'] == 'vk'

                # å°†è¿™äº›è¡Œä¸­'comment_text'åˆ—çš„å†…å®¹ï¼Œå¡«å……åˆ°'Unit_Text'åˆ—çš„ç©ºå€¼ä½ç½®
                # ä½¿ç”¨.locç¡®ä¿å®‰å…¨èµ‹å€¼
                df_final.loc[vk_mask, 'Unit_Text'] = df_final.loc[vk_mask, 'Unit_Text'].fillna(df_final.loc[vk_mask, 'comment_text'])

                UX.ok("æ–‡æœ¬åˆ—å'Unit_Text'æ ‡å‡†åŒ–å®Œæˆã€‚")
                # --- END INSERTED BLOCK ---
                
                final_path = os.path.join(OUTPUT_PATH, 'ç¤¾äº¤åª’ä½“_æœ€ç»ˆåˆ†ææ•°æ®åº“.xlsx')
                df_final.to_excel(final_path, index=False)
                UX.info(f"æœ€ç»ˆåˆ†ææ•°æ®åº“è·¯å¾„: {final_path}")
                
                # ç»Ÿè®¡
                if 'processing_status' in df_final.columns:
                    success = (df_final['processing_status'] == ProcessingStatus.SUCCESS).sum()
                    failed = (df_final['processing_status'] == ProcessingStatus.API_FAILED).sum()
                    no_relevant = (df_final['processing_status'] == ProcessingStatus.NO_RELEVANT).sum()
                    UX.ok(f"æœ€ç»ˆæ•°æ®åº“: æ€»{len(df_final)}æ¡, æˆåŠŸ{success}, å¤±è´¥{failed}, æ— ç›¸å…³å†…å®¹{no_relevant}")
                else:
                    UX.warn("æœ€ç»ˆæ•°æ®åº“ä¸­æ²¡æœ‰processing_statusåˆ—")
            else:
                UX.warn("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•ç»“æœæ–‡ä»¶")
        else:
            UX.warn("æ²¡æœ‰ç»“æœæ–‡ä»¶éœ€è¦åˆå¹¶")
        
        # ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶ï¼ˆç›´æ¥ä»åŸå§‹è¾“å…¥æ–‡ä»¶ï¼‰
        if final_path:
            try:
                UX.info("å¼€å§‹ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶...")
                generate_reliability_files_from_input(INPUT_PATH, final_path, OUTPUT_PATH)
                UX.ok("ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆå®Œæˆ")
            except Exception as e:
                UX.err(f"ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶å¤±è´¥: {str(e)}")
                import traceback
                UX.err(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        else:
            UX.warn(f"æ— æ³•ç”Ÿæˆä¿¡åº¦æ£€éªŒæ–‡ä»¶ - æœ€ç»ˆç»“æœæ–‡ä»¶: {final_path}")
    else:
        UX.info("ä¿¡åº¦æ£€éªŒæ¨¡å¼å·²å…³é—­ï¼Œè·³è¿‡ä¿¡åº¦æ£€éªŒæ–‡ä»¶ç”Ÿæˆ")
    
    # é…ç½®éªŒè¯æµ‹è¯•
    UX.info("=== é…ç½®éªŒè¯æµ‹è¯• ===")
    UX.info(f"VKé•¿æ–‡æœ¬é˜ˆå€¼: {VK_LONG_TEXT_THRESHOLD} tokens")
    UX.info(f"çŸ¥ä¹çŸ­æ–‡æœ¬é˜ˆå€¼: {ZHIHU_SHORT_TOKEN_THRESHOLD} tokens")  
    UX.info(f"çŸ¥ä¹é•¿æ–‡æœ¬é˜ˆå€¼: {ZHIHU_LONG_TOKEN_THRESHOLD} tokens")
    UX.info(f"æœ€å¤§å¹¶å‘è¯·æ±‚æ•°: {MAX_CONCURRENT_REQUESTS}")
    UX.info(f"APIé‡è¯•æ¬¡æ•°: {API_RETRY_ATTEMPTS}")
    
    # æµ‹è¯•tokenè®¡ç®—å™¨
    test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ç”¨æ¥éªŒè¯tokenè®¡ç®—åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ"
    test_tokens = count_tokens(test_text)
    UX.info(f"æµ‹è¯•æ–‡æœ¬tokenæ•°: {test_tokens}")
    
    UX.phase("æ‰€æœ‰ä»»åŠ¡å®Œæˆ")

if __name__ == "__main__":
    import sys
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())